[
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n1. First, describe the design idea and main steps of your algorithm in one sentence. The description must be inside within boxed {}. \n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 1,
        "algorithm": "The algorithm selects the most promising solution from the archive (based on average value-to-weight density) and applies a hybrid local search strategy that prioritizes swapping low-value items for high-value ones while ensuring feasibility through adaptive perturbations. If no swaps are feasible, it randomly removes items to free up capacity. The approach balances exploration (via adaptive swaps) and exploitation (via value-to-weight ratios) to improve both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution from the archive\n    # Calculate the density (value/weight) for each objective\n    densities = []\n    for sol, (v1, v2) in archive:\n        included = sol == 1\n        total_weight = np.sum(weight_lst[included])\n        if total_weight == 0:\n            density1 = density2 = 0\n        else:\n            density1 = np.sum(value1_lst[included]) / total_weight\n            density2 = np.sum(value2_lst[included]) / total_weight\n        densities.append((density1 + density2) / 2)  # Average density\n\n    # Select the solution with the highest density (most promising for improvement)\n    best_idx = np.argmax(densities)\n    base_solution = archive[best_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Apply novel local search operator\n    # Hybrid strategy: adaptive item swaps and perturbations\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Adaptive swap: replace a low-value item with a high-value item\n        # Calculate value-to-weight ratio for included items\n        included_ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_ratios)]\n\n        # Calculate value-to-weight ratio for excluded items\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        # Check if swap is feasible\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n        else:\n            # If swap is not feasible, perform a perturbation\n            # Randomly flip a small number of items to find a feasible solution\n            max_perturbations = min(3, len(included_items))\n            for _ in range(max_perturbations):\n                candidate = random.choice(included_items)\n                if current_weight - weight_lst[candidate] <= capacity:\n                    new_solution[candidate] = 0\n                    current_weight -= weight_lst[candidate]\n                    break\n\n    # If no items are included, add the best item if possible\n    elif len(included_items) == 0 and len(excluded_items) > 0:\n        # Add the item with the highest value-to-weight ratio\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        if weight_lst[best_excluded] <= capacity:\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.41552698679575917,
            0.3817824423313141
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution from the archive\n    # Calculate the density (value/weight) for each objective\n    densities = []\n    for sol, (v1, v2) in archive:\n        included = sol == 1\n        total_weight = np.sum(weight_lst[included])\n        if total_weight == 0:\n            density1 = density2 = 0\n        else:\n            density1 = np.sum(value1_lst[included]) / total_weight\n            density2 = np.sum(value2_lst[included]) / total_weight\n        densities.append((density1 + density2) / 2)  # Average density\n\n    # Select the solution with the highest density (most promising for improvement)\n    best_idx = np.argmax(densities)\n    base_solution = archive[best_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Apply novel local search operator\n    # Hybrid strategy: adaptive item swaps and perturbations\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Adaptive swap: replace a low-value item with a high-value item\n        # Calculate value-to-weight ratio for included items\n        included_ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_ratios)]\n\n        # Calculate value-to-weight ratio for excluded items\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        # Check if swap is feasible\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n        else:\n            # If swap is not feasible, perform a perturbation\n            # Randomly flip a small number of items to find a feasible solution\n            max_perturbations = min(3, len(included_items))\n            for _ in range(max_perturbations):\n                candidate = random.choice(included_items)\n                if current_weight - weight_lst[candidate] <= capacity:\n                    new_solution[candidate] = 0\n                    current_weight -= weight_lst[candidate]\n                    break\n\n    # If no items are included, add the best item if possible\n    elif len(included_items) == 0 and len(excluded_items) > 0:\n        # Add the item with the highest value-to-weight ratio\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        if weight_lst[best_excluded] <= capacity:\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n1. First, describe the design idea and main steps of your algorithm in one sentence. The description must be inside within boxed {}. \n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 1,
        "algorithm": "The algorithm selects the most promising solution from the archive (based on average value-to-weight density) and applies a hybrid local search strategy that prioritizes swapping low-value items for high-value ones while ensuring feasibility through adaptive perturbations. If no swaps are feasible, it randomly removes items to free up capacity. The approach balances exploration (via adaptive swaps) and exploitation (via value-to-weight ratios) to improve both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution from the archive\n    # Calculate the density (value/weight) for each objective\n    densities = []\n    for sol, (v1, v2) in archive:\n        included = sol == 1\n        total_weight = np.sum(weight_lst[included])\n        if total_weight == 0:\n            density1 = density2 = 0\n        else:\n            density1 = np.sum(value1_lst[included]) / total_weight\n            density2 = np.sum(value2_lst[included]) / total_weight\n        densities.append((density1 + density2) / 2)  # Average density\n\n    # Select the solution with the highest density (most promising for improvement)\n    best_idx = np.argmax(densities)\n    base_solution = archive[best_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Apply novel local search operator\n    # Hybrid strategy: adaptive item swaps and perturbations\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Adaptive swap: replace a low-value item with a high-value item\n        # Calculate value-to-weight ratio for included items\n        included_ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_ratios)]\n\n        # Calculate value-to-weight ratio for excluded items\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        # Check if swap is feasible\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n        else:\n            # If swap is not feasible, perform a perturbation\n            # Randomly flip a small number of items to find a feasible solution\n            max_perturbations = min(3, len(included_items))\n            for _ in range(max_perturbations):\n                candidate = random.choice(included_items)\n                if current_weight - weight_lst[candidate] <= capacity:\n                    new_solution[candidate] = 0\n                    current_weight -= weight_lst[candidate]\n                    break\n\n    # If no items are included, add the best item if possible\n    elif len(included_items) == 0 and len(excluded_items) > 0:\n        # Add the item with the highest value-to-weight ratio\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        if weight_lst[best_excluded] <= capacity:\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.41552698679575917,
            0.3817824423313141
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution from the archive\n    # Calculate the density (value/weight) for each objective\n    densities = []\n    for sol, (v1, v2) in archive:\n        included = sol == 1\n        total_weight = np.sum(weight_lst[included])\n        if total_weight == 0:\n            density1 = density2 = 0\n        else:\n            density1 = np.sum(value1_lst[included]) / total_weight\n            density2 = np.sum(value2_lst[included]) / total_weight\n        densities.append((density1 + density2) / 2)  # Average density\n\n    # Select the solution with the highest density (most promising for improvement)\n    best_idx = np.argmax(densities)\n    base_solution = archive[best_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Apply novel local search operator\n    # Hybrid strategy: adaptive item swaps and perturbations\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Adaptive swap: replace a low-value item with a high-value item\n        # Calculate value-to-weight ratio for included items\n        included_ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_ratios)]\n\n        # Calculate value-to-weight ratio for excluded items\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        # Check if swap is feasible\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n        else:\n            # If swap is not feasible, perform a perturbation\n            # Randomly flip a small number of items to find a feasible solution\n            max_perturbations = min(3, len(included_items))\n            for _ in range(max_perturbations):\n                candidate = random.choice(included_items)\n                if current_weight - weight_lst[candidate] <= capacity:\n                    new_solution[candidate] = 0\n                    current_weight -= weight_lst[candidate]\n                    break\n\n    # If no items are included, add the best item if possible\n    elif len(included_items) == 0 and len(excluded_items) > 0:\n        # Add the item with the highest value-to-weight ratio\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        if weight_lst[best_excluded] <= capacity:\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n1. First, describe the design idea and main steps of your algorithm in one sentence. The description must be inside within boxed {}. \n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 1,
        "algorithm": "The algorithm selects the most promising solution from the archive (based on average value-to-weight density) and applies a hybrid local search strategy that prioritizes swapping low-value items for high-value ones while ensuring feasibility through adaptive perturbations. If no swaps are feasible, it randomly removes items to free up capacity. The approach balances exploration (via adaptive swaps) and exploitation (via value-to-weight ratios) to improve both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution from the archive\n    # Calculate the density (value/weight) for each objective\n    densities = []\n    for sol, (v1, v2) in archive:\n        included = sol == 1\n        total_weight = np.sum(weight_lst[included])\n        if total_weight == 0:\n            density1 = density2 = 0\n        else:\n            density1 = np.sum(value1_lst[included]) / total_weight\n            density2 = np.sum(value2_lst[included]) / total_weight\n        densities.append((density1 + density2) / 2)  # Average density\n\n    # Select the solution with the highest density (most promising for improvement)\n    best_idx = np.argmax(densities)\n    base_solution = archive[best_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Apply novel local search operator\n    # Hybrid strategy: adaptive item swaps and perturbations\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Adaptive swap: replace a low-value item with a high-value item\n        # Calculate value-to-weight ratio for included items\n        included_ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_ratios)]\n\n        # Calculate value-to-weight ratio for excluded items\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        # Check if swap is feasible\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n        else:\n            # If swap is not feasible, perform a perturbation\n            # Randomly flip a small number of items to find a feasible solution\n            max_perturbations = min(3, len(included_items))\n            for _ in range(max_perturbations):\n                candidate = random.choice(included_items)\n                if current_weight - weight_lst[candidate] <= capacity:\n                    new_solution[candidate] = 0\n                    current_weight -= weight_lst[candidate]\n                    break\n\n    # If no items are included, add the best item if possible\n    elif len(included_items) == 0 and len(excluded_items) > 0:\n        # Add the item with the highest value-to-weight ratio\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        if weight_lst[best_excluded] <= capacity:\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.41552698679575917,
            0.3817824423313141
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution from the archive\n    # Calculate the density (value/weight) for each objective\n    densities = []\n    for sol, (v1, v2) in archive:\n        included = sol == 1\n        total_weight = np.sum(weight_lst[included])\n        if total_weight == 0:\n            density1 = density2 = 0\n        else:\n            density1 = np.sum(value1_lst[included]) / total_weight\n            density2 = np.sum(value2_lst[included]) / total_weight\n        densities.append((density1 + density2) / 2)  # Average density\n\n    # Select the solution with the highest density (most promising for improvement)\n    best_idx = np.argmax(densities)\n    base_solution = archive[best_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Apply novel local search operator\n    # Hybrid strategy: adaptive item swaps and perturbations\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Adaptive swap: replace a low-value item with a high-value item\n        # Calculate value-to-weight ratio for included items\n        included_ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_ratios)]\n\n        # Calculate value-to-weight ratio for excluded items\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        # Check if swap is feasible\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n        else:\n            # If swap is not feasible, perform a perturbation\n            # Randomly flip a small number of items to find a feasible solution\n            max_perturbations = min(3, len(included_items))\n            for _ in range(max_perturbations):\n                candidate = random.choice(included_items)\n                if current_weight - weight_lst[candidate] <= capacity:\n                    new_solution[candidate] = 0\n                    current_weight -= weight_lst[candidate]\n                    break\n\n    # If no items are included, add the best item if possible\n    elif len(included_items) == 0 and len(excluded_items) > 0:\n        # Add the item with the highest value-to-weight ratio\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        if weight_lst[best_excluded] <= capacity:\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n",
        "operation": "i1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n1. First, describe the design idea and main steps of your algorithm in one sentence. The description must be inside within boxed {}. \n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 2,
        "algorithm": "The algorithm intelligently selects a solution from the archive by prioritizing those with high objective diversity (via crowding distance) and applies a hybrid local search combining item swaps and subset replacements to generate a feasible neighbor solution while ensuring weight constraints are not violated. It first evaluates solutions based on their crowding distances to identify promising candidates, then performs targeted swaps and subset operations to explore the neighborhood while maintaining feasibility. The algorithm balances exploration and exploitation by focusing on high-diversity regions of the Pareto front while dynamically adjusting the search based on the current solution's characteristics.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high objective diversity and low crowding distance)\n    # Calculate crowding distances for solutions in the archive\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n    objectives_sorted = objectives[sorted_indices]\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance (most promising for improvement)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Randomly swap items if feasible\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Strategy 2: Replace a subset of items with new ones if feasible\n    subset_size = min(3, n_items // 4)\n    if subset_size > 0:\n        # Remove a subset of items\n        remove_indices = np.where(new_solution == 1)[0]\n        if len(remove_indices) > 0:\n            remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Add a subset of new items\n        add_indices = np.where(new_solution == 0)[0]\n        if len(add_indices) > 0:\n            add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n",
        "score": [
            -0.7196288894938169,
            0.7204856276512146
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high objective diversity and low crowding distance)\n    # Calculate crowding distances for solutions in the archive\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n    objectives_sorted = objectives[sorted_indices]\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance (most promising for improvement)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Randomly swap items if feasible\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Strategy 2: Replace a subset of items with new ones if feasible\n    subset_size = min(3, n_items // 4)\n    if subset_size > 0:\n        # Remove a subset of items\n        remove_indices = np.where(new_solution == 1)[0]\n        if len(remove_indices) > 0:\n            remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Add a subset of new items\n        add_indices = np.where(new_solution == 0)[0]\n        if len(add_indices) > 0:\n            add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n1. First, describe the design idea and main steps of your algorithm in one sentence. The description must be inside within boxed {}. \n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 2,
        "algorithm": "The algorithm intelligently selects a solution from the archive by prioritizing those with high objective diversity (via crowding distance) and applies a hybrid local search combining item swaps and subset replacements to generate a feasible neighbor solution while ensuring weight constraints are not violated. It first evaluates solutions based on their crowding distances to identify promising candidates, then performs targeted swaps and subset operations to explore the neighborhood while maintaining feasibility. The algorithm balances exploration and exploitation by focusing on high-diversity regions of the Pareto front while dynamically adjusting the search based on the current solution's characteristics.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high objective diversity and low crowding distance)\n    # Calculate crowding distances for solutions in the archive\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n    objectives_sorted = objectives[sorted_indices]\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance (most promising for improvement)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Randomly swap items if feasible\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Strategy 2: Replace a subset of items with new ones if feasible\n    subset_size = min(3, n_items // 4)\n    if subset_size > 0:\n        # Remove a subset of items\n        remove_indices = np.where(new_solution == 1)[0]\n        if len(remove_indices) > 0:\n            remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Add a subset of new items\n        add_indices = np.where(new_solution == 0)[0]\n        if len(add_indices) > 0:\n            add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n",
        "score": [
            -0.7196288894938169,
            0.7204856276512146
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high objective diversity and low crowding distance)\n    # Calculate crowding distances for solutions in the archive\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n    objectives_sorted = objectives[sorted_indices]\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance (most promising for improvement)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Randomly swap items if feasible\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Strategy 2: Replace a subset of items with new ones if feasible\n    subset_size = min(3, n_items // 4)\n    if subset_size > 0:\n        # Remove a subset of items\n        remove_indices = np.where(new_solution == 1)[0]\n        if len(remove_indices) > 0:\n            remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Add a subset of new items\n        add_indices = np.where(new_solution == 0)[0]\n        if len(add_indices) > 0:\n            add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n",
        "operation": "i1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n1. First, describe the design idea and main steps of your algorithm in one sentence. The description must be inside within boxed {}. \n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 3,
        "algorithm": "The algorithm selects a promising solution from the archive (top 30% by combined value) and applies a hybrid local search: 70% chance for a random item swap (ensuring feasibility) or 30% chance for flipping items based on high value-to-weight density, prioritizing items with the best combined value density. It ensures feasibility by checking weight constraints before applying changes.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) > 1:\n        # Sort archive by total value (sum of value1 and value2) in descending order\n        archive_sorted = sorted(archive, key=lambda x: x[1][0] + x[1][1], reverse=True)\n        # Select top 30% of solutions and pick one randomly\n        candidate_indices = min(3, len(archive_sorted))  # Ensure at least 1 candidate\n        selected_idx = np.random.randint(0, candidate_indices)\n        base_solution = archive_sorted[selected_idx][0].copy()\n    else:\n        base_solution = archive[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: randomly swap items or flip items based on value density\n    if np.random.rand() < 0.7:  # 70% chance for random swap\n        # Randomly select two items to swap\n        item_indices = np.where(new_solution == 1)[0]\n        if len(item_indices) >= 2:\n            i, j = np.random.choice(item_indices, 2, replace=False)\n            # Check if swapping is feasible (no weight violation)\n            if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n    else:  # 30% chance for value-based flip\n        # Calculate value density (value1 + value2) / weight for each item\n        value_density = (value1_lst + value2_lst) / weight_lst\n        # Sort items by value density in descending order\n        sorted_items = np.argsort(value_density)[::-1]\n        # Try to flip the most valuable items first\n        for item in sorted_items:\n            if new_solution[item] == 0:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n            else:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    return new_solution\n\n",
        "score": [
            -0.4045771915310815,
            10.611218482255936
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) > 1:\n        # Sort archive by total value (sum of value1 and value2) in descending order\n        archive_sorted = sorted(archive, key=lambda x: x[1][0] + x[1][1], reverse=True)\n        # Select top 30% of solutions and pick one randomly\n        candidate_indices = min(3, len(archive_sorted))  # Ensure at least 1 candidate\n        selected_idx = np.random.randint(0, candidate_indices)\n        base_solution = archive_sorted[selected_idx][0].copy()\n    else:\n        base_solution = archive[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: randomly swap items or flip items based on value density\n    if np.random.rand() < 0.7:  # 70% chance for random swap\n        # Randomly select two items to swap\n        item_indices = np.where(new_solution == 1)[0]\n        if len(item_indices) >= 2:\n            i, j = np.random.choice(item_indices, 2, replace=False)\n            # Check if swapping is feasible (no weight violation)\n            if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n    else:  # 30% chance for value-based flip\n        # Calculate value density (value1 + value2) / weight for each item\n        value_density = (value1_lst + value2_lst) / weight_lst\n        # Sort items by value density in descending order\n        sorted_items = np.argsort(value_density)[::-1]\n        # Try to flip the most valuable items first\n        for item in sorted_items:\n            if new_solution[item] == 0:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n            else:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    return new_solution\n\n",
        "operation": "i1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n1. First, describe the design idea and main steps of your algorithm in one sentence. The description must be inside within boxed {}. \n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 4,
        "algorithm": "The algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n",
        "score": [
            -0.5069312012465736,
            0.26115044951438904
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n1. First, describe the design idea and main steps of your algorithm in one sentence. The description must be inside within boxed {}. \n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 4,
        "algorithm": "The algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n",
        "score": [
            -0.5069312012465736,
            0.26115044951438904
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n",
        "operation": "i1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n1. First, describe the design idea and main steps of your algorithm in one sentence. The description must be inside within boxed {}. \n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 5,
        "algorithm": "The algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.954731742570925,
            0.796350508928299
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n1. First, describe the design idea and main steps of your algorithm in one sentence. The description must be inside within boxed {}. \n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 5,
        "algorithm": "The algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.954731742570925,
            0.796350508928299
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "i1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            \n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 6,
        "algorithm": "The algorithm selects high-performing solutions from the archive (top 30% by normalized objective sum), then applies a three-phase local search: first flipping high value-to-weight ratio items, then probabilistically swapping low-value items for high-value-to-weight items, and finally random diversification if no improvement is found. It ensures feasibility by strict capacity checks at each step, prioritizing items with combined high values and balancing objectives through adaptive selection criteria.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Selection phase: prioritize solutions with high combined normalized objectives\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1\n    normalized_scores = [(obj[0]/max_v1 + obj[1]/max_v2) for _, obj in archive]\n    threshold = np.percentile(normalized_scores, 70)\n    candidates = [sol for (sol, obj), score in zip(archive, normalized_scores) if score >= threshold]\n    base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Local search phase 1: Flip items with high value-to-weight ratio\n    vw_ratio = (value1_lst + value2_lst) / weight_lst\n    sorted_indices = np.argsort(-vw_ratio)\n    for idx in sorted_indices[:max(1, len(sorted_indices)//5)]:\n        if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            break\n\n    # Local search phase 2: Probabilistic swap considering both objectives\n    for _ in range(5):\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            # Select out item with low combined value\n            out_candidates = sorted(out_items, key=lambda x: value1_lst[x] + value2_lst[x])\n            out_idx = out_candidates[0]\n\n            # Select in item with high value-to-weight ratio\n            in_candidates = sorted(in_items, key=lambda x: -(value1_lst[x] + value2_lst[x])/weight_lst[x])\n            in_idx = in_candidates[0] if in_candidates else -1\n\n            if in_idx != -1 and current_weight - weight_lst[out_idx] + weight_lst[in_idx] <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                break\n\n    # Diversification phase: Random valid flip if no improvement found\n    if new_solution.tolist() == base_solution.tolist():\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.3334021766381675,
            5.786891281604767
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Selection phase: prioritize solutions with high combined normalized objectives\n    max_v1 = max(obj[0] for _, obj in archive) if archive else 1\n    max_v2 = max(obj[1] for _, obj in archive) if archive else 1\n    normalized_scores = [(obj[0]/max_v1 + obj[1]/max_v2) for _, obj in archive]\n    threshold = np.percentile(normalized_scores, 70)\n    candidates = [sol for (sol, obj), score in zip(archive, normalized_scores) if score >= threshold]\n    base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Local search phase 1: Flip items with high value-to-weight ratio\n    vw_ratio = (value1_lst + value2_lst) / weight_lst\n    sorted_indices = np.argsort(-vw_ratio)\n    for idx in sorted_indices[:max(1, len(sorted_indices)//5)]:\n        if new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n            break\n\n    # Local search phase 2: Probabilistic swap considering both objectives\n    for _ in range(5):\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            # Select out item with low combined value\n            out_candidates = sorted(out_items, key=lambda x: value1_lst[x] + value2_lst[x])\n            out_idx = out_candidates[0]\n\n            # Select in item with high value-to-weight ratio\n            in_candidates = sorted(in_items, key=lambda x: -(value1_lst[x] + value2_lst[x])/weight_lst[x])\n            in_idx = in_candidates[0] if in_candidates else -1\n\n            if in_idx != -1 and current_weight - weight_lst[out_idx] + weight_lst[in_idx] <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                break\n\n    # Diversification phase: Random valid flip if no improvement found\n    if new_solution.tolist() == base_solution.tolist():\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            \n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 7,
        "algorithm": "The algorithm combines normalized objective prioritization with a dynamic weighted local search, where solutions are first selected based on their combined normalized objective scores, then improved by evaluating single-item flips with weights adjusted to the current solution's balance between objectives, occasionally escaping local optima with random moves while ensuring feasibility. The dynamic weights (weight_v1, weight_v2) bias improvements toward under-represented objectives, and the hybrid strategy balances greedy improvement with exploration.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight between objectives based on current solution's balance\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n    total_v1 = np.sum(value1_lst)\n    total_v2 = np.sum(value2_lst)\n\n    # Calculate dynamic weights (0.5 if balanced, else biased towards under-represented objective)\n    if total_v1 > 0 and total_v2 > 0:\n        weight_v1 = 0.5 * (1 + (current_v1 / total_v1 - current_v2 / total_v2))\n        weight_v2 = 1 - weight_v1\n    else:\n        weight_v1 = weight_v2 = 0.5\n\n    # Hybrid local search strategy:\n    # 1. Evaluate all possible single-item flips\n    # 2. Select the move with the highest weighted improvement\n    # 3. Occasionally perform a random move with probability 0.1\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in range(len(new_solution)):\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Apply the best candidate if found, otherwise perform a random move\n    if best_candidate is not None and (best_improvement > 0 or random.random() < 0.1):\n        new_solution = best_candidate\n    else:\n        # Perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8239236056587971,
            1.9472983479499817
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight between objectives based on current solution's balance\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n    total_v1 = np.sum(value1_lst)\n    total_v2 = np.sum(value2_lst)\n\n    # Calculate dynamic weights (0.5 if balanced, else biased towards under-represented objective)\n    if total_v1 > 0 and total_v2 > 0:\n        weight_v1 = 0.5 * (1 + (current_v1 / total_v1 - current_v2 / total_v2))\n        weight_v2 = 1 - weight_v1\n    else:\n        weight_v1 = weight_v2 = 0.5\n\n    # Hybrid local search strategy:\n    # 1. Evaluate all possible single-item flips\n    # 2. Select the move with the highest weighted improvement\n    # 3. Occasionally perform a random move with probability 0.1\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in range(len(new_solution)):\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Apply the best candidate if found, otherwise perform a random move\n    if best_candidate is not None and (best_improvement > 0 or random.random() < 0.1):\n        new_solution = best_candidate\n    else:\n        # Perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 8,
        "algorithm": "The algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9062541463556129,
            1.3094992935657501
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 9,
        "algorithm": "The algorithm selects promising solutions from the archive based on hypervolume contributions, then applies a dynamic local search that flips items in a subset size proportional to solution quality, using adaptive weights to balance objective improvements. It prioritizes flips that maximize a weighted sum of normalized objective gains while ensuring feasibility, falling back to random valid flips if no improvement is found. The subset size and weights adjust based on solution quality and current objective values, enabling focused exploration of high-potential regions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by hypervolume contribution to prioritize promising ones\n        hypervolumes = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            hypervolumes.append(norm_v1 * norm_v2)  # Hypervolume approximation\n        # Select top 20% of solutions and choose randomly among them\n        threshold = np.percentile(hypervolumes, 80)\n        candidates = [sol for (sol, _), hv in zip(archive, hypervolumes) if hv >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic subset size based on solution quality\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])\n    max_quality = np.sum(value1_lst) + np.sum(value2_lst)\n    subset_size = max(1, int(total_items * (1 - (solution_quality / max_quality)) * 0.5))  # Larger subset for lower quality solutions\n\n    subset_indices = np.random.choice(total_items, size=subset_size, replace=False)\n\n    # Evaluate potential improvements with adaptive weights\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Adaptive weights based on current solution's objective values\n        current_v1 = np.sum(value1_lst[new_solution == 1])\n        current_v2 = np.sum(value2_lst[new_solution == 1])\n        w1 = current_v2 / (current_v1 + current_v2 + 1e-6) if (current_v1 + current_v2) > 0 else 0.5\n        w2 = current_v1 / (current_v1 + current_v2 + 1e-6) if (current_v1 + current_v2) > 0 else 0.5\n\n        improvement = (w1 * delta_v1 + w2 * delta_v2) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9040388126076564,
            1.946214199066162
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by hypervolume contribution to prioritize promising ones\n        hypervolumes = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            hypervolumes.append(norm_v1 * norm_v2)  # Hypervolume approximation\n        # Select top 20% of solutions and choose randomly among them\n        threshold = np.percentile(hypervolumes, 80)\n        candidates = [sol for (sol, _), hv in zip(archive, hypervolumes) if hv >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic subset size based on solution quality\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])\n    max_quality = np.sum(value1_lst) + np.sum(value2_lst)\n    subset_size = max(1, int(total_items * (1 - (solution_quality / max_quality)) * 0.5))  # Larger subset for lower quality solutions\n\n    subset_indices = np.random.choice(total_items, size=subset_size, replace=False)\n\n    # Evaluate potential improvements with adaptive weights\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Adaptive weights based on current solution's objective values\n        current_v1 = np.sum(value1_lst[new_solution == 1])\n        current_v2 = np.sum(value2_lst[new_solution == 1])\n        w1 = current_v2 / (current_v1 + current_v2 + 1e-6) if (current_v1 + current_v2) > 0 else 0.5\n        w2 = current_v1 / (current_v1 + current_v2 + 1e-6) if (current_v1 + current_v2) > 0 else 0.5\n\n        improvement = (w1 * delta_v1 + w2 * delta_v2) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 3 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing those with high objective diversity (via crowding distance) and applies a hybrid local search combining item swaps and subset replacements to generate a feasible neighbor solution while ensuring weight constraints are not violated. It first evaluates solutions based on their crowding distances to identify promising candidates, then performs targeted swaps and subset operations to explore the neighborhood while maintaining feasibility. The algorithm balances exploration and exploitation by focusing on high-diversity regions of the Pareto front while dynamically adjusting the search based on the current solution's characteristics.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high objective diversity and low crowding distance)\n    # Calculate crowding distances for solutions in the archive\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n    objectives_sorted = objectives[sorted_indices]\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance (most promising for improvement)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Randomly swap items if feasible\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Strategy 2: Replace a subset of items with new ones if feasible\n    subset_size = min(3, n_items // 4)\n    if subset_size > 0:\n        # Remove a subset of items\n        remove_indices = np.where(new_solution == 1)[0]\n        if len(remove_indices) > 0:\n            remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Add a subset of new items\n        add_indices = np.where(new_solution == 0)[0]\n        if len(add_indices) > 0:\n            add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: ### Ineffective Self-Reflection\n\n- **Keywords**: Over-reliance, bias, rigidity, suboptimal exploration.\n- **Advice**: Avoid rigid selection strategies; blend crowding-distance with adaptive weighting.\n- **Avoid**: Static neighborhood operators; prioritize dynamic, Pareto-aware local search.\n- **Explanation**: Fixed methods (e.g., swaps-only) limit exploration; hybridize with adaptive subset replacements to escape local optima while preserving feasibility.\n\n*(Word count: 50)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 10,
        "algorithm": "The algorithm dynamically selects a promising solution from the archive using a hybrid of crowding distance and adaptive weighting, then applies a novel local search combining probabilistic swaps (prioritizing high-value-to-weight ratio items) and Pareto-aware subset replacements to generate high-quality neighbors while ensuring feasibility. The selection prioritizes solutions with high crowding distances and normalized objective values, while the local search focuses on swapping items and replacing subsets to explore the solution space effectively.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hybrid selection (crowding distance + adaptive weighting)\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Adaptive weighting: prioritize solutions with high crowding distance and high normalized objectives\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_scores = np.array([(obj[0]/max_v1 if max_v1 > 0 else 0) + (obj[1]/max_v2 if max_v2 > 0 else 0) for obj in objectives])\n    weighted_scores = crowding_distances * normalized_scores\n    selected_idx = np.argmax(weighted_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Novel local search (probabilistic swaps + Pareto-aware subset replacements)\n    n_items = len(weight_lst)\n    subset_size = min(5, n_items // 3)\n\n    # Probabilistic swaps (prioritize high-value-to-weight ratio items)\n    value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    for _ in range(min(10, n_items // 2)):\n        if np.random.random() < 0.7:  # Higher probability for swaps\n            candidates = np.argsort(value_ratios)[-subset_size:]  # Top subset_size items by value ratio\n            i, j = np.random.choice(candidates, 2, replace=False)\n            if new_solution[i] != new_solution[j]:\n                if new_solution[i] == 1 and new_solution[j] == 0:\n                    new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                    if new_weight <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = new_weight\n                elif new_solution[i] == 0 and new_solution[j] == 1:\n                    new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                    if new_weight <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = new_weight\n        else:  # Subset replacement\n            remove_indices = np.where(new_solution == 1)[0]\n            if len(remove_indices) > 0:\n                remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n                new_solution[remove_subset] = 0\n                current_weight -= np.sum(weight_lst[remove_subset])\n\n                add_indices = np.where(new_solution == 0)[0]\n                if len(add_indices) > 0:\n                    add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n                    potential_weight = current_weight + np.sum(weight_lst[add_subset])\n                    if potential_weight <= capacity:\n                        new_solution[add_subset] = 1\n                        current_weight = potential_weight\n\n    return new_solution\n\n",
        "score": [
            -0.7730586733889593,
            1.4795197546482086
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hybrid selection (crowding distance + adaptive weighting)\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Adaptive weighting: prioritize solutions with high crowding distance and high normalized objectives\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_scores = np.array([(obj[0]/max_v1 if max_v1 > 0 else 0) + (obj[1]/max_v2 if max_v2 > 0 else 0) for obj in objectives])\n    weighted_scores = crowding_distances * normalized_scores\n    selected_idx = np.argmax(weighted_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Novel local search (probabilistic swaps + Pareto-aware subset replacements)\n    n_items = len(weight_lst)\n    subset_size = min(5, n_items // 3)\n\n    # Probabilistic swaps (prioritize high-value-to-weight ratio items)\n    value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    for _ in range(min(10, n_items // 2)):\n        if np.random.random() < 0.7:  # Higher probability for swaps\n            candidates = np.argsort(value_ratios)[-subset_size:]  # Top subset_size items by value ratio\n            i, j = np.random.choice(candidates, 2, replace=False)\n            if new_solution[i] != new_solution[j]:\n                if new_solution[i] == 1 and new_solution[j] == 0:\n                    new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                    if new_weight <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = new_weight\n                elif new_solution[i] == 0 and new_solution[j] == 1:\n                    new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                    if new_weight <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = new_weight\n        else:  # Subset replacement\n            remove_indices = np.where(new_solution == 1)[0]\n            if len(remove_indices) > 0:\n                remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n                new_solution[remove_subset] = 0\n                current_weight -= np.sum(weight_lst[remove_subset])\n\n                add_indices = np.where(new_solution == 0)[0]\n                if len(add_indices) > 0:\n                    add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n                    potential_weight = current_weight + np.sum(weight_lst[add_subset])\n                    if potential_weight <= capacity:\n                        new_solution[add_subset] = 1\n                        current_weight = potential_weight\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### Ineffective Self-Reflection\n\n- **Keywords**: Over-reliance, bias, rigidity, suboptimal exploration.\n- **Advice**: Avoid rigid selection strategies; blend crowding-distance with adaptive weighting.\n- **Avoid**: Static neighborhood operators; prioritize dynamic, Pareto-aware local search.\n- **Explanation**: Fixed methods (e.g., swaps-only) limit exploration; hybridize with adaptive subset replacements to escape local optima while preserving feasibility.\n\n*(Word count: 50)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 11,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Calculate crowding distances for solutions in the archive\n        objectives = np.array([obj for _, obj in archive])\n        crowding_distances = []\n        for m in range(2):\n            sorted_indices = np.argsort(objectives[:, m])\n            distances = np.zeros(len(sorted_indices))\n            distances[0] = distances[-1] = float('inf')\n            for i in range(1, len(sorted_indices)-1):\n                distances[i] = objectives[sorted_indices[i+1], m] - objectives[sorted_indices[i-1], m]\n            crowding_distances.append(distances)\n        crowding_distances = np.sum(crowding_distances, axis=0)\n\n        # Select solutions with high crowding distance (promising for improvement)\n        threshold = np.percentile(crowding_distances, 70)\n        candidates = [sol for (sol, _), dist in zip(archive, crowding_distances) if dist >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic Pareto-aware local search\n    # Step 1: Evaluate current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Step 2: Select a subset of items to evaluate based on their potential to improve the Pareto front\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    # Calculate potential improvements for each item\n    potential_improvements = []\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # If item is included, consider removing it\n            delta_v1 = -value1_lst[i]\n            delta_v2 = -value2_lst[i]\n            new_weight = current_weight - weight_lst[i]\n        else:\n            # If item is excluded, consider adding it\n            delta_v1 = value1_lst[i]\n            delta_v2 = value2_lst[i]\n            new_weight = current_weight + weight_lst[i]\n\n        if new_weight > capacity:\n            potential_improvements.append((i, 0, 0))  # No improvement if infeasible\n        else:\n            # Use crowding distance-like metric to prioritize items that can improve the Pareto front\n            improvement = np.sqrt((delta_v1 ** 2) + (delta_v2 ** 2))\n            potential_improvements.append((i, improvement, new_weight))\n\n    # Sort items by potential improvement (descending) and select top subset_size\n    potential_improvements.sort(key=lambda x: -x[1])\n    selected_indices = [i for i, _, _ in potential_improvements[:subset_size]]\n\n    # Step 3: Evaluate flipping each selected item\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in selected_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight and objectives\n        if temp_solution[idx] == 1:\n            new_weight = current_weight + weight_lst[idx]\n        else:\n            new_weight = current_weight - weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a combination of individual improvements and crowding distance-like metric\n        improvement = np.sqrt((delta_v1 ** 2) + (delta_v2 ** 2))\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Fallback: Probabilistic selection based on potential improvements\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential improvements\n            improvements = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                improvement = np.sqrt((potential_v1 ** 2) + (potential_v2 ** 2))\n                improvements.append(improvement)\n            # Select with probability proportional to improvement\n            if sum(improvements) > 0:\n                probabilities = [c / sum(improvements) for c in improvements]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8835753104902517,
            2.8031237721443176
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Calculate crowding distances for solutions in the archive\n        objectives = np.array([obj for _, obj in archive])\n        crowding_distances = []\n        for m in range(2):\n            sorted_indices = np.argsort(objectives[:, m])\n            distances = np.zeros(len(sorted_indices))\n            distances[0] = distances[-1] = float('inf')\n            for i in range(1, len(sorted_indices)-1):\n                distances[i] = objectives[sorted_indices[i+1], m] - objectives[sorted_indices[i-1], m]\n            crowding_distances.append(distances)\n        crowding_distances = np.sum(crowding_distances, axis=0)\n\n        # Select solutions with high crowding distance (promising for improvement)\n        threshold = np.percentile(crowding_distances, 70)\n        candidates = [sol for (sol, _), dist in zip(archive, crowding_distances) if dist >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic Pareto-aware local search\n    # Step 1: Evaluate current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Step 2: Select a subset of items to evaluate based on their potential to improve the Pareto front\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    # Calculate potential improvements for each item\n    potential_improvements = []\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # If item is included, consider removing it\n            delta_v1 = -value1_lst[i]\n            delta_v2 = -value2_lst[i]\n            new_weight = current_weight - weight_lst[i]\n        else:\n            # If item is excluded, consider adding it\n            delta_v1 = value1_lst[i]\n            delta_v2 = value2_lst[i]\n            new_weight = current_weight + weight_lst[i]\n\n        if new_weight > capacity:\n            potential_improvements.append((i, 0, 0))  # No improvement if infeasible\n        else:\n            # Use crowding distance-like metric to prioritize items that can improve the Pareto front\n            improvement = np.sqrt((delta_v1 ** 2) + (delta_v2 ** 2))\n            potential_improvements.append((i, improvement, new_weight))\n\n    # Sort items by potential improvement (descending) and select top subset_size\n    potential_improvements.sort(key=lambda x: -x[1])\n    selected_indices = [i for i, _, _ in potential_improvements[:subset_size]]\n\n    # Step 3: Evaluate flipping each selected item\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in selected_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight and objectives\n        if temp_solution[idx] == 1:\n            new_weight = current_weight + weight_lst[idx]\n        else:\n            new_weight = current_weight - weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a combination of individual improvements and crowding distance-like metric\n        improvement = np.sqrt((delta_v1 ** 2) + (delta_v2 ** 2))\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Fallback: Probabilistic selection based on potential improvements\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential improvements\n            improvements = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                improvement = np.sqrt((potential_v1 ** 2) + (potential_v2 ** 2))\n                improvements.append(improvement)\n            # Select with probability proportional to improvement\n            if sum(improvements) > 0:\n                probabilities = [c / sum(improvements) for c in improvements]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects promising solutions from the archive based on hypervolume contributions, then applies a dynamic local search that flips items in a subset size proportional to solution quality, using adaptive weights to balance objective improvements. It prioritizes flips that maximize a weighted sum of normalized objective gains while ensuring feasibility, falling back to random valid flips if no improvement is found. The subset size and weights adjust based on solution quality and current objective values, enabling focused exploration of high-potential regions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by hypervolume contribution to prioritize promising ones\n        hypervolumes = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            hypervolumes.append(norm_v1 * norm_v2)  # Hypervolume approximation\n        # Select top 20% of solutions and choose randomly among them\n        threshold = np.percentile(hypervolumes, 80)\n        candidates = [sol for (sol, _), hv in zip(archive, hypervolumes) if hv >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic subset size based on solution quality\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])\n    max_quality = np.sum(value1_lst) + np.sum(value2_lst)\n    subset_size = max(1, int(total_items * (1 - (solution_quality / max_quality)) * 0.5))  # Larger subset for lower quality solutions\n\n    subset_indices = np.random.choice(total_items, size=subset_size, replace=False)\n\n    # Evaluate potential improvements with adaptive weights\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Adaptive weights based on current solution's objective values\n        current_v1 = np.sum(value1_lst[new_solution == 1])\n        current_v2 = np.sum(value2_lst[new_solution == 1])\n        w1 = current_v2 / (current_v1 + current_v2 + 1e-6) if (current_v1 + current_v2) > 0 else 0.5\n        w2 = current_v1 / (current_v1 + current_v2 + 1e-6) if (current_v1 + current_v2) > 0 else 0.5\n\n        improvement = (w1 * delta_v1 + w2 * delta_v2) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### Ineffective Self-Reflection\n\n- **Keywords**: Over-reliance, bias, rigidity, suboptimal exploration.\n- **Advice**: Avoid rigid selection strategies; blend crowding-distance with adaptive weighting.\n- **Avoid**: Static neighborhood operators; prioritize dynamic, Pareto-aware local search.\n- **Explanation**: Fixed methods (e.g., swaps-only) limit exploration; hybridize with adaptive subset replacements to escape local optima while preserving feasibility.\n\n*(Word count: 50)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 12,
        "algorithm": "The algorithm combines crowding-distance-based selection with a dynamic local search that alternates between value-driven flips and feasibility-preserving swaps, prioritizing moves that maximize the geometric mean of normalized objective gains while adapting neighborhood size based on solution quality. It ensures feasibility through constrained random walks and dynamically adjusts exploration by balancing objective improvements with solution diversity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Crowding-distance-based selection\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Calculate crowding distances\n        objectives = np.array([obj for _, obj in archive])\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding_dist = np.zeros(len(archive))\n\n        # Crowding distance for objective 1\n        crowding_dist[sorted_indices[0]] = np.inf\n        crowding_dist[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive)-1):\n            crowding_dist[sorted_indices[i]] += (objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]) / (objectives[sorted_indices[-1], 0] - objectives[sorted_indices[0], 0] + 1e-6)\n\n        # Crowding distance for objective 2\n        sorted_indices = np.argsort(objectives[:, 1])\n        crowding_dist[sorted_indices[0]] += np.inf\n        crowding_dist[sorted_indices[-1]] += np.inf\n        for i in range(1, len(archive)-1):\n            crowding_dist[sorted_indices[i]] += (objectives[sorted_indices[i+1], 1] - objectives[sorted_indices[i-1], 1]) / (objectives[sorted_indices[-1], 1] - objectives[sorted_indices[0], 1] + 1e-6)\n\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding_dist)\n        base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality\n    total_items = len(new_solution)\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n    max_v1 = np.sum(value1_lst)\n    max_v2 = np.sum(value2_lst)\n    solution_quality = (current_v1 / max_v1 + current_v2 / max_v2) / 2 if (max_v1 + max_v2) > 0 else 0.5\n    neighborhood_size = max(1, int(total_items * (1 - solution_quality) * 0.7))\n\n    # Alternate between value-driven replacements and feasibility-preserving swaps\n    for _ in range(5):  # Number of attempts\n        # Value-driven subset replacement\n        subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            # Calculate geometric mean of normalized objective gains\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            norm_v1 = delta_v1 / (max_v1 + 1e-6)\n            norm_v2 = delta_v2 / (max_v2 + 1e-6)\n            improvement = (norm_v1 * norm_v2) ** 0.5\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n            continue\n\n        # Feasibility-preserving swap\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n",
        "score": [
            -0.34125688702614876,
            4.628337532281876
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Crowding-distance-based selection\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Calculate crowding distances\n        objectives = np.array([obj for _, obj in archive])\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding_dist = np.zeros(len(archive))\n\n        # Crowding distance for objective 1\n        crowding_dist[sorted_indices[0]] = np.inf\n        crowding_dist[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive)-1):\n            crowding_dist[sorted_indices[i]] += (objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]) / (objectives[sorted_indices[-1], 0] - objectives[sorted_indices[0], 0] + 1e-6)\n\n        # Crowding distance for objective 2\n        sorted_indices = np.argsort(objectives[:, 1])\n        crowding_dist[sorted_indices[0]] += np.inf\n        crowding_dist[sorted_indices[-1]] += np.inf\n        for i in range(1, len(archive)-1):\n            crowding_dist[sorted_indices[i]] += (objectives[sorted_indices[i+1], 1] - objectives[sorted_indices[i-1], 1]) / (objectives[sorted_indices[-1], 1] - objectives[sorted_indices[0], 1] + 1e-6)\n\n        # Select solution with highest crowding distance\n        selected_idx = np.argmax(crowding_dist)\n        base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality\n    total_items = len(new_solution)\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n    max_v1 = np.sum(value1_lst)\n    max_v2 = np.sum(value2_lst)\n    solution_quality = (current_v1 / max_v1 + current_v2 / max_v2) / 2 if (max_v1 + max_v2) > 0 else 0.5\n    neighborhood_size = max(1, int(total_items * (1 - solution_quality) * 0.7))\n\n    # Alternate between value-driven replacements and feasibility-preserving swaps\n    for _ in range(5):  # Number of attempts\n        # Value-driven subset replacement\n        subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            # Calculate geometric mean of normalized objective gains\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            norm_v1 = delta_v1 / (max_v1 + 1e-6)\n            norm_v2 = delta_v2 / (max_v2 + 1e-6)\n            improvement = (norm_v1 * norm_v2) ** 0.5\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n            continue\n\n        # Feasibility-preserving swap\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 13,
        "algorithm": "The algorithm selects a high-potential solution from the archive (top 30% by combined value) and applies a probabilistic local search: with a dynamic temperature, it either swaps high-value-to-weight excluded items with low-value-to-weight included ones (ensuring feasibility) or probabilistically flips included items based on their value-to-weight ratios. The temperature decreases over iterations to balance exploration and exploitation, prioritizing items with better combined ratios while maintaining feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high potential (top 30% by combined value)\n    combined_values = np.array([v1 + v2 for _, (v1, v2) in archive])\n    top_indices = np.argsort(combined_values)[-max(1, len(archive) // 3):]\n    selected_idx = random.choice(top_indices)\n    selected_solution, _ = archive[selected_idx]\n    new_solution = selected_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic temperature for probabilistic decisions\n    temperature = max(0.1, 1.0 - (selected_idx / len(archive)))\n\n    # Calculate value-to-weight ratios for all items\n    vw_ratio1 = value1_lst / (weight_lst + 1e-6)\n    vw_ratio2 = value2_lst / (weight_lst + 1e-6)\n    combined_ratio = vw_ratio1 + vw_ratio2\n\n    # Probabilistic item selection based on ratios\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for _ in range(15):  # More attempts for better exploration\n        # With probability based on temperature, either:\n        if random.random() < temperature:\n            # 1. Swap a high-value excluded item with a low-value included item\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                # Select highest ratio excluded item\n                best_excluded = excluded_items[np.argmax(combined_ratio[excluded_items])]\n                # Select lowest ratio included item\n                worst_included = included_items[np.argmin(combined_ratio[included_items])]\n\n                # Check feasibility\n                delta_weight = weight_lst[best_excluded] - weight_lst[worst_included]\n                if current_weight + delta_weight <= capacity:\n                    new_solution[worst_included] = 0\n                    new_solution[best_excluded] = 1\n                    current_weight += delta_weight\n                    included_items = np.where(new_solution == 1)[0]\n                    excluded_items = np.where(new_solution == 0)[0]\n        else:\n            # 2. Flip a random item with probability based on its ratio\n            if len(included_items) > 0:\n                flip_idx = random.choice(included_items)\n                if random.random() < (combined_ratio[flip_idx] / (np.max(combined_ratio) + 1e-6)):\n                    new_weight = current_weight - weight_lst[flip_idx]\n                    if new_weight <= capacity:\n                        new_solution[flip_idx] = 0\n                        current_weight = new_weight\n                        included_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n",
        "score": [
            -0.2922728895699793,
            6.638999044895172
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution with high potential (top 30% by combined value)\n    combined_values = np.array([v1 + v2 for _, (v1, v2) in archive])\n    top_indices = np.argsort(combined_values)[-max(1, len(archive) // 3):]\n    selected_idx = random.choice(top_indices)\n    selected_solution, _ = archive[selected_idx]\n    new_solution = selected_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic temperature for probabilistic decisions\n    temperature = max(0.1, 1.0 - (selected_idx / len(archive)))\n\n    # Calculate value-to-weight ratios for all items\n    vw_ratio1 = value1_lst / (weight_lst + 1e-6)\n    vw_ratio2 = value2_lst / (weight_lst + 1e-6)\n    combined_ratio = vw_ratio1 + vw_ratio2\n\n    # Probabilistic item selection based on ratios\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    for _ in range(15):  # More attempts for better exploration\n        # With probability based on temperature, either:\n        if random.random() < temperature:\n            # 1. Swap a high-value excluded item with a low-value included item\n            if len(included_items) > 0 and len(excluded_items) > 0:\n                # Select highest ratio excluded item\n                best_excluded = excluded_items[np.argmax(combined_ratio[excluded_items])]\n                # Select lowest ratio included item\n                worst_included = included_items[np.argmin(combined_ratio[included_items])]\n\n                # Check feasibility\n                delta_weight = weight_lst[best_excluded] - weight_lst[worst_included]\n                if current_weight + delta_weight <= capacity:\n                    new_solution[worst_included] = 0\n                    new_solution[best_excluded] = 1\n                    current_weight += delta_weight\n                    included_items = np.where(new_solution == 1)[0]\n                    excluded_items = np.where(new_solution == 0)[0]\n        else:\n            # 2. Flip a random item with probability based on its ratio\n            if len(included_items) > 0:\n                flip_idx = random.choice(included_items)\n                if random.random() < (combined_ratio[flip_idx] / (np.max(combined_ratio) + 1e-6)):\n                    new_weight = current_weight - weight_lst[flip_idx]\n                    if new_weight <= capacity:\n                        new_solution[flip_idx] = 0\n                        current_weight = new_weight\n                        included_items = np.where(new_solution == 1)[0]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 14,
        "algorithm": "The algorithm selects the most crowded solution from the archive (promising for improvement) and applies a multi-objective local search combining probabilistic flips (weighted by value-to-weight ratios) and item swaps prioritizing Pareto-dominance, ensuring feasibility through 20 attempts. It focuses on improving both objectives by flipping high-value-to-weight items and swapping items that improve both objectives simultaneously.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select the most crowded solution (promising for improvement)\n    if len(archive) > 1:\n        # Calculate crowding distances\n        objectives = np.array([obj for _, obj in archive])\n        normalized = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0))\n        distances = np.zeros(len(archive))\n        for i in range(2):  # For each objective\n            sorted_idx = np.argsort(normalized[:, i])\n            distances[sorted_idx[0]] = np.inf\n            distances[sorted_idx[-1]] = np.inf\n            for j in range(1, len(archive)-1):\n                distances[sorted_idx[j]] += normalized[sorted_idx[j+1], i] - normalized[sorted_idx[j-1], i]\n        selected_idx = np.argmax(distances)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_value1 = np.sum(value1_lst[new_solution == 1])\n    current_value2 = np.sum(value2_lst[new_solution == 1])\n\n    for _ in range(20):  # More attempts for better exploration\n        # Probabilistic flip with value-to-weight ratio weighting\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            vw_ratios = (value1_lst[flip_candidates] + value2_lst[flip_candidates]) / weight_lst[flip_candidates]\n            probs = vw_ratios / np.sum(vw_ratios)\n            flip_idx = np.random.choice(flip_candidates, p=probs)\n            new_weight = current_weight - weight_lst[flip_idx]\n            if new_weight <= capacity:\n                new_solution[flip_idx] = 0\n                current_weight = new_weight\n\n        # Multi-objective swap: prioritize items improving both objectives\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            # Calculate potential improvements\n            out_v1 = value1_lst[out_items]\n            out_v2 = value2_lst[out_items]\n            in_v1 = value1_lst[in_items]\n            in_v2 = value2_lst[in_items]\n\n            # Find swaps that improve both objectives\n            improvements = (in_v1[:, None] - out_v1) * (in_v2[:, None] - out_v2)\n            valid_swaps = improvements > 0\n\n            if np.any(valid_swaps):\n                # Select the most promising swap\n                swap_idx = np.unravel_index(np.argmax(improvements), improvements.shape)\n                in_idx = in_items[swap_idx[0]]\n                out_idx = out_items[swap_idx[1]]\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    new_solution[out_idx] = 0\n                    new_solution[in_idx] = 1\n                    current_weight = new_weight\n                    break\n\n    return new_solution\n\n",
        "score": [
            -0.8329998167664839,
            1.6450177431106567
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select the most crowded solution (promising for improvement)\n    if len(archive) > 1:\n        # Calculate crowding distances\n        objectives = np.array([obj for _, obj in archive])\n        normalized = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0))\n        distances = np.zeros(len(archive))\n        for i in range(2):  # For each objective\n            sorted_idx = np.argsort(normalized[:, i])\n            distances[sorted_idx[0]] = np.inf\n            distances[sorted_idx[-1]] = np.inf\n            for j in range(1, len(archive)-1):\n                distances[sorted_idx[j]] += normalized[sorted_idx[j+1], i] - normalized[sorted_idx[j-1], i]\n        selected_idx = np.argmax(distances)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_value1 = np.sum(value1_lst[new_solution == 1])\n    current_value2 = np.sum(value2_lst[new_solution == 1])\n\n    for _ in range(20):  # More attempts for better exploration\n        # Probabilistic flip with value-to-weight ratio weighting\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            vw_ratios = (value1_lst[flip_candidates] + value2_lst[flip_candidates]) / weight_lst[flip_candidates]\n            probs = vw_ratios / np.sum(vw_ratios)\n            flip_idx = np.random.choice(flip_candidates, p=probs)\n            new_weight = current_weight - weight_lst[flip_idx]\n            if new_weight <= capacity:\n                new_solution[flip_idx] = 0\n                current_weight = new_weight\n\n        # Multi-objective swap: prioritize items improving both objectives\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            # Calculate potential improvements\n            out_v1 = value1_lst[out_items]\n            out_v2 = value2_lst[out_items]\n            in_v1 = value1_lst[in_items]\n            in_v2 = value2_lst[in_items]\n\n            # Find swaps that improve both objectives\n            improvements = (in_v1[:, None] - out_v1) * (in_v2[:, None] - out_v2)\n            valid_swaps = improvements > 0\n\n            if np.any(valid_swaps):\n                # Select the most promising swap\n                swap_idx = np.unravel_index(np.argmax(improvements), improvements.shape)\n                in_idx = in_items[swap_idx[0]]\n                out_idx = out_items[swap_idx[1]]\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    new_solution[out_idx] = 0\n                    new_solution[in_idx] = 1\n                    current_weight = new_weight\n                    break\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 3 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing those with high objective diversity (via crowding distance) and applies a hybrid local search combining item swaps and subset replacements to generate a feasible neighbor solution while ensuring weight constraints are not violated. It first evaluates solutions based on their crowding distances to identify promising candidates, then performs targeted swaps and subset operations to explore the neighborhood while maintaining feasibility. The algorithm balances exploration and exploitation by focusing on high-diversity regions of the Pareto front while dynamically adjusting the search based on the current solution's characteristics.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high objective diversity and low crowding distance)\n    # Calculate crowding distances for solutions in the archive\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n    objectives_sorted = objectives[sorted_indices]\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance (most promising for improvement)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Randomly swap items if feasible\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Strategy 2: Replace a subset of items with new ones if feasible\n    subset_size = min(3, n_items // 4)\n    if subset_size > 0:\n        # Remove a subset of items\n        remove_indices = np.where(new_solution == 1)[0]\n        if len(remove_indices) > 0:\n            remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Add a subset of new items\n        add_indices = np.where(new_solution == 0)[0]\n        if len(add_indices) > 0:\n            add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: - **Keywords**: Adaptive selection, hybrid operators, Pareto-aware, dynamic feasibility\n- **Advice**: Combine crowding-distance selection with objective-weighted improvements, using adaptive subset flips to balance exploration and exploitation.\n- **Avoid**: Static neighborhood operators; prioritize dynamic, feasibility-preserving local search.\n- **Explanation**: Dynamic operators (e.g., weighted subset swaps + feasibility checks) escape local optima while ensuring Pareto efficiency.\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 15,
        "algorithm": "The algorithm selects a promising solution from the archive using a weighted combination of crowding distance and normalized objective values, then applies a dynamic hybrid local search that adaptively flips subsets of items based on Pareto-aware improvement criteria (prioritizing value1 over value2 with a 70/30 weighting) while ensuring feasibility. It balances exploration (random valid flips when no improvement) with exploitation (targeted subset flips for potential gains).",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a promising solution using weighted crowding distance and objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_obj = objectives / np.array([max_v1, max_v2]) if max_v1 > 0 and max_v2 > 0 else objectives\n\n    # Calculate crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance with normalized objectives for selection\n    selection_scores = crowding_distances + np.sum(normalized_obj, axis=1)\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Dynamic hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    max_attempts = min(10, n_items)\n\n    for _ in range(max_attempts):\n        # Adaptive subset selection based on current solution's characteristics\n        subset_size = max(1, int(n_items * 0.15))  # 15% of items\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = -np.inf\n        best_candidate = None\n\n        for idx in subset_indices:\n            # Evaluate flip\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            # Check feasibility\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            # Calculate Pareto-aware improvement\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement (balance both objectives)\n            improvement = 0.7 * delta_v1 + 0.3 * delta_v2  # Adjust weights as needed\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n        else:\n            # If no improvement, perform a random valid flip\n            valid_indices = [i for i in range(n_items) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n                current_weight = np.sum(weight_lst[new_solution == 1])\n\n    return new_solution\n\n",
        "score": [
            -0.3569431002273198,
            4.758812844753265
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a promising solution using weighted crowding distance and objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_obj = objectives / np.array([max_v1, max_v2]) if max_v1 > 0 and max_v2 > 0 else objectives\n\n    # Calculate crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance with normalized objectives for selection\n    selection_scores = crowding_distances + np.sum(normalized_obj, axis=1)\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Dynamic hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    max_attempts = min(10, n_items)\n\n    for _ in range(max_attempts):\n        # Adaptive subset selection based on current solution's characteristics\n        subset_size = max(1, int(n_items * 0.15))  # 15% of items\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = -np.inf\n        best_candidate = None\n\n        for idx in subset_indices:\n            # Evaluate flip\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            # Check feasibility\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            # Calculate Pareto-aware improvement\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement (balance both objectives)\n            improvement = 0.7 * delta_v1 + 0.3 * delta_v2  # Adjust weights as needed\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n        else:\n            # If no improvement, perform a random valid flip\n            valid_indices = [i for i in range(n_items) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n                current_weight = np.sum(weight_lst[new_solution == 1])\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing those with high objective diversity (via crowding distance) and applies a hybrid local search combining item swaps and subset replacements to generate a feasible neighbor solution while ensuring weight constraints are not violated. It first evaluates solutions based on their crowding distances to identify promising candidates, then performs targeted swaps and subset operations to explore the neighborhood while maintaining feasibility. The algorithm balances exploration and exploitation by focusing on high-diversity regions of the Pareto front while dynamically adjusting the search based on the current solution's characteristics.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high objective diversity and low crowding distance)\n    # Calculate crowding distances for solutions in the archive\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n    objectives_sorted = objectives[sorted_indices]\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance (most promising for improvement)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Randomly swap items if feasible\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Strategy 2: Replace a subset of items with new ones if feasible\n    subset_size = min(3, n_items // 4)\n    if subset_size > 0:\n        # Remove a subset of items\n        remove_indices = np.where(new_solution == 1)[0]\n        if len(remove_indices) > 0:\n            remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Add a subset of new items\n        add_indices = np.where(new_solution == 0)[0]\n        if len(add_indices) > 0:\n            add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Adaptive selection, hybrid operators, Pareto-aware, dynamic feasibility\n- **Advice**: Combine crowding-distance selection with objective-weighted improvements, using adaptive subset flips to balance exploration and exploitation.\n- **Avoid**: Static neighborhood operators; prioritize dynamic, feasibility-preserving local search.\n- **Explanation**: Dynamic operators (e.g., weighted subset swaps + feasibility checks) escape local optima while ensuring Pareto efficiency.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 16,
        "algorithm": "The heuristic algorithm selects a solution from the archive with high crowding distance to focus on under-explored regions of the Pareto front, then applies a dynamic local search that adaptively flips item subsets based on weighted improvements in both objectives while ensuring feasibility through iterative checks and adaptive subset adjustments. The method balances exploration and exploitation by dynamically adjusting subset size and flip strategy, prioritizing objective improvements with adaptive weights that depend on the current solution's density.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high crowding distance to explore under-represented regions\n    objectives = np.array([obj for _, obj in archive])\n    n_solutions = len(archive)\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(n_solutions)\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, n_solutions - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Adaptive subset flip with dynamic size and Pareto-aware improvements\n    n_items = len(weight_lst)\n    max_subset_size = min(5, n_items // 3)  # Dynamic subset size\n    subset_size = np.random.randint(1, max_subset_size + 1)\n\n    # Evaluate potential improvements for both objectives\n    candidate_improvements = []\n    for _ in range(min(10, n_items)):\n        # Randomly select a subset of items\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n        temp_solution = new_solution.copy()\n        temp_solution[subset_indices] = 1 - temp_solution[subset_indices]\n\n        # Calculate new weight\n        new_weight = current_weight\n        for idx in subset_indices:\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = np.sum((temp_solution[subset_indices] - new_solution[subset_indices]) * value1_lst[subset_indices])\n        delta_v2 = np.sum((temp_solution[subset_indices] - new_solution[subset_indices]) * value2_lst[subset_indices])\n\n        # Weighted improvement (adaptive weights based on current solution's characteristics)\n        weight_v1 = 0.5 if np.sum(new_solution) < n_items / 2 else 0.3\n        weight_v2 = 1 - weight_v1\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        candidate_improvements.append((improvement, temp_solution))\n\n    # Apply the best candidate if found\n    if candidate_improvements:\n        best_improvement, best_solution = max(candidate_improvements, key=lambda x: x[0])\n        if best_improvement > 0:\n            return best_solution\n\n    # If no improvement found, perform a random feasible flip\n    valid_indices = [i for i in range(n_items) if\n                    (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                    (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n    if valid_indices:\n        idx = np.random.choice(valid_indices)\n        new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.6119106592899944,
            1.6910967230796814
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high crowding distance to explore under-represented regions\n    objectives = np.array([obj for _, obj in archive])\n    n_solutions = len(archive)\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(n_solutions)\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, n_solutions - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Adaptive subset flip with dynamic size and Pareto-aware improvements\n    n_items = len(weight_lst)\n    max_subset_size = min(5, n_items // 3)  # Dynamic subset size\n    subset_size = np.random.randint(1, max_subset_size + 1)\n\n    # Evaluate potential improvements for both objectives\n    candidate_improvements = []\n    for _ in range(min(10, n_items)):\n        # Randomly select a subset of items\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n        temp_solution = new_solution.copy()\n        temp_solution[subset_indices] = 1 - temp_solution[subset_indices]\n\n        # Calculate new weight\n        new_weight = current_weight\n        for idx in subset_indices:\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = np.sum((temp_solution[subset_indices] - new_solution[subset_indices]) * value1_lst[subset_indices])\n        delta_v2 = np.sum((temp_solution[subset_indices] - new_solution[subset_indices]) * value2_lst[subset_indices])\n\n        # Weighted improvement (adaptive weights based on current solution's characteristics)\n        weight_v1 = 0.5 if np.sum(new_solution) < n_items / 2 else 0.3\n        weight_v2 = 1 - weight_v1\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        candidate_improvements.append((improvement, temp_solution))\n\n    # Apply the best candidate if found\n    if candidate_improvements:\n        best_improvement, best_solution = max(candidate_improvements, key=lambda x: x[0])\n        if best_improvement > 0:\n            return best_solution\n\n    # If no improvement found, perform a random feasible flip\n    valid_indices = [i for i in range(n_items) if\n                    (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                    (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n    if valid_indices:\n        idx = np.random.choice(valid_indices)\n        new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects promising solutions from the archive based on hypervolume contributions, then applies a dynamic local search that flips items in a subset size proportional to solution quality, using adaptive weights to balance objective improvements. It prioritizes flips that maximize a weighted sum of normalized objective gains while ensuring feasibility, falling back to random valid flips if no improvement is found. The subset size and weights adjust based on solution quality and current objective values, enabling focused exploration of high-potential regions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by hypervolume contribution to prioritize promising ones\n        hypervolumes = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            hypervolumes.append(norm_v1 * norm_v2)  # Hypervolume approximation\n        # Select top 20% of solutions and choose randomly among them\n        threshold = np.percentile(hypervolumes, 80)\n        candidates = [sol for (sol, _), hv in zip(archive, hypervolumes) if hv >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic subset size based on solution quality\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])\n    max_quality = np.sum(value1_lst) + np.sum(value2_lst)\n    subset_size = max(1, int(total_items * (1 - (solution_quality / max_quality)) * 0.5))  # Larger subset for lower quality solutions\n\n    subset_indices = np.random.choice(total_items, size=subset_size, replace=False)\n\n    # Evaluate potential improvements with adaptive weights\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Adaptive weights based on current solution's objective values\n        current_v1 = np.sum(value1_lst[new_solution == 1])\n        current_v2 = np.sum(value2_lst[new_solution == 1])\n        w1 = current_v2 / (current_v1 + current_v2 + 1e-6) if (current_v1 + current_v2) > 0 else 0.5\n        w2 = current_v1 / (current_v1 + current_v2 + 1e-6) if (current_v1 + current_v2) > 0 else 0.5\n\n        improvement = (w1 * delta_v1 + w2 * delta_v2) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing those with high objective diversity (via crowding distance) and applies a hybrid local search combining item swaps and subset replacements to generate a feasible neighbor solution while ensuring weight constraints are not violated. It first evaluates solutions based on their crowding distances to identify promising candidates, then performs targeted swaps and subset operations to explore the neighborhood while maintaining feasibility. The algorithm balances exploration and exploitation by focusing on high-diversity regions of the Pareto front while dynamically adjusting the search based on the current solution's characteristics.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high objective diversity and low crowding distance)\n    # Calculate crowding distances for solutions in the archive\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n    objectives_sorted = objectives[sorted_indices]\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance (most promising for improvement)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Randomly swap items if feasible\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Strategy 2: Replace a subset of items with new ones if feasible\n    subset_size = min(3, n_items // 4)\n    if subset_size > 0:\n        # Remove a subset of items\n        remove_indices = np.where(new_solution == 1)[0]\n        if len(remove_indices) > 0:\n            remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Add a subset of new items\n        add_indices = np.where(new_solution == 0)[0]\n        if len(add_indices) > 0:\n            add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            - **Keywords**: Adaptive selection, hybrid operators, Pareto-aware, dynamic feasibility\n- **Advice**: Combine crowding-distance selection with objective-weighted improvements, using adaptive subset flips to balance exploration and exploitation.\n- **Avoid**: Static neighborhood operators; prioritize dynamic, feasibility-preserving local search.\n- **Explanation**: Dynamic operators (e.g., weighted subset swaps + feasibility checks) escape local optima while ensuring Pareto efficiency.\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 17,
        "algorithm": "The algorithm combines Pareto-aware selection with an adaptive, feasibility-preserving local search that prioritizes items offering balanced improvements in both objectives, dynamically adjusting flip weights and subset sizes based on solution quality and crowding distance. It first selects a solution with high diversity (via crowding distance) from the archive, then applies weighted subset flips to generate neighbors, ensuring feasibility by validating all moves against capacity constraints. The operator balances exploration and exploitation by favoring items that improve both objectives proportionally to their weights, falling back to random valid flips when no improvement is found.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high crowding distance and objective diversity)\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n\n    # Compute crowding distances for each objective\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using adaptive weighted subset flips\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Calculate solution quality and adaptive weights\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n    w1 = current_v2 / (current_v1 + current_v2 + 1e-6) if (current_v1 + current_v2) > 0 else 0.5\n    w2 = current_v1 / (current_v1 + current_v2 + 1e-6) if (current_v1 + current_v2) > 0 else 0.5\n\n    # Dynamic subset size based on solution quality\n    total_items = len(new_solution)\n    solution_quality = current_v1 + current_v2\n    max_quality = np.sum(value1_lst) + np.sum(value2_lst)\n    subset_size = max(1, int(total_items * (1 - (solution_quality / max_quality)) * 0.5))\n\n    # Select random subset of items to flip\n    subset_indices = np.random.choice(total_items, size=subset_size, replace=False)\n\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = (w1 * delta_v1 + w2 * delta_v2) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.7712015042339431,
            1.748958259820938
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high crowding distance and objective diversity)\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n\n    # Compute crowding distances for each objective\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using adaptive weighted subset flips\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Calculate solution quality and adaptive weights\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n    w1 = current_v2 / (current_v1 + current_v2 + 1e-6) if (current_v1 + current_v2) > 0 else 0.5\n    w2 = current_v1 / (current_v1 + current_v2 + 1e-6) if (current_v1 + current_v2) > 0 else 0.5\n\n    # Dynamic subset size based on solution quality\n    total_items = len(new_solution)\n    solution_quality = current_v1 + current_v2\n    max_quality = np.sum(value1_lst) + np.sum(value2_lst)\n    subset_size = max(1, int(total_items * (1 - (solution_quality / max_quality)) * 0.5))\n\n    # Select random subset of items to flip\n    subset_indices = np.random.choice(total_items, size=subset_size, replace=False)\n\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = (w1 * delta_v1 + w2 * delta_v2) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm intelligently selects a solution from the archive by prioritizing those with high objective diversity (via crowding distance) and applies a hybrid local search combining item swaps and subset replacements to generate a feasible neighbor solution while ensuring weight constraints are not violated. It first evaluates solutions based on their crowding distances to identify promising candidates, then performs targeted swaps and subset operations to explore the neighborhood while maintaining feasibility. The algorithm balances exploration and exploitation by focusing on high-diversity regions of the Pareto front while dynamically adjusting the search based on the current solution's characteristics.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high objective diversity and low crowding distance)\n    # Calculate crowding distances for solutions in the archive\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n    objectives_sorted = objectives[sorted_indices]\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance (most promising for improvement)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Randomly swap items if feasible\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Strategy 2: Replace a subset of items with new ones if feasible\n    subset_size = min(3, n_items // 4)\n    if subset_size > 0:\n        # Remove a subset of items\n        remove_indices = np.where(new_solution == 1)[0]\n        if len(remove_indices) > 0:\n            remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Add a subset of new items\n        add_indices = np.where(new_solution == 0)[0]\n        if len(add_indices) > 0:\n            add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 18,
        "algorithm": "The algorithm selects a promising solution from the archive using a combined score of normalized objective values and diversity, then applies a hybrid local search combining targeted item swaps (prioritizing value-dense items) and probabilistic subset replacements (weighted by objective-aware densities) to generate a feasible neighbor solution while maintaining capacity constraints. It dynamically adjusts the solution by first swapping items based on weight differences and then probabilistically replacing subsets of items to balance both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Calculate combined scores for selection\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0) + 1e-10)\n    diversity_scores = np.sum(normalized_obj, axis=1)\n    combined_scores = diversity_scores * np.prod(normalized_obj, axis=1)\n    selected_idx = np.argmax(combined_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Targeted item swaps with value-based prioritization\n    n_items = len(weight_lst)\n    swap_candidates = np.where(new_solution == 1)[0]\n    if len(swap_candidates) > 0:\n        for _ in range(min(3, len(swap_candidates))):\n            i = np.random.choice(swap_candidates)\n            j = np.random.choice(n_items)\n            if new_solution[j] == 0:\n                weight_diff = weight_lst[j] - weight_lst[i]\n                if weight_diff <= 0 or (current_weight + weight_diff) <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight += weight_diff\n                    if weight_diff > 0:\n                        swap_candidates = np.append(swap_candidates, j)\n                    swap_candidates = swap_candidates[swap_candidates != i]\n\n    # Step 3: Probabilistic subset replacement with objective-aware selection\n    subset_size = min(2, n_items // 5)\n    if subset_size > 0:\n        # Calculate value densities for each objective\n        value_density1 = value1_lst / (weight_lst + 1e-10)\n        value_density2 = value2_lst / (weight_lst + 1e-10)\n\n        # Select items to remove based on combined density\n        remove_candidates = np.where(new_solution == 1)[0]\n        if len(remove_candidates) > 0:\n            combined_density = value_density1 + value_density2\n            remove_probs = combined_density[remove_candidates] / np.sum(combined_density[remove_candidates])\n            remove_subset = np.random.choice(remove_candidates, size=min(subset_size, len(remove_candidates)), p=remove_probs, replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Select items to add based on weighted probability\n        add_candidates = np.where(new_solution == 0)[0]\n        if len(add_candidates) > 0:\n            obj_weights = np.random.uniform(0.3, 0.7)\n            combined_density = obj_weights * value_density1 + (1 - obj_weights) * value_density2\n            add_probs = combined_density[add_candidates] / np.sum(combined_density[add_candidates])\n            add_subset = np.random.choice(add_candidates, size=min(subset_size, len(add_candidates)), p=add_probs, replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n",
        "score": [
            -0.5399179665523841,
            0.9202896356582642
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Calculate combined scores for selection\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0) + 1e-10)\n    diversity_scores = np.sum(normalized_obj, axis=1)\n    combined_scores = diversity_scores * np.prod(normalized_obj, axis=1)\n    selected_idx = np.argmax(combined_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Targeted item swaps with value-based prioritization\n    n_items = len(weight_lst)\n    swap_candidates = np.where(new_solution == 1)[0]\n    if len(swap_candidates) > 0:\n        for _ in range(min(3, len(swap_candidates))):\n            i = np.random.choice(swap_candidates)\n            j = np.random.choice(n_items)\n            if new_solution[j] == 0:\n                weight_diff = weight_lst[j] - weight_lst[i]\n                if weight_diff <= 0 or (current_weight + weight_diff) <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight += weight_diff\n                    if weight_diff > 0:\n                        swap_candidates = np.append(swap_candidates, j)\n                    swap_candidates = swap_candidates[swap_candidates != i]\n\n    # Step 3: Probabilistic subset replacement with objective-aware selection\n    subset_size = min(2, n_items // 5)\n    if subset_size > 0:\n        # Calculate value densities for each objective\n        value_density1 = value1_lst / (weight_lst + 1e-10)\n        value_density2 = value2_lst / (weight_lst + 1e-10)\n\n        # Select items to remove based on combined density\n        remove_candidates = np.where(new_solution == 1)[0]\n        if len(remove_candidates) > 0:\n            combined_density = value_density1 + value_density2\n            remove_probs = combined_density[remove_candidates] / np.sum(combined_density[remove_candidates])\n            remove_subset = np.random.choice(remove_candidates, size=min(subset_size, len(remove_candidates)), p=remove_probs, replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Select items to add based on weighted probability\n        add_candidates = np.where(new_solution == 0)[0]\n        if len(add_candidates) > 0:\n            obj_weights = np.random.uniform(0.3, 0.7)\n            combined_density = obj_weights * value_density1 + (1 - obj_weights) * value_density2\n            add_probs = combined_density[add_candidates] / np.sum(combined_density[add_candidates])\n            add_subset = np.random.choice(add_candidates, size=min(subset_size, len(add_candidates)), p=add_probs, replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm intelligently selects a solution from the archive by prioritizing those with high objective diversity (via crowding distance) and applies a hybrid local search combining item swaps and subset replacements to generate a feasible neighbor solution while ensuring weight constraints are not violated. It first evaluates solutions based on their crowding distances to identify promising candidates, then performs targeted swaps and subset operations to explore the neighborhood while maintaining feasibility. The algorithm balances exploration and exploitation by focusing on high-diversity regions of the Pareto front while dynamically adjusting the search based on the current solution's characteristics.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high objective diversity and low crowding distance)\n    # Calculate crowding distances for solutions in the archive\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n    objectives_sorted = objectives[sorted_indices]\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance (most promising for improvement)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Randomly swap items if feasible\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Strategy 2: Replace a subset of items with new ones if feasible\n    subset_size = min(3, n_items // 4)\n    if subset_size > 0:\n        # Remove a subset of items\n        remove_indices = np.where(new_solution == 1)[0]\n        if len(remove_indices) > 0:\n            remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Add a subset of new items\n        add_indices = np.where(new_solution == 0)[0]\n        if len(add_indices) > 0:\n            add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 19,
        "algorithm": "The algorithm selects promising solutions from the archive by combining dominance and density scores, then applies a dynamic local search that adaptively flips items based on value ratios and weight constraints, followed by targeted subset replacements to improve both objectives while ensuring feasibility. It prioritizes solutions with high dominance (non-dominated status) and low density (diversity) in the archive, and uses weight-aware perturbations and value-based item swaps to generate high-quality neighbors.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution combining dominance and density\n    objectives = np.array([obj for _, obj in archive])\n    solutions = np.array([sol for sol, _ in archive])\n\n    # Calculate dominance scores (higher is better)\n    dominance_scores = np.zeros(len(archive))\n    for i in range(len(archive)):\n        dominated = 0\n        for j in range(len(archive)):\n            if i != j:\n                if (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] > objectives[i, 1]) or \\\n                   (objectives[j, 0] > objectives[i, 0] and objectives[j, 1] >= objectives[i, 1]):\n                    dominated += 1\n        dominance_scores[i] = 1 / (1 + dominated)\n\n    # Calculate density scores (lower is better)\n    density_scores = np.zeros(len(archive))\n    for i in range(len(archive)):\n        distances = np.sum(np.abs(solutions - solutions[i]), axis=1)\n        density_scores[i] = np.mean(distances)\n\n    # Combine scores (weighted sum)\n    combined_scores = 0.6 * dominance_scores + 0.4 * (1 / (1 + density_scores))\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Dynamic local search with adaptive perturbations\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Adaptive item flips based on value ratios\n    flip_prob = 0.3 + 0.7 * (current_weight / capacity)  # Higher probability when knapsack is full\n    for i in range(n_items):\n        if np.random.random() < flip_prob:\n            if new_solution[i] == 1:\n                new_weight = current_weight - weight_lst[i]\n                if new_weight >= 0:\n                    new_solution[i] = 0\n                    current_weight = new_weight\n            else:\n                new_weight = current_weight + weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i] = 1\n                    current_weight = new_weight\n\n    # Targeted subset replacement based on value dominance\n    subset_size = max(1, int(0.1 * n_items))\n    if subset_size > 0:\n        # Remove low-value items\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) > 0:\n            remove_values = (value1_lst[included_items] + value2_lst[included_items]) / 2\n            remove_indices = included_items[np.argsort(remove_values)[:subset_size]]\n            new_solution[remove_indices] = 0\n            current_weight -= np.sum(weight_lst[remove_indices])\n\n        # Add high-value items\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(excluded_items) > 0:\n            add_values = (value1_lst[excluded_items] + value2_lst[excluded_items]) / 2\n            add_indices = excluded_items[np.argsort(add_values)[-subset_size:]]\n            potential_weight = current_weight + np.sum(weight_lst[add_indices])\n            if potential_weight <= capacity:\n                new_solution[add_indices] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n",
        "score": [
            -0.5561362416144122,
            2.502936005592346
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution combining dominance and density\n    objectives = np.array([obj for _, obj in archive])\n    solutions = np.array([sol for sol, _ in archive])\n\n    # Calculate dominance scores (higher is better)\n    dominance_scores = np.zeros(len(archive))\n    for i in range(len(archive)):\n        dominated = 0\n        for j in range(len(archive)):\n            if i != j:\n                if (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] > objectives[i, 1]) or \\\n                   (objectives[j, 0] > objectives[i, 0] and objectives[j, 1] >= objectives[i, 1]):\n                    dominated += 1\n        dominance_scores[i] = 1 / (1 + dominated)\n\n    # Calculate density scores (lower is better)\n    density_scores = np.zeros(len(archive))\n    for i in range(len(archive)):\n        distances = np.sum(np.abs(solutions - solutions[i]), axis=1)\n        density_scores[i] = np.mean(distances)\n\n    # Combine scores (weighted sum)\n    combined_scores = 0.6 * dominance_scores + 0.4 * (1 / (1 + density_scores))\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Dynamic local search with adaptive perturbations\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Adaptive item flips based on value ratios\n    flip_prob = 0.3 + 0.7 * (current_weight / capacity)  # Higher probability when knapsack is full\n    for i in range(n_items):\n        if np.random.random() < flip_prob:\n            if new_solution[i] == 1:\n                new_weight = current_weight - weight_lst[i]\n                if new_weight >= 0:\n                    new_solution[i] = 0\n                    current_weight = new_weight\n            else:\n                new_weight = current_weight + weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i] = 1\n                    current_weight = new_weight\n\n    # Targeted subset replacement based on value dominance\n    subset_size = max(1, int(0.1 * n_items))\n    if subset_size > 0:\n        # Remove low-value items\n        included_items = np.where(new_solution == 1)[0]\n        if len(included_items) > 0:\n            remove_values = (value1_lst[included_items] + value2_lst[included_items]) / 2\n            remove_indices = included_items[np.argsort(remove_values)[:subset_size]]\n            new_solution[remove_indices] = 0\n            current_weight -= np.sum(weight_lst[remove_indices])\n\n        # Add high-value items\n        excluded_items = np.where(new_solution == 0)[0]\n        if len(excluded_items) > 0:\n            add_values = (value1_lst[excluded_items] + value2_lst[excluded_items]) / 2\n            add_indices = excluded_items[np.argsort(add_values)[-subset_size:]]\n            potential_weight = current_weight + np.sum(weight_lst[add_indices])\n            if potential_weight <= capacity:\n                new_solution[add_indices] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 3 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing those with high objective diversity (via crowding distance) and applies a hybrid local search combining item swaps and subset replacements to generate a feasible neighbor solution while ensuring weight constraints are not violated. It first evaluates solutions based on their crowding distances to identify promising candidates, then performs targeted swaps and subset operations to explore the neighborhood while maintaining feasibility. The algorithm balances exploration and exploitation by focusing on high-diversity regions of the Pareto front while dynamically adjusting the search based on the current solution's characteristics.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high objective diversity and low crowding distance)\n    # Calculate crowding distances for solutions in the archive\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n    objectives_sorted = objectives[sorted_indices]\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance (most promising for improvement)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Randomly swap items if feasible\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Strategy 2: Replace a subset of items with new ones if feasible\n    subset_size = min(3, n_items // 4)\n    if subset_size > 0:\n        # Remove a subset of items\n        remove_indices = np.where(new_solution == 1)[0]\n        if len(remove_indices) > 0:\n            remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Add a subset of new items\n        add_indices = np.where(new_solution == 0)[0]\n        if len(add_indices) > 0:\n            add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s a refined, step-by-step approach to designing an effective heuristic for the BI-KP:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware exploration**, **feasibility-preserving operators**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset replacements** (e.g., flipping clusters of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 20,
        "algorithm": "This algorithm selects a solution from the archive using a weighted crowding-distance metric that prioritizes high-diversity regions, then applies an adaptive hybrid local search combining subset flips and objective-weighted swaps, dynamically adjusting neighborhood size and ensuring feasibility through precomputed weight checks. It emphasizes balanced improvement across both objectives by weighting improvements based on their combined value and relative weight impact, while maintaining feasibility through careful capacity checks. The selection process favors solutions with both high crowding distance and objective values, while the local search prioritizes flips and swaps that maximize the combined value-to-weight ratio.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution using weighted crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    n_solutions = len(archive)\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(n_solutions)\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, n_solutions - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Weighted selection: prioritize high crowding distance and high objective values\n    weights = crowding_distances * (objectives[:, 0] + objectives[:, 1])\n    selected_idx = np.argmax(weights)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Adaptive hybrid local search\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(2, n_items // 10))  # Dynamic adjustment\n\n    # Strategy 1: Subset flips with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(3, n_items // 4)\n        if subset_size <= 0:\n            break\n\n        # Select a random subset of items\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n        temp_solution = new_solution.copy()\n        temp_weight = current_weight\n\n        # Evaluate flips in the subset\n        best_improvement = 0\n        best_flip = None\n\n        for idx in subset_indices:\n            if temp_solution[idx] == 1:\n                delta_weight = -weight_lst[idx]\n                delta_v1 = -value1_lst[idx]\n                delta_v2 = -value2_lst[idx]\n            else:\n                delta_weight = weight_lst[idx]\n                delta_v1 = value1_lst[idx]\n                delta_v2 = value2_lst[idx]\n\n            new_weight = temp_weight + delta_weight\n            if new_weight > capacity:\n                continue\n\n            # Weighted improvement (prioritize both objectives)\n            improvement = (delta_v1 + delta_v2) / (abs(delta_weight) + 1e-6)\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_flip = idx\n\n        # Apply the best flip if found\n        if best_flip is not None:\n            new_solution[best_flip] = 1 - new_solution[best_flip]\n            current_weight += delta_weight if new_solution[best_flip] == 1 else -delta_weight\n\n    # Strategy 2: Objective-weighted swaps\n    for _ in range(neighborhood_size // 2):\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = np.random.choice(out_items)\n            in_idx = np.random.choice(in_items)\n\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                # Weighted swap decision\n                delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                improvement = (delta_v1 + delta_v2) / (abs(weight_lst[in_idx] - weight_lst[out_idx]) + 1e-6)\n\n                if improvement > 0 or np.random.random() < 0.3:  # Allow some randomness\n                    new_solution[out_idx] = 0\n                    new_solution[in_idx] = 1\n                    current_weight = new_weight\n\n    return new_solution\n\n",
        "score": [
            -0.7039260139205756,
            3.6923465728759766
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution using weighted crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    n_solutions = len(archive)\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(n_solutions)\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, n_solutions - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Weighted selection: prioritize high crowding distance and high objective values\n    weights = crowding_distances * (objectives[:, 0] + objectives[:, 1])\n    selected_idx = np.argmax(weights)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Adaptive hybrid local search\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(2, n_items // 10))  # Dynamic adjustment\n\n    # Strategy 1: Subset flips with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(3, n_items // 4)\n        if subset_size <= 0:\n            break\n\n        # Select a random subset of items\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n        temp_solution = new_solution.copy()\n        temp_weight = current_weight\n\n        # Evaluate flips in the subset\n        best_improvement = 0\n        best_flip = None\n\n        for idx in subset_indices:\n            if temp_solution[idx] == 1:\n                delta_weight = -weight_lst[idx]\n                delta_v1 = -value1_lst[idx]\n                delta_v2 = -value2_lst[idx]\n            else:\n                delta_weight = weight_lst[idx]\n                delta_v1 = value1_lst[idx]\n                delta_v2 = value2_lst[idx]\n\n            new_weight = temp_weight + delta_weight\n            if new_weight > capacity:\n                continue\n\n            # Weighted improvement (prioritize both objectives)\n            improvement = (delta_v1 + delta_v2) / (abs(delta_weight) + 1e-6)\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_flip = idx\n\n        # Apply the best flip if found\n        if best_flip is not None:\n            new_solution[best_flip] = 1 - new_solution[best_flip]\n            current_weight += delta_weight if new_solution[best_flip] == 1 else -delta_weight\n\n    # Strategy 2: Objective-weighted swaps\n    for _ in range(neighborhood_size // 2):\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = np.random.choice(out_items)\n            in_idx = np.random.choice(in_items)\n\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                # Weighted swap decision\n                delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                improvement = (delta_v1 + delta_v2) / (abs(weight_lst[in_idx] - weight_lst[out_idx]) + 1e-6)\n\n                if improvement > 0 or np.random.random() < 0.3:  # Allow some randomness\n                    new_solution[out_idx] = 0\n                    new_solution[in_idx] = 1\n                    current_weight = new_weight\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe heuristic algorithm selects a solution from the archive with high crowding distance to focus on under-explored regions of the Pareto front, then applies a dynamic local search that adaptively flips item subsets based on weighted improvements in both objectives while ensuring feasibility through iterative checks and adaptive subset adjustments. The method balances exploration and exploitation by dynamically adjusting subset size and flip strategy, prioritizing objective improvements with adaptive weights that depend on the current solution's density.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high crowding distance to explore under-represented regions\n    objectives = np.array([obj for _, obj in archive])\n    n_solutions = len(archive)\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(n_solutions)\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, n_solutions - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Adaptive subset flip with dynamic size and Pareto-aware improvements\n    n_items = len(weight_lst)\n    max_subset_size = min(5, n_items // 3)  # Dynamic subset size\n    subset_size = np.random.randint(1, max_subset_size + 1)\n\n    # Evaluate potential improvements for both objectives\n    candidate_improvements = []\n    for _ in range(min(10, n_items)):\n        # Randomly select a subset of items\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n        temp_solution = new_solution.copy()\n        temp_solution[subset_indices] = 1 - temp_solution[subset_indices]\n\n        # Calculate new weight\n        new_weight = current_weight\n        for idx in subset_indices:\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = np.sum((temp_solution[subset_indices] - new_solution[subset_indices]) * value1_lst[subset_indices])\n        delta_v2 = np.sum((temp_solution[subset_indices] - new_solution[subset_indices]) * value2_lst[subset_indices])\n\n        # Weighted improvement (adaptive weights based on current solution's characteristics)\n        weight_v1 = 0.5 if np.sum(new_solution) < n_items / 2 else 0.3\n        weight_v2 = 1 - weight_v1\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        candidate_improvements.append((improvement, temp_solution))\n\n    # Apply the best candidate if found\n    if candidate_improvements:\n        best_improvement, best_solution = max(candidate_improvements, key=lambda x: x[0])\n        if best_improvement > 0:\n            return best_solution\n\n    # If no improvement found, perform a random feasible flip\n    valid_indices = [i for i in range(n_items) if\n                    (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                    (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n    if valid_indices:\n        idx = np.random.choice(valid_indices)\n        new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s a refined, step-by-step approach to designing an effective heuristic for the BI-KP:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware exploration**, **feasibility-preserving operators**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset replacements** (e.g., flipping clusters of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 21,
        "algorithm": "The algorithm selects a promising solution from the archive using a weighted combination of normalized objectives and crowding distance, then applies an adaptive clustered flip operator that evaluates random item clusters, dynamically adjusts weights based on their contribution to both objectives, and ensures feasibility by precomputing item contributions and checking weight constraints. It prioritizes solutions with high crowding distance (70%) and normalized objective values (30%), and uses adaptive weights for objective improvements to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution using weighted combination of normalized objectives and crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    n_solutions = len(archive)\n\n    # Compute normalized objectives\n    max_v1 = max(obj[0] for _, obj in archive)\n    max_v2 = max(obj[1] for _, obj in archive)\n    norm_objectives = np.zeros((n_solutions, 2))\n    for i, obj in enumerate(objectives):\n        norm_objectives[i, 0] = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_objectives[i, 1] = obj[1] / max_v2 if max_v2 > 0 else 0\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(n_solutions)\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, n_solutions - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Weighted selection (70% crowding, 30% normalized objectives)\n    combined_scores = 0.7 * crowding_distances + 0.3 * np.sum(norm_objectives, axis=1)\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Adaptive clustered flip operator\n    n_items = len(weight_lst)\n    max_cluster_size = min(5, n_items // 4)\n    cluster_size = np.random.randint(1, max_cluster_size + 1)\n\n    # Precompute item contributions\n    contributions = np.zeros(n_items)\n    for i in range(n_items):\n        contributions[i] = (value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-6)\n\n    # Generate candidate clusters\n    candidate_clusters = []\n    for _ in range(min(10, n_items)):\n        cluster_indices = np.random.choice(n_items, size=cluster_size, replace=False)\n        temp_solution = new_solution.copy()\n        temp_solution[cluster_indices] = 1 - temp_solution[cluster_indices]\n\n        # Calculate new weight\n        new_weight = current_weight\n        for idx in cluster_indices:\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = np.sum((temp_solution[cluster_indices] - new_solution[cluster_indices]) * value1_lst[cluster_indices])\n        delta_v2 = np.sum((temp_solution[cluster_indices] - new_solution[cluster_indices]) * value2_lst[cluster_indices])\n\n        # Weighted improvement (adaptive weights based on cluster contribution)\n        total_contribution = np.sum(contributions[cluster_indices])\n        weight_v1 = 0.5 if total_contribution < np.mean(contributions) else 0.3\n        weight_v2 = 1 - weight_v1\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        candidate_clusters.append((improvement, temp_solution))\n\n    # Apply best cluster if found\n    if candidate_clusters:\n        best_improvement, best_solution = max(candidate_clusters, key=lambda x: x[0])\n        if best_improvement > 0:\n            return best_solution\n\n    # Fallback: Random feasible flip\n    valid_indices = [i for i in range(n_items) if\n                    (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                    (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n    if valid_indices:\n        idx = np.random.choice(valid_indices)\n        new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.5925760651487451,
            2.119412899017334
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution using weighted combination of normalized objectives and crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    n_solutions = len(archive)\n\n    # Compute normalized objectives\n    max_v1 = max(obj[0] for _, obj in archive)\n    max_v2 = max(obj[1] for _, obj in archive)\n    norm_objectives = np.zeros((n_solutions, 2))\n    for i, obj in enumerate(objectives):\n        norm_objectives[i, 0] = obj[0] / max_v1 if max_v1 > 0 else 0\n        norm_objectives[i, 1] = obj[1] / max_v2 if max_v2 > 0 else 0\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(n_solutions)\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, n_solutions - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Weighted selection (70% crowding, 30% normalized objectives)\n    combined_scores = 0.7 * crowding_distances + 0.3 * np.sum(norm_objectives, axis=1)\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Adaptive clustered flip operator\n    n_items = len(weight_lst)\n    max_cluster_size = min(5, n_items // 4)\n    cluster_size = np.random.randint(1, max_cluster_size + 1)\n\n    # Precompute item contributions\n    contributions = np.zeros(n_items)\n    for i in range(n_items):\n        contributions[i] = (value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-6)\n\n    # Generate candidate clusters\n    candidate_clusters = []\n    for _ in range(min(10, n_items)):\n        cluster_indices = np.random.choice(n_items, size=cluster_size, replace=False)\n        temp_solution = new_solution.copy()\n        temp_solution[cluster_indices] = 1 - temp_solution[cluster_indices]\n\n        # Calculate new weight\n        new_weight = current_weight\n        for idx in cluster_indices:\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = np.sum((temp_solution[cluster_indices] - new_solution[cluster_indices]) * value1_lst[cluster_indices])\n        delta_v2 = np.sum((temp_solution[cluster_indices] - new_solution[cluster_indices]) * value2_lst[cluster_indices])\n\n        # Weighted improvement (adaptive weights based on cluster contribution)\n        total_contribution = np.sum(contributions[cluster_indices])\n        weight_v1 = 0.5 if total_contribution < np.mean(contributions) else 0.3\n        weight_v2 = 1 - weight_v1\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        candidate_clusters.append((improvement, temp_solution))\n\n    # Apply best cluster if found\n    if candidate_clusters:\n        best_improvement, best_solution = max(candidate_clusters, key=lambda x: x[0])\n        if best_improvement > 0:\n            return best_solution\n\n    # Fallback: Random feasible flip\n    valid_indices = [i for i in range(n_items) if\n                    (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                    (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n    if valid_indices:\n        idx = np.random.choice(valid_indices)\n        new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a combined score of normalized objective values and diversity, then applies a hybrid local search combining targeted item swaps (prioritizing value-dense items) and probabilistic subset replacements (weighted by objective-aware densities) to generate a feasible neighbor solution while maintaining capacity constraints. It dynamically adjusts the solution by first swapping items based on weight differences and then probabilistically replacing subsets of items to balance both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Calculate combined scores for selection\n    objectives = np.array([obj for _, obj in archive])\n    normalized_obj = (objectives - np.min(objectives, axis=0)) / (np.max(objectives, axis=0) - np.min(objectives, axis=0) + 1e-10)\n    diversity_scores = np.sum(normalized_obj, axis=1)\n    combined_scores = diversity_scores * np.prod(normalized_obj, axis=1)\n    selected_idx = np.argmax(combined_scores)\n\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Targeted item swaps with value-based prioritization\n    n_items = len(weight_lst)\n    swap_candidates = np.where(new_solution == 1)[0]\n    if len(swap_candidates) > 0:\n        for _ in range(min(3, len(swap_candidates))):\n            i = np.random.choice(swap_candidates)\n            j = np.random.choice(n_items)\n            if new_solution[j] == 0:\n                weight_diff = weight_lst[j] - weight_lst[i]\n                if weight_diff <= 0 or (current_weight + weight_diff) <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight += weight_diff\n                    if weight_diff > 0:\n                        swap_candidates = np.append(swap_candidates, j)\n                    swap_candidates = swap_candidates[swap_candidates != i]\n\n    # Step 3: Probabilistic subset replacement with objective-aware selection\n    subset_size = min(2, n_items // 5)\n    if subset_size > 0:\n        # Calculate value densities for each objective\n        value_density1 = value1_lst / (weight_lst + 1e-10)\n        value_density2 = value2_lst / (weight_lst + 1e-10)\n\n        # Select items to remove based on combined density\n        remove_candidates = np.where(new_solution == 1)[0]\n        if len(remove_candidates) > 0:\n            combined_density = value_density1 + value_density2\n            remove_probs = combined_density[remove_candidates] / np.sum(combined_density[remove_candidates])\n            remove_subset = np.random.choice(remove_candidates, size=min(subset_size, len(remove_candidates)), p=remove_probs, replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Select items to add based on weighted probability\n        add_candidates = np.where(new_solution == 0)[0]\n        if len(add_candidates) > 0:\n            obj_weights = np.random.uniform(0.3, 0.7)\n            combined_density = obj_weights * value_density1 + (1 - obj_weights) * value_density2\n            add_probs = combined_density[add_candidates] / np.sum(combined_density[add_candidates])\n            add_subset = np.random.choice(add_candidates, size=min(subset_size, len(add_candidates)), p=add_probs, replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe heuristic algorithm selects a solution from the archive with high crowding distance to focus on under-explored regions of the Pareto front, then applies a dynamic local search that adaptively flips item subsets based on weighted improvements in both objectives while ensuring feasibility through iterative checks and adaptive subset adjustments. The method balances exploration and exploitation by dynamically adjusting subset size and flip strategy, prioritizing objective improvements with adaptive weights that depend on the current solution's density.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high crowding distance to explore under-represented regions\n    objectives = np.array([obj for _, obj in archive])\n    n_solutions = len(archive)\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(n_solutions)\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, n_solutions - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Adaptive subset flip with dynamic size and Pareto-aware improvements\n    n_items = len(weight_lst)\n    max_subset_size = min(5, n_items // 3)  # Dynamic subset size\n    subset_size = np.random.randint(1, max_subset_size + 1)\n\n    # Evaluate potential improvements for both objectives\n    candidate_improvements = []\n    for _ in range(min(10, n_items)):\n        # Randomly select a subset of items\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n        temp_solution = new_solution.copy()\n        temp_solution[subset_indices] = 1 - temp_solution[subset_indices]\n\n        # Calculate new weight\n        new_weight = current_weight\n        for idx in subset_indices:\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = np.sum((temp_solution[subset_indices] - new_solution[subset_indices]) * value1_lst[subset_indices])\n        delta_v2 = np.sum((temp_solution[subset_indices] - new_solution[subset_indices]) * value2_lst[subset_indices])\n\n        # Weighted improvement (adaptive weights based on current solution's characteristics)\n        weight_v1 = 0.5 if np.sum(new_solution) < n_items / 2 else 0.3\n        weight_v2 = 1 - weight_v1\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        candidate_improvements.append((improvement, temp_solution))\n\n    # Apply the best candidate if found\n    if candidate_improvements:\n        best_improvement, best_solution = max(candidate_improvements, key=lambda x: x[0])\n        if best_improvement > 0:\n            return best_solution\n\n    # If no improvement found, perform a random feasible flip\n    valid_indices = [i for i in range(n_items) if\n                    (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                    (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n    if valid_indices:\n        idx = np.random.choice(valid_indices)\n        new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s a refined, step-by-step approach to designing an effective heuristic for the BI-KP:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware exploration**, **feasibility-preserving operators**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset replacements** (e.g., flipping clusters of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 22,
        "algorithm": "The algorithm selects a solution from the archive with the highest weighted crowding distance (prioritizing boundary solutions), then applies a hybrid local search combining cluster-based flips (grouping similar items) and objective-aware swaps (probabilistically swapping items based on value density), dynamically adjusting cluster sizes and swap probabilities based on solution quality and objective contributions while ensuring feasibility through weight checks. The method balances exploration and exploitation by focusing on promising regions of the Pareto front and using adaptive strategies to improve both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Calculate weighted crowding distances\n    objectives = np.array([obj for _, obj in archive])\n    n_solutions = len(archive)\n    crowding_distances = np.zeros(n_solutions)\n\n    # Assign dynamic weights based on solution position\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        for i in range(n_solutions):\n            if i == 0 or i == n_solutions - 1:\n                crowding_distances[i] = np.inf\n            else:\n                # Dynamic weight: higher for solutions near extremes\n                weight = 1.0 if i < n_solutions // 3 or i > 2 * n_solutions // 3 else 0.5\n                crowding_distances[i] += weight * (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx] + 1e-10)\n\n    # Select solution with highest weighted crowding distance\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Cluster-based adaptive flips\n    n_items = len(weight_lst)\n    max_cluster_size = min(4, n_items // 4)\n    cluster_size = np.random.randint(1, max_cluster_size + 1)\n\n    # Form clusters based on objective similarity\n    if n_items > 0:\n        # Normalize values for similarity calculation\n        norm_value1 = (value1_lst - np.min(value1_lst)) / (np.max(value1_lst) - np.min(value1_lst) + 1e-10)\n        norm_value2 = (value2_lst - np.min(value2_lst)) / (np.max(value2_lst) - np.min(value2_lst) + 1e-10)\n\n        # Compute similarity matrix\n        similarity = np.sqrt((norm_value1[:, None] - norm_value1[None, :])**2 + (norm_value2[:, None] - norm_value2[None, :])**2)\n\n        # Greedily select clusters\n        remaining_items = np.where(new_solution == 1)[0]\n        for _ in range(min(3, len(remaining_items))):\n            if len(remaining_items) == 0:\n                break\n            # Select a seed item\n            seed = np.random.choice(remaining_items)\n            # Find similar items\n            similar_items = np.argsort(similarity[seed])[:cluster_size]\n            # Flip the cluster\n            flip_mask = np.isin(np.arange(n_items), similar_items)\n            new_solution[flip_mask] = 1 - new_solution[flip_mask]\n\n            # Update weight and check feasibility\n            weight_change = np.sum((2 * new_solution[flip_mask] - 1) * weight_lst[flip_mask])\n            if current_weight + weight_change > capacity:\n                # Revert if infeasible\n                new_solution[flip_mask] = 1 - new_solution[flip_mask]\n            else:\n                current_weight += weight_change\n                remaining_items = remaining_items[~np.isin(remaining_items, similar_items)]\n\n    # Step 3: Objective-aware probabilistic swaps\n    swap_prob = 0.7 if crowding_distances[selected_idx] > np.median(crowding_distances) else 0.4\n    if np.random.rand() < swap_prob:\n        # Calculate objective weights based on solution's position\n        obj_weight = 0.6 if objectives[selected_idx][0] > np.median(objectives[:, 0]) else 0.4\n\n        # Compute value densities\n        value_density1 = value1_lst / (weight_lst + 1e-10)\n        value_density2 = value2_lst / (weight_lst + 1e-10)\n        combined_density = obj_weight * value_density1 + (1 - obj_weight) * value_density2\n\n        # Select items to swap based on density\n        included = np.where(new_solution == 1)[0]\n        excluded = np.where(new_solution == 0)[0]\n\n        if len(included) > 0 and len(excluded) > 0:\n            # Select item to remove\n            remove_probs = combined_density[included] / np.sum(combined_density[included])\n            i = np.random.choice(included, p=remove_probs)\n\n            # Select item to add\n            add_probs = combined_density[excluded] / np.sum(combined_density[excluded])\n            j = np.random.choice(excluded, p=add_probs)\n\n            # Check feasibility\n            weight_diff = weight_lst[j] - weight_lst[i]\n            if weight_diff <= 0 or (current_weight + weight_diff) <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight += weight_diff\n\n    return new_solution\n\n",
        "score": [
            -0.5336593222222337,
            3.1698358356952667
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Calculate weighted crowding distances\n    objectives = np.array([obj for _, obj in archive])\n    n_solutions = len(archive)\n    crowding_distances = np.zeros(n_solutions)\n\n    # Assign dynamic weights based on solution position\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        for i in range(n_solutions):\n            if i == 0 or i == n_solutions - 1:\n                crowding_distances[i] = np.inf\n            else:\n                # Dynamic weight: higher for solutions near extremes\n                weight = 1.0 if i < n_solutions // 3 or i > 2 * n_solutions // 3 else 0.5\n                crowding_distances[i] += weight * (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx] + 1e-10)\n\n    # Select solution with highest weighted crowding distance\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Cluster-based adaptive flips\n    n_items = len(weight_lst)\n    max_cluster_size = min(4, n_items // 4)\n    cluster_size = np.random.randint(1, max_cluster_size + 1)\n\n    # Form clusters based on objective similarity\n    if n_items > 0:\n        # Normalize values for similarity calculation\n        norm_value1 = (value1_lst - np.min(value1_lst)) / (np.max(value1_lst) - np.min(value1_lst) + 1e-10)\n        norm_value2 = (value2_lst - np.min(value2_lst)) / (np.max(value2_lst) - np.min(value2_lst) + 1e-10)\n\n        # Compute similarity matrix\n        similarity = np.sqrt((norm_value1[:, None] - norm_value1[None, :])**2 + (norm_value2[:, None] - norm_value2[None, :])**2)\n\n        # Greedily select clusters\n        remaining_items = np.where(new_solution == 1)[0]\n        for _ in range(min(3, len(remaining_items))):\n            if len(remaining_items) == 0:\n                break\n            # Select a seed item\n            seed = np.random.choice(remaining_items)\n            # Find similar items\n            similar_items = np.argsort(similarity[seed])[:cluster_size]\n            # Flip the cluster\n            flip_mask = np.isin(np.arange(n_items), similar_items)\n            new_solution[flip_mask] = 1 - new_solution[flip_mask]\n\n            # Update weight and check feasibility\n            weight_change = np.sum((2 * new_solution[flip_mask] - 1) * weight_lst[flip_mask])\n            if current_weight + weight_change > capacity:\n                # Revert if infeasible\n                new_solution[flip_mask] = 1 - new_solution[flip_mask]\n            else:\n                current_weight += weight_change\n                remaining_items = remaining_items[~np.isin(remaining_items, similar_items)]\n\n    # Step 3: Objective-aware probabilistic swaps\n    swap_prob = 0.7 if crowding_distances[selected_idx] > np.median(crowding_distances) else 0.4\n    if np.random.rand() < swap_prob:\n        # Calculate objective weights based on solution's position\n        obj_weight = 0.6 if objectives[selected_idx][0] > np.median(objectives[:, 0]) else 0.4\n\n        # Compute value densities\n        value_density1 = value1_lst / (weight_lst + 1e-10)\n        value_density2 = value2_lst / (weight_lst + 1e-10)\n        combined_density = obj_weight * value_density1 + (1 - obj_weight) * value_density2\n\n        # Select items to swap based on density\n        included = np.where(new_solution == 1)[0]\n        excluded = np.where(new_solution == 0)[0]\n\n        if len(included) > 0 and len(excluded) > 0:\n            # Select item to remove\n            remove_probs = combined_density[included] / np.sum(combined_density[included])\n            i = np.random.choice(included, p=remove_probs)\n\n            # Select item to add\n            add_probs = combined_density[excluded] / np.sum(combined_density[excluded])\n            j = np.random.choice(excluded, p=add_probs)\n\n            # Check feasibility\n            weight_diff = weight_lst[j] - weight_lst[i]\n            if weight_diff <= 0 or (current_weight + weight_diff) <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                current_weight += weight_diff\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The heuristic algorithm selects a solution from the archive with high crowding distance to focus on under-explored regions of the Pareto front, then applies a dynamic local search that adaptively flips item subsets based on weighted improvements in both objectives while ensuring feasibility through iterative checks and adaptive subset adjustments. The method balances exploration and exploitation by dynamically adjusting subset size and flip strategy, prioritizing objective improvements with adaptive weights that depend on the current solution's density.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high crowding distance to explore under-represented regions\n    objectives = np.array([obj for _, obj in archive])\n    n_solutions = len(archive)\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(n_solutions)\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, n_solutions - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Adaptive subset flip with dynamic size and Pareto-aware improvements\n    n_items = len(weight_lst)\n    max_subset_size = min(5, n_items // 3)  # Dynamic subset size\n    subset_size = np.random.randint(1, max_subset_size + 1)\n\n    # Evaluate potential improvements for both objectives\n    candidate_improvements = []\n    for _ in range(min(10, n_items)):\n        # Randomly select a subset of items\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n        temp_solution = new_solution.copy()\n        temp_solution[subset_indices] = 1 - temp_solution[subset_indices]\n\n        # Calculate new weight\n        new_weight = current_weight\n        for idx in subset_indices:\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = np.sum((temp_solution[subset_indices] - new_solution[subset_indices]) * value1_lst[subset_indices])\n        delta_v2 = np.sum((temp_solution[subset_indices] - new_solution[subset_indices]) * value2_lst[subset_indices])\n\n        # Weighted improvement (adaptive weights based on current solution's characteristics)\n        weight_v1 = 0.5 if np.sum(new_solution) < n_items / 2 else 0.3\n        weight_v2 = 1 - weight_v1\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        candidate_improvements.append((improvement, temp_solution))\n\n    # Apply the best candidate if found\n    if candidate_improvements:\n        best_improvement, best_solution = max(candidate_improvements, key=lambda x: x[0])\n        if best_improvement > 0:\n            return best_solution\n\n    # If no improvement found, perform a random feasible flip\n    valid_indices = [i for i in range(n_items) if\n                    (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                    (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n    if valid_indices:\n        idx = np.random.choice(valid_indices)\n        new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 23,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid of crowding distance and objective-space exploration, then applies a dynamic flip strategy that adaptively prioritizes items based on their marginal contributions to both objectives while ensuring feasibility through iterative weight checks and a novel 'criticality' metric. The criticality metric balances exploration and exploitation by dynamically adjusting flip probabilities based on item contributions to the Pareto front, with higher-priority items being more likely to be flipped. The algorithm ensures feasibility by validating weight constraints before applying flips and falls back to random feasible flips if no improvements are made.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hybrid selection combining crowding distance and objective-space exploration\n    objectives = np.array([obj for _, obj in archive])\n    n_solutions = len(archive)\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(n_solutions)\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, n_solutions - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine with objective-space exploration factor\n    objective_exploration = np.zeros(n_solutions)\n    for i in range(n_solutions):\n        objective_exploration[i] = np.prod([objectives[i, j] / np.max(objectives[:, j]) for j in range(2)])\n\n    selection_scores = crowding_distances * objective_exploration\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Dynamic flip strategy with criticality metric\n    n_items = len(weight_lst)\n    max_flips = min(5, n_items // 2)\n\n    # Calculate item criticality (balance of contributions to both objectives)\n    item_criticality = np.zeros(n_items)\n    for i in range(n_items):\n        if base_solution[i] == 1:\n            # For included items, criticality is based on relative contribution\n            v1_contrib = value1_lst[i] / np.sum(value1_lst[base_solution == 1])\n            v2_contrib = value2_lst[i] / np.sum(value2_lst[base_solution == 1])\n            item_criticality[i] = np.sqrt(v1_contrib * v2_contrib)  # Geometric mean\n        else:\n            # For excluded items, criticality is based on potential contribution\n            if current_weight + weight_lst[i] <= capacity:\n                v1_ratio = value1_lst[i] / np.mean(value1_lst)\n                v2_ratio = value2_lst[i] / np.mean(value2_lst)\n                item_criticality[i] = np.max([v1_ratio, v2_ratio])  # Max to prioritize better items\n\n    # Select items to flip based on criticality and adaptive probability\n    flip_probabilities = item_criticality / np.sum(item_criticality)\n    flip_candidates = np.random.choice(n_items, size=max_flips, p=flip_probabilities, replace=False)\n\n    # Apply flips while maintaining feasibility\n    for idx in flip_candidates:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight <= capacity:\n            new_solution = temp_solution\n            current_weight = new_weight\n\n    # If no flips made, perform a random feasible flip\n    if np.array_equal(new_solution, base_solution):\n        valid_indices = [i for i in range(n_items) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = np.random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.820804228048671,
            4.179685354232788
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hybrid selection combining crowding distance and objective-space exploration\n    objectives = np.array([obj for _, obj in archive])\n    n_solutions = len(archive)\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(n_solutions)\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, n_solutions - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine with objective-space exploration factor\n    objective_exploration = np.zeros(n_solutions)\n    for i in range(n_solutions):\n        objective_exploration[i] = np.prod([objectives[i, j] / np.max(objectives[:, j]) for j in range(2)])\n\n    selection_scores = crowding_distances * objective_exploration\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Dynamic flip strategy with criticality metric\n    n_items = len(weight_lst)\n    max_flips = min(5, n_items // 2)\n\n    # Calculate item criticality (balance of contributions to both objectives)\n    item_criticality = np.zeros(n_items)\n    for i in range(n_items):\n        if base_solution[i] == 1:\n            # For included items, criticality is based on relative contribution\n            v1_contrib = value1_lst[i] / np.sum(value1_lst[base_solution == 1])\n            v2_contrib = value2_lst[i] / np.sum(value2_lst[base_solution == 1])\n            item_criticality[i] = np.sqrt(v1_contrib * v2_contrib)  # Geometric mean\n        else:\n            # For excluded items, criticality is based on potential contribution\n            if current_weight + weight_lst[i] <= capacity:\n                v1_ratio = value1_lst[i] / np.mean(value1_lst)\n                v2_ratio = value2_lst[i] / np.mean(value2_lst)\n                item_criticality[i] = np.max([v1_ratio, v2_ratio])  # Max to prioritize better items\n\n    # Select items to flip based on criticality and adaptive probability\n    flip_probabilities = item_criticality / np.sum(item_criticality)\n    flip_candidates = np.random.choice(n_items, size=max_flips, p=flip_probabilities, replace=False)\n\n    # Apply flips while maintaining feasibility\n    for idx in flip_candidates:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight <= capacity:\n            new_solution = temp_solution\n            current_weight = new_weight\n\n    # If no flips made, perform a random feasible flip\n    if np.array_equal(new_solution, base_solution):\n        valid_indices = [i for i in range(n_items) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = np.random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The heuristic algorithm selects a solution from the archive with high crowding distance to focus on under-explored regions of the Pareto front, then applies a dynamic local search that adaptively flips item subsets based on weighted improvements in both objectives while ensuring feasibility through iterative checks and adaptive subset adjustments. The method balances exploration and exploitation by dynamically adjusting subset size and flip strategy, prioritizing objective improvements with adaptive weights that depend on the current solution's density.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high crowding distance to explore under-represented regions\n    objectives = np.array([obj for _, obj in archive])\n    n_solutions = len(archive)\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(n_solutions)\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, n_solutions - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Adaptive subset flip with dynamic size and Pareto-aware improvements\n    n_items = len(weight_lst)\n    max_subset_size = min(5, n_items // 3)  # Dynamic subset size\n    subset_size = np.random.randint(1, max_subset_size + 1)\n\n    # Evaluate potential improvements for both objectives\n    candidate_improvements = []\n    for _ in range(min(10, n_items)):\n        # Randomly select a subset of items\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n        temp_solution = new_solution.copy()\n        temp_solution[subset_indices] = 1 - temp_solution[subset_indices]\n\n        # Calculate new weight\n        new_weight = current_weight\n        for idx in subset_indices:\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = np.sum((temp_solution[subset_indices] - new_solution[subset_indices]) * value1_lst[subset_indices])\n        delta_v2 = np.sum((temp_solution[subset_indices] - new_solution[subset_indices]) * value2_lst[subset_indices])\n\n        # Weighted improvement (adaptive weights based on current solution's characteristics)\n        weight_v1 = 0.5 if np.sum(new_solution) < n_items / 2 else 0.3\n        weight_v2 = 1 - weight_v1\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        candidate_improvements.append((improvement, temp_solution))\n\n    # Apply the best candidate if found\n    if candidate_improvements:\n        best_improvement, best_solution = max(candidate_improvements, key=lambda x: x[0])\n        if best_improvement > 0:\n            return best_solution\n\n    # If no improvement found, perform a random feasible flip\n    valid_indices = [i for i in range(n_items) if\n                    (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                    (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n    if valid_indices:\n        idx = np.random.choice(valid_indices)\n        new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 24,
        "algorithm": "The algorithm selects a solution from the archive with low crowding distance (prioritizing well-explored regions) and applies a greedy local search that flips items based on normalized marginal contributions to both objectives (equal-weighted improvement), ensuring feasibility through iterative checks and falling back to random feasible flips when no improvement is found. The key design ideas are selecting under-explored regions via crowding distance and using normalized marginal contributions to guide local search with equal weighting between objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with low crowding distance to focus on well-explored regions\n    objectives = np.array([obj for _, obj in archive])\n    n_solutions = len(archive)\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(n_solutions)\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, n_solutions - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the lowest crowding distance\n    selected_idx = np.argmin(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Greedy local search with normalized marginal contributions\n    n_items = len(weight_lst)\n    valid_indices = [i for i in range(n_items) if\n                    (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                    (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n\n    if not valid_indices:\n        return new_solution\n\n    # Calculate normalized marginal contributions\n    marginal_contributions = []\n    for i in valid_indices:\n        if new_solution[i] == 0:\n            delta_v1 = value1_lst[i]\n            delta_v2 = value2_lst[i]\n        else:\n            delta_v1 = -value1_lst[i]\n            delta_v2 = -value2_lst[i]\n\n        # Normalize contributions\n        norm_v1 = delta_v1 / (np.max(value1_lst) - np.min(value1_lst) + 1e-6)\n        norm_v2 = delta_v2 / (np.max(value2_lst) - np.min(value2_lst) + 1e-6)\n\n        # Fixed weighted improvement (equal weights)\n        improvement = 0.5 * norm_v1 + 0.5 * norm_v2\n        marginal_contributions.append((improvement, i))\n\n    # Apply the best candidate if found\n    if marginal_contributions:\n        best_improvement, best_idx = max(marginal_contributions, key=lambda x: x[0])\n        if best_improvement > 0:\n            new_solution[best_idx] = 1 - new_solution[best_idx]\n            return new_solution\n\n    # If no improvement found, perform a random feasible flip\n    idx = np.random.choice(valid_indices)\n    new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8889149444330082,
            4.1011267602443695
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with low crowding distance to focus on well-explored regions\n    objectives = np.array([obj for _, obj in archive])\n    n_solutions = len(archive)\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(n_solutions)\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, n_solutions - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the lowest crowding distance\n    selected_idx = np.argmin(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Greedy local search with normalized marginal contributions\n    n_items = len(weight_lst)\n    valid_indices = [i for i in range(n_items) if\n                    (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                    (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n\n    if not valid_indices:\n        return new_solution\n\n    # Calculate normalized marginal contributions\n    marginal_contributions = []\n    for i in valid_indices:\n        if new_solution[i] == 0:\n            delta_v1 = value1_lst[i]\n            delta_v2 = value2_lst[i]\n        else:\n            delta_v1 = -value1_lst[i]\n            delta_v2 = -value2_lst[i]\n\n        # Normalize contributions\n        norm_v1 = delta_v1 / (np.max(value1_lst) - np.min(value1_lst) + 1e-6)\n        norm_v2 = delta_v2 / (np.max(value2_lst) - np.min(value2_lst) + 1e-6)\n\n        # Fixed weighted improvement (equal weights)\n        improvement = 0.5 * norm_v1 + 0.5 * norm_v2\n        marginal_contributions.append((improvement, i))\n\n    # Apply the best candidate if found\n    if marginal_contributions:\n        best_improvement, best_idx = max(marginal_contributions, key=lambda x: x[0])\n        if best_improvement > 0:\n            new_solution[best_idx] = 1 - new_solution[best_idx]\n            return new_solution\n\n    # If no improvement found, perform a random feasible flip\n    idx = np.random.choice(valid_indices)\n    new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe heuristic algorithm selects a solution from the archive with high crowding distance to focus on under-explored regions of the Pareto front, then applies a dynamic local search that adaptively flips item subsets based on weighted improvements in both objectives while ensuring feasibility through iterative checks and adaptive subset adjustments. The method balances exploration and exploitation by dynamically adjusting subset size and flip strategy, prioritizing objective improvements with adaptive weights that depend on the current solution's density.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high crowding distance to explore under-represented regions\n    objectives = np.array([obj for _, obj in archive])\n    n_solutions = len(archive)\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(n_solutions)\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, n_solutions - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Adaptive subset flip with dynamic size and Pareto-aware improvements\n    n_items = len(weight_lst)\n    max_subset_size = min(5, n_items // 3)  # Dynamic subset size\n    subset_size = np.random.randint(1, max_subset_size + 1)\n\n    # Evaluate potential improvements for both objectives\n    candidate_improvements = []\n    for _ in range(min(10, n_items)):\n        # Randomly select a subset of items\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n        temp_solution = new_solution.copy()\n        temp_solution[subset_indices] = 1 - temp_solution[subset_indices]\n\n        # Calculate new weight\n        new_weight = current_weight\n        for idx in subset_indices:\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = np.sum((temp_solution[subset_indices] - new_solution[subset_indices]) * value1_lst[subset_indices])\n        delta_v2 = np.sum((temp_solution[subset_indices] - new_solution[subset_indices]) * value2_lst[subset_indices])\n\n        # Weighted improvement (adaptive weights based on current solution's characteristics)\n        weight_v1 = 0.5 if np.sum(new_solution) < n_items / 2 else 0.3\n        weight_v2 = 1 - weight_v1\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        candidate_improvements.append((improvement, temp_solution))\n\n    # Apply the best candidate if found\n    if candidate_improvements:\n        best_improvement, best_solution = max(candidate_improvements, key=lambda x: x[0])\n        if best_improvement > 0:\n            return best_solution\n\n    # If no improvement found, perform a random feasible flip\n    valid_indices = [i for i in range(n_items) if\n                    (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                    (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n    if valid_indices:\n        idx = np.random.choice(valid_indices)\n        new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing those with high objective diversity (via crowding distance) and applies a hybrid local search combining item swaps and subset replacements to generate a feasible neighbor solution while ensuring weight constraints are not violated. It first evaluates solutions based on their crowding distances to identify promising candidates, then performs targeted swaps and subset operations to explore the neighborhood while maintaining feasibility. The algorithm balances exploration and exploitation by focusing on high-diversity regions of the Pareto front while dynamically adjusting the search based on the current solution's characteristics.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high objective diversity and low crowding distance)\n    # Calculate crowding distances for solutions in the archive\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n    objectives_sorted = objectives[sorted_indices]\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance (most promising for improvement)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Randomly swap items if feasible\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Strategy 2: Replace a subset of items with new ones if feasible\n    subset_size = min(3, n_items // 4)\n    if subset_size > 0:\n        # Remove a subset of items\n        remove_indices = np.where(new_solution == 1)[0]\n        if len(remove_indices) > 0:\n            remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Add a subset of new items\n        add_indices = np.where(new_solution == 0)[0]\n        if len(add_indices) > 0:\n            add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s a refined, step-by-step approach to designing an effective heuristic for the BI-KP:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware exploration**, **feasibility-preserving operators**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset replacements** (e.g., flipping clusters of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 25,
        "algorithm": "The algorithm first selects a promising solution from the archive using a weighted crowding-distance metric that prioritizes solutions with high diversity in both objectives, then applies a hybrid local search combining large-scale subset flips and targeted item swaps, dynamically adjusting neighborhood size and evaluation weights based on solution quality and position in the Pareto front, while always ensuring feasibility through precomputed weight checks.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high weighted crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    n_solutions = len(archive)\n\n    # Compute weighted crowding distances\n    crowding_distances = np.zeros(n_solutions)\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, n_solutions - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Weight crowding distances based on solution quality\n    solution_qualities = np.sum(objectives, axis=1)\n    weighted_distances = crowding_distances * solution_qualities\n    selected_idx = np.argmax(weighted_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Adaptive hybrid local search\n    n_items = len(weight_lst)\n    max_subset_size = min(7, n_items // 2)  # Larger dynamic subset size\n    subset_size = np.random.randint(1, max_subset_size + 1)\n\n    # Strategy 1: Large-scale subset flip\n    candidate_improvements = []\n    for _ in range(min(15, n_items)):\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n        temp_solution = new_solution.copy()\n        temp_solution[subset_indices] = 1 - temp_solution[subset_indices]\n\n        new_weight = current_weight\n        for idx in subset_indices:\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = np.sum((temp_solution[subset_indices] - new_solution[subset_indices]) * value1_lst[subset_indices])\n        delta_v2 = np.sum((temp_solution[subset_indices] - new_solution[subset_indices]) * value2_lst[subset_indices])\n\n        # Adaptive weights based on solution position\n        weight_v1 = 0.4 if np.sum(new_solution) < n_items / 2 else 0.6\n        weight_v2 = 1 - weight_v1\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        candidate_improvements.append((improvement, temp_solution))\n\n    # Strategy 2: Targeted item swaps\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Apply the best candidate if found\n    if candidate_improvements:\n        best_improvement, best_solution = max(candidate_improvements, key=lambda x: x[0])\n        if best_improvement > 0:\n            return best_solution\n\n    # If no improvement found, perform a random feasible flip\n    valid_indices = [i for i in range(n_items) if\n                    (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                    (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n    if valid_indices:\n        idx = np.random.choice(valid_indices)\n        new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.5419857364810728,
            3.2880769073963165
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high weighted crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    n_solutions = len(archive)\n\n    # Compute weighted crowding distances\n    crowding_distances = np.zeros(n_solutions)\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, n_solutions - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Weight crowding distances based on solution quality\n    solution_qualities = np.sum(objectives, axis=1)\n    weighted_distances = crowding_distances * solution_qualities\n    selected_idx = np.argmax(weighted_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Adaptive hybrid local search\n    n_items = len(weight_lst)\n    max_subset_size = min(7, n_items // 2)  # Larger dynamic subset size\n    subset_size = np.random.randint(1, max_subset_size + 1)\n\n    # Strategy 1: Large-scale subset flip\n    candidate_improvements = []\n    for _ in range(min(15, n_items)):\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n        temp_solution = new_solution.copy()\n        temp_solution[subset_indices] = 1 - temp_solution[subset_indices]\n\n        new_weight = current_weight\n        for idx in subset_indices:\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = np.sum((temp_solution[subset_indices] - new_solution[subset_indices]) * value1_lst[subset_indices])\n        delta_v2 = np.sum((temp_solution[subset_indices] - new_solution[subset_indices]) * value2_lst[subset_indices])\n\n        # Adaptive weights based on solution position\n        weight_v1 = 0.4 if np.sum(new_solution) < n_items / 2 else 0.6\n        weight_v2 = 1 - weight_v1\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        candidate_improvements.append((improvement, temp_solution))\n\n    # Strategy 2: Targeted item swaps\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Apply the best candidate if found\n    if candidate_improvements:\n        best_improvement, best_solution = max(candidate_improvements, key=lambda x: x[0])\n        if best_improvement > 0:\n            return best_solution\n\n    # If no improvement found, perform a random feasible flip\n    valid_indices = [i for i in range(n_items) if\n                    (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                    (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n    if valid_indices:\n        idx = np.random.choice(valid_indices)\n        new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 3 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing those with high objective diversity (via crowding distance) and applies a hybrid local search combining item swaps and subset replacements to generate a feasible neighbor solution while ensuring weight constraints are not violated. It first evaluates solutions based on their crowding distances to identify promising candidates, then performs targeted swaps and subset operations to explore the neighborhood while maintaining feasibility. The algorithm balances exploration and exploitation by focusing on high-diversity regions of the Pareto front while dynamically adjusting the search based on the current solution's characteristics.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high objective diversity and low crowding distance)\n    # Calculate crowding distances for solutions in the archive\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n    objectives_sorted = objectives[sorted_indices]\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance (most promising for improvement)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Randomly swap items if feasible\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Strategy 2: Replace a subset of items with new ones if feasible\n    subset_size = min(3, n_items // 4)\n    if subset_size > 0:\n        # Remove a subset of items\n        remove_indices = np.where(new_solution == 1)[0]\n        if len(remove_indices) > 0:\n            remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Add a subset of new items\n        add_indices = np.where(new_solution == 0)[0]\n        if len(add_indices) > 0:\n            add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware exploration**, **feasibility-preserving operators**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset replacements** (e.g., flipping clusters of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n---\n### **Heuristic Implementation (Python-like Pseudocode)**\n```python\ndef select_neighbor(archive, weights, capacity, item_weights):\n    # Step 1: Adaptive selection (weighted crowding-distance)\n    selected = weighted_crowding_selection(archive, weights)\n\n    # Step 2: Hybrid local search (adaptive subset flips + objective-weighted swaps)\n    neighbor = selected.copy()\n    if random_choice():  # Adaptive subset flip\n        subset_size = min(len(selected), int(len(selected) * 0.3 * (1 - selected.sum() / capacity)))\n        flip_indices = random.sample(range(len(selected)), subset_size)\n        for i in flip_indices:\n            if selected[i] == 1:\n                if total_weight - item_weights[i] <= capacity:\n                    neighbor[i] = 0\n            else:\n                if total_weight + item_weights[i] <= capacity:\n                    neighbor[i] = 1\n    else:  # Objective-weighted swap\n        obj1_weight, obj2_weight = weights\n        for i in random.sample(range(len(selected)), min(3, len(selected))):\n            if selected[i] == 1 and (obj1_weight > 0 or obj2_weight > 0):\n                for j in range(len(selected)):\n                    if selected[j] == 0 and (total_weight - item_weights[i] + item_weights[j] <= capacity):\n                        if (obj1_weight * (archive[i][0] - archive[j][0]) + obj2_weight * (archive[i][1] - archive[j][1])) > 0:\n                            neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n                            break\n\n    return neighbor\n```\n\n*(Word count: 100)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 26,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive selection - prioritize solutions with high crowding distance and normalized objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance with normalized objective diversity\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_diversity = np.zeros(len(archive))\n    for i, (v1, v2) in enumerate(objectives):\n        normalized_diversity[i] = (v1 / max_v1 + v2 / max_v2) if max_v1 > 0 and max_v2 > 0 else 0\n\n    selection_scores = crowding_distances * normalized_diversity\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search - adaptive subset flips with objective-weighted improvements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic neighborhood size based on solution quality\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Strategy 1: Adaptive subset flip with objective-weighted selection\n    for _ in range(neighborhood_size):\n        # Select a random subset of items to consider\n        subset_size = min(3, max(1, int(n_items * 0.2)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            # Try flipping the item\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            # Calculate new weight\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            # Check feasibility\n            if new_weight > capacity:\n                continue\n\n            # Calculate objective improvements\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-weighted improvement\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Strategy 2: Objective-weighted item swaps if no improvement found\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    # Objective-weighted swap decision\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n",
        "score": [
            -0.9603317168957264,
            1.4646897912025452
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive selection - prioritize solutions with high crowding distance and normalized objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance with normalized objective diversity\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_diversity = np.zeros(len(archive))\n    for i, (v1, v2) in enumerate(objectives):\n        normalized_diversity[i] = (v1 / max_v1 + v2 / max_v2) if max_v1 > 0 and max_v2 > 0 else 0\n\n    selection_scores = crowding_distances * normalized_diversity\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search - adaptive subset flips with objective-weighted improvements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic neighborhood size based on solution quality\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Strategy 1: Adaptive subset flip with objective-weighted selection\n    for _ in range(neighborhood_size):\n        # Select a random subset of items to consider\n        subset_size = min(3, max(1, int(n_items * 0.2)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            # Try flipping the item\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            # Calculate new weight\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            # Check feasibility\n            if new_weight > capacity:\n                continue\n\n            # Calculate objective improvements\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-weighted improvement\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Strategy 2: Objective-weighted item swaps if no improvement found\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    # Objective-weighted swap decision\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 3 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing those with high objective diversity (via crowding distance) and applies a hybrid local search combining item swaps and subset replacements to generate a feasible neighbor solution while ensuring weight constraints are not violated. It first evaluates solutions based on their crowding distances to identify promising candidates, then performs targeted swaps and subset operations to explore the neighborhood while maintaining feasibility. The algorithm balances exploration and exploitation by focusing on high-diversity regions of the Pareto front while dynamically adjusting the search based on the current solution's characteristics.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high objective diversity and low crowding distance)\n    # Calculate crowding distances for solutions in the archive\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n    objectives_sorted = objectives[sorted_indices]\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance (most promising for improvement)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Randomly swap items if feasible\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Strategy 2: Replace a subset of items with new ones if feasible\n    subset_size = min(3, n_items // 4)\n    if subset_size > 0:\n        # Remove a subset of items\n        remove_indices = np.where(new_solution == 1)[0]\n        if len(remove_indices) > 0:\n            remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Add a subset of new items\n        add_indices = np.where(new_solution == 0)[0]\n        if len(add_indices) > 0:\n            add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware exploration**, **feasibility-preserving operators**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset replacements** (e.g., flipping clusters of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n---\n### **Heuristic Implementation (Python-like Pseudocode)**\n```python\ndef select_neighbor(archive, weights, capacity, item_weights):\n    # Step 1: Adaptive selection (weighted crowding-distance)\n    selected = weighted_crowding_selection(archive, weights)\n\n    # Step 2: Hybrid local search (adaptive subset flips + objective-weighted swaps)\n    neighbor = selected.copy()\n    if random_choice():  # Adaptive subset flip\n        subset_size = min(len(selected), int(len(selected) * 0.3 * (1 - selected.sum() / capacity)))\n        flip_indices = random.sample(range(len(selected)), subset_size)\n        for i in flip_indices:\n            if selected[i] == 1:\n                if total_weight - item_weights[i] <= capacity:\n                    neighbor[i] = 0\n            else:\n                if total_weight + item_weights[i] <= capacity:\n                    neighbor[i] = 1\n    else:  # Objective-weighted swap\n        obj1_weight, obj2_weight = weights\n        for i in random.sample(range(len(selected)), min(3, len(selected))):\n            if selected[i] == 1 and (obj1_weight > 0 or obj2_weight > 0):\n                for j in range(len(selected)):\n                    if selected[j] == 0 and (total_weight - item_weights[i] + item_weights[j] <= capacity):\n                        if (obj1_weight * (archive[i][0] - archive[j][0]) + obj2_weight * (archive[i][1] - archive[j][1])) > 0:\n                            neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n                            break\n\n    return neighbor\n```\n\n*(Word count: 100)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 26,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive selection - prioritize solutions with high crowding distance and normalized objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance with normalized objective diversity\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_diversity = np.zeros(len(archive))\n    for i, (v1, v2) in enumerate(objectives):\n        normalized_diversity[i] = (v1 / max_v1 + v2 / max_v2) if max_v1 > 0 and max_v2 > 0 else 0\n\n    selection_scores = crowding_distances * normalized_diversity\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search - adaptive subset flips with objective-weighted improvements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic neighborhood size based on solution quality\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Strategy 1: Adaptive subset flip with objective-weighted selection\n    for _ in range(neighborhood_size):\n        # Select a random subset of items to consider\n        subset_size = min(3, max(1, int(n_items * 0.2)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            # Try flipping the item\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            # Calculate new weight\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            # Check feasibility\n            if new_weight > capacity:\n                continue\n\n            # Calculate objective improvements\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-weighted improvement\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Strategy 2: Objective-weighted item swaps if no improvement found\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    # Objective-weighted swap decision\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n",
        "score": [
            -0.9603317168957264,
            1.4646897912025452
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive selection - prioritize solutions with high crowding distance and normalized objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance with normalized objective diversity\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_diversity = np.zeros(len(archive))\n    for i, (v1, v2) in enumerate(objectives):\n        normalized_diversity[i] = (v1 / max_v1 + v2 / max_v2) if max_v1 > 0 and max_v2 > 0 else 0\n\n    selection_scores = crowding_distances * normalized_diversity\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search - adaptive subset flips with objective-weighted improvements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic neighborhood size based on solution quality\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Strategy 1: Adaptive subset flip with objective-weighted selection\n    for _ in range(neighborhood_size):\n        # Select a random subset of items to consider\n        subset_size = min(3, max(1, int(n_items * 0.2)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            # Try flipping the item\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            # Calculate new weight\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            # Check feasibility\n            if new_weight > capacity:\n                continue\n\n            # Calculate objective improvements\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-weighted improvement\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Strategy 2: Objective-weighted item swaps if no improvement found\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    # Objective-weighted swap decision\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware exploration**, **feasibility-preserving operators**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset replacements** (e.g., flipping clusters of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n---\n### **Heuristic Implementation (Python-like Pseudocode)**\n```python\ndef select_neighbor(archive, weights, capacity, item_weights):\n    # Step 1: Adaptive selection (weighted crowding-distance)\n    selected = weighted_crowding_selection(archive, weights)\n\n    # Step 2: Hybrid local search (adaptive subset flips + objective-weighted swaps)\n    neighbor = selected.copy()\n    if random_choice():  # Adaptive subset flip\n        subset_size = min(len(selected), int(len(selected) * 0.3 * (1 - selected.sum() / capacity)))\n        flip_indices = random.sample(range(len(selected)), subset_size)\n        for i in flip_indices:\n            if selected[i] == 1:\n                if total_weight - item_weights[i] <= capacity:\n                    neighbor[i] = 0\n            else:\n                if total_weight + item_weights[i] <= capacity:\n                    neighbor[i] = 1\n    else:  # Objective-weighted swap\n        obj1_weight, obj2_weight = weights\n        for i in random.sample(range(len(selected)), min(3, len(selected))):\n            if selected[i] == 1 and (obj1_weight > 0 or obj2_weight > 0):\n                for j in range(len(selected)):\n                    if selected[j] == 0 and (total_weight - item_weights[i] + item_weights[j] <= capacity):\n                        if (obj1_weight * (archive[i][0] - archive[j][0]) + obj2_weight * (archive[i][1] - archive[j][1])) > 0:\n                            neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n                            break\n\n    return neighbor\n```\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 27,
        "algorithm": "The algorithm combines Pareto ranking with crowding-distance selection to prioritize diverse, high-quality solutions from the archive, then applies an adaptive cluster-based local search that dynamically adjusts flip cluster sizes based on weight utilization and objective weights, ensuring feasibility through precomputed item contributions and move validation. It balances exploration/exploitation by flipping items within clusters while favoring moves that improve both objectives proportionally, falling back to random valid flips if no improvement is found.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max(obj[0] for _, obj in pareto_front) - min(obj[0] for _, obj in pareto_front) + 1e-6) + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max(obj[1] for _, obj in pareto_front) - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                crowding_distances.append(dist)\n\n            # Select solution with highest crowding distance\n            max_dist_idx = np.argmax(crowding_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive cluster-based item flips\n    cluster_size = max(1, int(len(new_solution) * 0.1 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8425008359101943,
            1.1933557987213135
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max(obj[0] for _, obj in pareto_front) - min(obj[0] for _, obj in pareto_front) + 1e-6) + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max(obj[1] for _, obj in pareto_front) - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                crowding_distances.append(dist)\n\n            # Select solution with highest crowding distance\n            max_dist_idx = np.argmax(crowding_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive cluster-based item flips\n    cluster_size = max(1, int(len(new_solution) * 0.1 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm dynamically selects a promising solution from the archive using a hybrid of crowding distance and adaptive weighting, then applies a novel local search combining probabilistic swaps (prioritizing high-value-to-weight ratio items) and Pareto-aware subset replacements to generate high-quality neighbors while ensuring feasibility. The selection prioritizes solutions with high crowding distances and normalized objective values, while the local search focuses on swapping items and replacing subsets to explore the solution space effectively.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hybrid selection (crowding distance + adaptive weighting)\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Adaptive weighting: prioritize solutions with high crowding distance and high normalized objectives\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_scores = np.array([(obj[0]/max_v1 if max_v1 > 0 else 0) + (obj[1]/max_v2 if max_v2 > 0 else 0) for obj in objectives])\n    weighted_scores = crowding_distances * normalized_scores\n    selected_idx = np.argmax(weighted_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Novel local search (probabilistic swaps + Pareto-aware subset replacements)\n    n_items = len(weight_lst)\n    subset_size = min(5, n_items // 3)\n\n    # Probabilistic swaps (prioritize high-value-to-weight ratio items)\n    value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    for _ in range(min(10, n_items // 2)):\n        if np.random.random() < 0.7:  # Higher probability for swaps\n            candidates = np.argsort(value_ratios)[-subset_size:]  # Top subset_size items by value ratio\n            i, j = np.random.choice(candidates, 2, replace=False)\n            if new_solution[i] != new_solution[j]:\n                if new_solution[i] == 1 and new_solution[j] == 0:\n                    new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                    if new_weight <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = new_weight\n                elif new_solution[i] == 0 and new_solution[j] == 1:\n                    new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                    if new_weight <= capacity:\n                        new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                        current_weight = new_weight\n        else:  # Subset replacement\n            remove_indices = np.where(new_solution == 1)[0]\n            if len(remove_indices) > 0:\n                remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n                new_solution[remove_subset] = 0\n                current_weight -= np.sum(weight_lst[remove_subset])\n\n                add_indices = np.where(new_solution == 0)[0]\n                if len(add_indices) > 0:\n                    add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n                    potential_weight = current_weight + np.sum(weight_lst[add_subset])\n                    if potential_weight <= capacity:\n                        new_solution[add_subset] = 1\n                        current_weight = potential_weight\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware exploration**, **feasibility-preserving operators**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset replacements** (e.g., flipping clusters of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n---\n### **Heuristic Implementation (Python-like Pseudocode)**\n```python\ndef select_neighbor(archive, weights, capacity, item_weights):\n    # Step 1: Adaptive selection (weighted crowding-distance)\n    selected = weighted_crowding_selection(archive, weights)\n\n    # Step 2: Hybrid local search (adaptive subset flips + objective-weighted swaps)\n    neighbor = selected.copy()\n    if random_choice():  # Adaptive subset flip\n        subset_size = min(len(selected), int(len(selected) * 0.3 * (1 - selected.sum() / capacity)))\n        flip_indices = random.sample(range(len(selected)), subset_size)\n        for i in flip_indices:\n            if selected[i] == 1:\n                if total_weight - item_weights[i] <= capacity:\n                    neighbor[i] = 0\n            else:\n                if total_weight + item_weights[i] <= capacity:\n                    neighbor[i] = 1\n    else:  # Objective-weighted swap\n        obj1_weight, obj2_weight = weights\n        for i in random.sample(range(len(selected)), min(3, len(selected))):\n            if selected[i] == 1 and (obj1_weight > 0 or obj2_weight > 0):\n                for j in range(len(selected)):\n                    if selected[j] == 0 and (total_weight - item_weights[i] + item_weights[j] <= capacity):\n                        if (obj1_weight * (archive[i][0] - archive[j][0]) + obj2_weight * (archive[i][1] - archive[j][1])) > 0:\n                            neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n                            break\n\n    return neighbor\n```\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 28,
        "algorithm": "The algorithm combines adaptive solution selection (prioritizing high-crowding-distance solutions) with a hybrid local search that alternates between probabilistic cluster flips (targeting high-value-to-weight items) and objective-weighted subset swaps (balancing both objectives). It ensures feasibility by precomputing item contributions and dynamically adjusting the solution while maintaining weight constraints, with a focus on exploration through random cluster selections and exploitation through value-weighted improvements.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Adaptive selection (combining crowding distance and objective dominance)\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n\n    # Compute crowding distances\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select top 30% of solutions based on crowding distance and objective dominance\n    threshold = np.percentile(crowding_distances, 70)\n    candidates = [sol for (sol, _), dist in zip(archive, crowding_distances) if dist >= threshold]\n\n    if not candidates:\n        candidates = [sol for sol, _ in archive]\n\n    base_solution = random.choice(candidates).copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Hybrid local search (probabilistic cluster flips + objective-weighted subset swaps)\n    n_items = len(weight_lst)\n    cluster_size = max(1, min(5, n_items // 4))\n\n    # Probabilistic cluster flips (prioritize high-value-to-weight ratio clusters)\n    value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)[::-1]\n\n    for _ in range(min(5, n_items // 3)):\n        if np.random.random() < 0.6:  # Higher probability for cluster flips\n            cluster_indices = np.random.choice(sorted_indices[:cluster_size * 2], size=cluster_size, replace=False)\n            for i in cluster_indices:\n                if new_solution[i] == 1:\n                    if current_weight - weight_lst[i] <= capacity:\n                        new_solution[i] = 0\n                        current_weight -= weight_lst[i]\n                else:\n                    if current_weight + weight_lst[i] <= capacity:\n                        new_solution[i] = 1\n                        current_weight += weight_lst[i]\n        else:  # Objective-weighted subset swaps\n            obj1_weight = np.random.random()\n            obj2_weight = 1 - obj1_weight\n            swap_candidates = np.random.choice(n_items, size=min(3, n_items), replace=False)\n\n            for i in swap_candidates:\n                if new_solution[i] == 1:\n                    for j in range(n_items):\n                        if new_solution[j] == 0 and (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                            delta_v1 = value1_lst[j] - value1_lst[i]\n                            delta_v2 = value2_lst[j] - value2_lst[i]\n                            improvement = obj1_weight * delta_v1 + obj2_weight * delta_v2\n                            if improvement > 0:\n                                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                                break\n\n    return new_solution\n\n",
        "score": [
            -0.4481600221081863,
            2.1738599240779877
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Adaptive selection (combining crowding distance and objective dominance)\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n\n    # Compute crowding distances\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select top 30% of solutions based on crowding distance and objective dominance\n    threshold = np.percentile(crowding_distances, 70)\n    candidates = [sol for (sol, _), dist in zip(archive, crowding_distances) if dist >= threshold]\n\n    if not candidates:\n        candidates = [sol for sol, _ in archive]\n\n    base_solution = random.choice(candidates).copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Hybrid local search (probabilistic cluster flips + objective-weighted subset swaps)\n    n_items = len(weight_lst)\n    cluster_size = max(1, min(5, n_items // 4))\n\n    # Probabilistic cluster flips (prioritize high-value-to-weight ratio clusters)\n    value_ratios = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_indices = np.argsort(value_ratios)[::-1]\n\n    for _ in range(min(5, n_items // 3)):\n        if np.random.random() < 0.6:  # Higher probability for cluster flips\n            cluster_indices = np.random.choice(sorted_indices[:cluster_size * 2], size=cluster_size, replace=False)\n            for i in cluster_indices:\n                if new_solution[i] == 1:\n                    if current_weight - weight_lst[i] <= capacity:\n                        new_solution[i] = 0\n                        current_weight -= weight_lst[i]\n                else:\n                    if current_weight + weight_lst[i] <= capacity:\n                        new_solution[i] = 1\n                        current_weight += weight_lst[i]\n        else:  # Objective-weighted subset swaps\n            obj1_weight = np.random.random()\n            obj2_weight = 1 - obj1_weight\n            swap_candidates = np.random.choice(n_items, size=min(3, n_items), replace=False)\n\n            for i in swap_candidates:\n                if new_solution[i] == 1:\n                    for j in range(n_items):\n                        if new_solution[j] == 0 and (current_weight - weight_lst[i] + weight_lst[j] <= capacity):\n                            delta_v1 = value1_lst[j] - value1_lst[i]\n                            delta_v2 = value2_lst[j] - value2_lst[i]\n                            improvement = obj1_weight * delta_v1 + obj2_weight * delta_v2\n                            if improvement > 0:\n                                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                                current_weight = current_weight - weight_lst[i] + weight_lst[j]\n                                break\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 29,
        "algorithm": "The algorithm prioritizes solutions with high normalized objective values by selecting from the top 30% of candidates, then applies a hybrid local search that dynamically adjusts weights between objectives based on the current solution's dominance. It explores a 30% random subset of items, flipping those that most improve a weighted sum of objectives while ensuring feasibility, with a fallback to diversity-aware flips using marginal contributions and item inclusion rates in the archive.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive neighborhood exploration\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Diversity-aware flip based on marginal contribution and solution diversity\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions and diversity scores\n            contributions = []\n            diversity_scores = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n                # Diversity score: higher if item is not frequently included in archive\n                inclusion_rate = sum(sol[i] for sol, _ in archive) / len(archive)\n                diversity_scores.append(1 - inclusion_rate)\n\n            # Combine contribution and diversity with temperature parameter\n            temperature = 0.7  # Controls exploration/exploitation trade-off\n            combined_scores = [c * (1 - temperature) + d * temperature for c, d in zip(contributions, diversity_scores)]\n\n            if sum(combined_scores) > 0:\n                probabilities = [c / sum(combined_scores) for c in combined_scores]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8527123918525894,
            1.001550316810608
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive neighborhood exploration\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Diversity-aware flip based on marginal contribution and solution diversity\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions and diversity scores\n            contributions = []\n            diversity_scores = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n                # Diversity score: higher if item is not frequently included in archive\n                inclusion_rate = sum(sol[i] for sol, _ in archive) / len(archive)\n                diversity_scores.append(1 - inclusion_rate)\n\n            # Combine contribution and diversity with temperature parameter\n            temperature = 0.7  # Controls exploration/exploitation trade-off\n            combined_scores = [c * (1 - temperature) + d * temperature for c, d in zip(contributions, diversity_scores)]\n\n            if sum(combined_scores) > 0:\n                probabilities = [c / sum(combined_scores) for c in combined_scores]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 30,
        "algorithm": "This algorithm selects a promising solution from the archive by prioritizing those with high value ratios (value1/value2), then applies a weighted local search where items are flipped based on their potential to improve a dynamically adjusted weighted combination of the two objectives. If no immediate improvement is found, it probabilistically flips items based on their potential contribution, ensuring feasibility by checking weight constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the ratio of objectives to prioritize diverse solutions\n        ratios = []\n        for _, obj in archive:\n            ratio = obj[0] / (obj[1] + 1e-6) if obj[1] > 0 else float('inf')\n            ratios.append(ratio)\n        # Select top 20% of solutions and choose randomly among them\n        threshold = np.percentile(ratios, 80)\n        candidates = [sol for (sol, _), r in zip(archive, ratios) if r >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's ratio\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    ratio = total_v1 / (total_v2 + 1e-6) if total_v2 > 0 else float('inf')\n    weight_v1 = 0.7 if ratio > 1 else 0.3\n    weight_v2 = 1 - weight_v1\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially flip\n    # 2. Evaluate potential improvements with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions with adjusted weights\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[idx]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.850898917536525,
            0.9733171164989471
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the ratio of objectives to prioritize diverse solutions\n        ratios = []\n        for _, obj in archive:\n            ratio = obj[0] / (obj[1] + 1e-6) if obj[1] > 0 else float('inf')\n            ratios.append(ratio)\n        # Select top 20% of solutions and choose randomly among them\n        threshold = np.percentile(ratios, 80)\n        candidates = [sol for (sol, _), r in zip(archive, ratios) if r >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's ratio\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    ratio = total_v1 / (total_v2 + 1e-6) if total_v2 > 0 else float('inf')\n    weight_v1 = 0.7 if ratio > 1 else 0.3\n    weight_v2 = 1 - weight_v1\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially flip\n    # 2. Evaluate potential improvements with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions with adjusted weights\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[idx]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware exploration**, **feasibility-preserving operators**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset replacements** (e.g., flipping clusters of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n---\n### **Heuristic Implementation (Python-like Pseudocode)**\n```python\ndef select_neighbor(archive, weights, capacity, item_weights):\n    # Step 1: Adaptive selection (weighted crowding-distance)\n    selected = weighted_crowding_selection(archive, weights)\n\n    # Step 2: Hybrid local search (adaptive subset flips + objective-weighted swaps)\n    neighbor = selected.copy()\n    if random_choice():  # Adaptive subset flip\n        subset_size = min(len(selected), int(len(selected) * 0.3 * (1 - selected.sum() / capacity)))\n        flip_indices = random.sample(range(len(selected)), subset_size)\n        for i in flip_indices:\n            if selected[i] == 1:\n                if total_weight - item_weights[i] <= capacity:\n                    neighbor[i] = 0\n            else:\n                if total_weight + item_weights[i] <= capacity:\n                    neighbor[i] = 1\n    else:  # Objective-weighted swap\n        obj1_weight, obj2_weight = weights\n        for i in random.sample(range(len(selected)), min(3, len(selected))):\n            if selected[i] == 1 and (obj1_weight > 0 or obj2_weight > 0):\n                for j in range(len(selected)):\n                    if selected[j] == 0 and (total_weight - item_weights[i] + item_weights[j] <= capacity):\n                        if (obj1_weight * (archive[i][0] - archive[j][0]) + obj2_weight * (archive[i][1] - archive[j][1])) > 0:\n                            neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n                            break\n\n    return neighbor\n```\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 31,
        "algorithm": "The algorithm selects a promising solution from the archive using a weighted crowding-distance metric, then applies a hybrid local search that combines adaptive subset flips, objective-weighted swaps, and probabilistic moves to generate a neighbor solution while ensuring feasibility. It dynamically adjusts neighborhood size and objective weights based on the solution's current quality and dominance, prioritizing moves that improve both objectives while maintaining diversity and exploration.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Adaptive selection using weighted crowding-distance\n        crowding_distances = []\n        for sol, obj in archive:\n            min_v1 = min(o[0] for _, o in archive)\n            max_v1 = max(o[0] for _, o in archive)\n            min_v2 = min(o[1] for _, o in archive)\n            max_v2 = max(o[1] for _, o in archive)\n            if max_v1 > min_v1 and max_v2 > min_v2:\n                norm_v1 = (obj[0] - min_v1) / (max_v1 - min_v1)\n                norm_v2 = (obj[1] - min_v2) / (max_v2 - min_v2)\n                crowding_distances.append(norm_v1 + norm_v2)\n            else:\n                crowding_distances.append(1.0)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(crowding_distances, 70)\n        candidates = [sol for (sol, _), dist in zip(archive, crowding_distances) if dist >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Adaptive subset flip with dynamic neighborhood size\n    # 2. Objective-weighted swaps\n    # 3. Probabilistic flip if no improvement found\n\n    # Step 1: Adaptive subset flip\n    subset_size = max(1, min(int(len(new_solution) * 0.3 * (1 - current_weight / capacity)), len(new_solution)))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Step 2: Objective-weighted swaps if no subset flip improvement\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8833184801012279,
            2.149962365627289
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Adaptive selection using weighted crowding-distance\n        crowding_distances = []\n        for sol, obj in archive:\n            min_v1 = min(o[0] for _, o in archive)\n            max_v1 = max(o[0] for _, o in archive)\n            min_v2 = min(o[1] for _, o in archive)\n            max_v2 = max(o[1] for _, o in archive)\n            if max_v1 > min_v1 and max_v2 > min_v2:\n                norm_v1 = (obj[0] - min_v1) / (max_v1 - min_v1)\n                norm_v2 = (obj[1] - min_v2) / (max_v2 - min_v2)\n                crowding_distances.append(norm_v1 + norm_v2)\n            else:\n                crowding_distances.append(1.0)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(crowding_distances, 70)\n        candidates = [sol for (sol, _), dist in zip(archive, crowding_distances) if dist >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Adaptive subset flip with dynamic neighborhood size\n    # 2. Objective-weighted swaps\n    # 3. Probabilistic flip if no improvement found\n\n    # Step 1: Adaptive subset flip\n    subset_size = max(1, min(int(len(new_solution) * 0.3 * (1 - current_weight / capacity)), len(new_solution)))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Step 2: Objective-weighted swaps if no subset flip improvement\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 4 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing those with high objective diversity (via crowding distance) and applies a hybrid local search combining item swaps and subset replacements to generate a feasible neighbor solution while ensuring weight constraints are not violated. It first evaluates solutions based on their crowding distances to identify promising candidates, then performs targeted swaps and subset operations to explore the neighborhood while maintaining feasibility. The algorithm balances exploration and exploitation by focusing on high-diversity regions of the Pareto front while dynamically adjusting the search based on the current solution's characteristics.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high objective diversity and low crowding distance)\n    # Calculate crowding distances for solutions in the archive\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n    objectives_sorted = objectives[sorted_indices]\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance (most promising for improvement)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Randomly swap items if feasible\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Strategy 2: Replace a subset of items with new ones if feasible\n    subset_size = min(3, n_items // 4)\n    if subset_size > 0:\n        # Remove a subset of items\n        remove_indices = np.where(new_solution == 1)[0]\n        if len(remove_indices) > 0:\n            remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Add a subset of new items\n        add_indices = np.where(new_solution == 0)[0]\n        if len(add_indices) > 0:\n            add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive selection - prioritize solutions with high crowding distance and normalized objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance with normalized objective diversity\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_diversity = np.zeros(len(archive))\n    for i, (v1, v2) in enumerate(objectives):\n        normalized_diversity[i] = (v1 / max_v1 + v2 / max_v2) if max_v1 > 0 and max_v2 > 0 else 0\n\n    selection_scores = crowding_distances * normalized_diversity\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search - adaptive subset flips with objective-weighted improvements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic neighborhood size based on solution quality\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Strategy 1: Adaptive subset flip with objective-weighted selection\n    for _ in range(neighborhood_size):\n        # Select a random subset of items to consider\n        subset_size = min(3, max(1, int(n_items * 0.2)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            # Try flipping the item\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            # Calculate new weight\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            # Check feasibility\n            if new_weight > capacity:\n                continue\n\n            # Calculate objective improvements\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-weighted improvement\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Strategy 2: Objective-weighted item swaps if no improvement found\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    # Objective-weighted swap decision\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s a concise, high-impact heuristic design for the BI-KP:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 32,
        "algorithm": "The algorithm selects a non-dominated solution from the archive using a Pareto-ranked crowding-distance metric, then applies an adaptive hybrid local search that combines subset flips and objective-weighted swaps to explore high-quality neighbors while ensuring feasibility. It prioritizes items with higher combined value-to-weight ratios and dynamically adjusts neighborhood size based on solution quality and capacity utilization. The search alternates between flipping subsets of items and performing targeted swaps when no improvements are found in the first phase.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high Pareto-ranked crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance with Pareto ranking\n    pareto_rank = np.zeros(len(archive))\n    for i in range(len(archive)):\n        dominated = False\n        for j in range(len(archive)):\n            if i != j and objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1]):\n                dominated = True\n                break\n        pareto_rank[i] = 0 if dominated else 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Adaptive hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic neighborhood size based on solution quality\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Strategy 1: Adaptive subset flip with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(3, max(1, int(n_items * 0.2)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Strategy 2: Objective-weighted item swaps if no improvement found\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n",
        "score": [
            -0.759328923213639,
            0.8390093743801117
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high Pareto-ranked crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance with Pareto ranking\n    pareto_rank = np.zeros(len(archive))\n    for i in range(len(archive)):\n        dominated = False\n        for j in range(len(archive)):\n            if i != j and objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1]):\n                dominated = True\n                break\n        pareto_rank[i] = 0 if dominated else 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Adaptive hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic neighborhood size based on solution quality\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Strategy 1: Adaptive subset flip with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(3, max(1, int(n_items * 0.2)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Strategy 2: Objective-weighted item swaps if no improvement found\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm prioritizes solutions with high normalized objective values by selecting from the top 30% of candidates, then applies a hybrid local search that dynamically adjusts weights between objectives based on the current solution's dominance. It explores a 30% random subset of items, flipping those that most improve a weighted sum of objectives while ensuring feasibility, with a fallback to diversity-aware flips using marginal contributions and item inclusion rates in the archive.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive neighborhood exploration\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Diversity-aware flip based on marginal contribution and solution diversity\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions and diversity scores\n            contributions = []\n            diversity_scores = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n                # Diversity score: higher if item is not frequently included in archive\n                inclusion_rate = sum(sol[i] for sol, _ in archive) / len(archive)\n                diversity_scores.append(1 - inclusion_rate)\n\n            # Combine contribution and diversity with temperature parameter\n            temperature = 0.7  # Controls exploration/exploitation trade-off\n            combined_scores = [c * (1 - temperature) + d * temperature for c, d in zip(contributions, diversity_scores)]\n\n            if sum(combined_scores) > 0:\n                probabilities = [c / sum(combined_scores) for c in combined_scores]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s a concise, high-impact heuristic design for the BI-KP:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 33,
        "algorithm": "The algorithm selects promising solutions from the archive using Pareto-ranked crowding-distance metrics to prioritize high-diversity candidates, then applies a hybrid local search with dynamic neighborhood size (20-50% of items) and adaptive objective-weighted swaps to balance exploration/exploitation while ensuring feasibility through precomputed contributions and greedy checks, falling back to diversity-aware flips if no improvements are found. The selection prioritizes solutions with higher crowding distances (70th percentile), and the local search dynamically adjusts flip probabilities based on item inclusion rates to maintain diversity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solutions using Pareto-ranked crowding-distance\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            for _, other_obj in archive:\n                if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n            if not dominated:\n                pareto_front.append((sol, obj))\n\n        if not pareto_front:\n            pareto_front = archive\n\n        # Calculate crowding distance for Pareto front\n        crowding_distances = []\n        for sol, obj in pareto_front:\n            distances = []\n            for _, other_obj in pareto_front:\n                if other_obj[0] == obj[0] and other_obj[1] == obj[1]:\n                    continue\n                distances.append(np.sqrt((other_obj[0] - obj[0])**2 + (other_obj[1] - obj[1])**2))\n            crowding_distances.append(np.mean(distances) if distances else 0)\n\n        # Select top 30% by crowding distance\n        threshold = np.percentile(crowding_distances, 70)\n        candidates = [sol for (sol, _), dist in zip(pareto_front, crowding_distances) if dist >= threshold]\n        base_solution = random.choice(candidates).copy() if candidates else archive[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    subset_size = max(1, int(len(new_solution) * random.uniform(0.2, 0.5)))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Adaptive weight adjustment based on current solution's objective dominance\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n        weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Diversity-aware flip based on item inclusion rates\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            inclusion_rates = [sum(sol[i] for sol, _ in archive) / len(archive) for i in valid_indices]\n            probabilities = [1 - rate for rate in inclusion_rates]\n            if sum(probabilities) > 0:\n                probabilities = [p / sum(probabilities) for p in probabilities]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.7918944663693056,
            3.040043890476227
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solutions using Pareto-ranked crowding-distance\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            for _, other_obj in archive:\n                if (other_obj[0] > obj[0] and other_obj[1] >= obj[1]) or (other_obj[0] >= obj[0] and other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n            if not dominated:\n                pareto_front.append((sol, obj))\n\n        if not pareto_front:\n            pareto_front = archive\n\n        # Calculate crowding distance for Pareto front\n        crowding_distances = []\n        for sol, obj in pareto_front:\n            distances = []\n            for _, other_obj in pareto_front:\n                if other_obj[0] == obj[0] and other_obj[1] == obj[1]:\n                    continue\n                distances.append(np.sqrt((other_obj[0] - obj[0])**2 + (other_obj[1] - obj[1])**2))\n            crowding_distances.append(np.mean(distances) if distances else 0)\n\n        # Select top 30% by crowding distance\n        threshold = np.percentile(crowding_distances, 70)\n        candidates = [sol for (sol, _), dist in zip(pareto_front, crowding_distances) if dist >= threshold]\n        base_solution = random.choice(candidates).copy() if candidates else archive[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    subset_size = max(1, int(len(new_solution) * random.uniform(0.2, 0.5)))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Adaptive weight adjustment based on current solution's objective dominance\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n        weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Diversity-aware flip based on item inclusion rates\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            inclusion_rates = [sum(sol[i] for sol, _ in archive) / len(archive) for i in valid_indices]\n            probabilities = [1 - rate for rate in inclusion_rates]\n            if sum(probabilities) > 0:\n                probabilities = [p / sum(probabilities) for p in probabilities]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects promising solutions from the archive based on hypervolume contributions, then applies a dynamic local search that flips items in a subset size proportional to solution quality, using adaptive weights to balance objective improvements. It prioritizes flips that maximize a weighted sum of normalized objective gains while ensuring feasibility, falling back to random valid flips if no improvement is found. The subset size and weights adjust based on solution quality and current objective values, enabling focused exploration of high-potential regions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by hypervolume contribution to prioritize promising ones\n        hypervolumes = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            hypervolumes.append(norm_v1 * norm_v2)  # Hypervolume approximation\n        # Select top 20% of solutions and choose randomly among them\n        threshold = np.percentile(hypervolumes, 80)\n        candidates = [sol for (sol, _), hv in zip(archive, hypervolumes) if hv >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic subset size based on solution quality\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])\n    max_quality = np.sum(value1_lst) + np.sum(value2_lst)\n    subset_size = max(1, int(total_items * (1 - (solution_quality / max_quality)) * 0.5))  # Larger subset for lower quality solutions\n\n    subset_indices = np.random.choice(total_items, size=subset_size, replace=False)\n\n    # Evaluate potential improvements with adaptive weights\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Adaptive weights based on current solution's objective values\n        current_v1 = np.sum(value1_lst[new_solution == 1])\n        current_v2 = np.sum(value2_lst[new_solution == 1])\n        w1 = current_v2 / (current_v1 + current_v2 + 1e-6) if (current_v1 + current_v2) > 0 else 0.5\n        w2 = current_v1 / (current_v1 + current_v2 + 1e-6) if (current_v1 + current_v2) > 0 else 0.5\n\n        improvement = (w1 * delta_v1 + w2 * delta_v2) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm prioritizes solutions with high normalized objective values by selecting from the top 30% of candidates, then applies a hybrid local search that dynamically adjusts weights between objectives based on the current solution's dominance. It explores a 30% random subset of items, flipping those that most improve a weighted sum of objectives while ensuring feasibility, with a fallback to diversity-aware flips using marginal contributions and item inclusion rates in the archive.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive neighborhood exploration\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Diversity-aware flip based on marginal contribution and solution diversity\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions and diversity scores\n            contributions = []\n            diversity_scores = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n                # Diversity score: higher if item is not frequently included in archive\n                inclusion_rate = sum(sol[i] for sol, _ in archive) / len(archive)\n                diversity_scores.append(1 - inclusion_rate)\n\n            # Combine contribution and diversity with temperature parameter\n            temperature = 0.7  # Controls exploration/exploitation trade-off\n            combined_scores = [c * (1 - temperature) + d * temperature for c, d in zip(contributions, diversity_scores)]\n\n            if sum(combined_scores) > 0:\n                probabilities = [c / sum(combined_scores) for c in combined_scores]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s a concise, high-impact heuristic design for the BI-KP:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 34,
        "algorithm": "This heuristic algorithm selects high-crowding-distance solutions from the archive, then applies a hybrid local search that combines adaptive subset flips (with dynamic neighborhood size based on solution quality) and dominance-aware swaps, prioritizing objective improvements through weighted marginal contributions while ensuring feasibility. The method dynamically adjusts exploration intensity based on solution quality and balances objective trade-offs through weighted improvements.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solutions with high crowding distance in the objective space\n        objectives = np.array([obj for _, obj in archive])\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding_distances = np.zeros(len(archive))\n        crowding_distances[sorted_indices[0]] = crowding_distances[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive)-1):\n            crowding_distances[sorted_indices[i]] = (objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]) / (objectives[sorted_indices[-1], 0] - objectives[sorted_indices[0], 0] + 1e-6)\n        sorted_indices = np.argsort(objectives[:, 1])\n        crowding_distances[sorted_indices[0]] = crowding_distances[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive)-1):\n            crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], 1] - objectives[sorted_indices[i-1], 1]) / (objectives[sorted_indices[-1], 1] - objectives[sorted_indices[0], 1] + 1e-6)\n        threshold = np.percentile(crowding_distances, 70)\n        candidates = [sol for (sol, _), cd in zip(archive, crowding_distances) if cd >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    total_items = len(new_solution)\n\n    # Dynamic neighborhood size based on solution quality\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    max_v1 = np.sum(value1_lst)\n    max_v2 = np.sum(value2_lst)\n    quality = (total_v1 / max_v1 + total_v2 / max_v2) / 2 if (max_v1 + max_v2) > 0 else 0.5\n    subset_size = max(1, int(total_items * (1 - quality) * 0.4))  # Larger subset for lower quality\n\n    # Hybrid local search: subset flip + dominance-aware swap\n    subset_indices = np.random.choice(total_items, size=subset_size, replace=False)\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement combining objective gains and marginal contributions\n        w1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n        w2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n        improvement = w1 * delta_v1 + w2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Dominance-aware swap: prioritize items that improve both objectives\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Select items that improve both objectives\n            improving_indices = [i for i in valid_indices if\n                               (new_solution[i] == 0 and value1_lst[i] > 0 and value2_lst[i] > 0) or\n                               (new_solution[i] == 1 and value1_lst[i] < 0 and value2_lst[i] < 0)]\n            if improving_indices:\n                idx = random.choice(improving_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n            else:\n                idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.824468349245623,
            1.0998099744319916
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solutions with high crowding distance in the objective space\n        objectives = np.array([obj for _, obj in archive])\n        sorted_indices = np.argsort(objectives[:, 0])\n        crowding_distances = np.zeros(len(archive))\n        crowding_distances[sorted_indices[0]] = crowding_distances[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive)-1):\n            crowding_distances[sorted_indices[i]] = (objectives[sorted_indices[i+1], 0] - objectives[sorted_indices[i-1], 0]) / (objectives[sorted_indices[-1], 0] - objectives[sorted_indices[0], 0] + 1e-6)\n        sorted_indices = np.argsort(objectives[:, 1])\n        crowding_distances[sorted_indices[0]] = crowding_distances[sorted_indices[-1]] = float('inf')\n        for i in range(1, len(archive)-1):\n            crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], 1] - objectives[sorted_indices[i-1], 1]) / (objectives[sorted_indices[-1], 1] - objectives[sorted_indices[0], 1] + 1e-6)\n        threshold = np.percentile(crowding_distances, 70)\n        candidates = [sol for (sol, _), cd in zip(archive, crowding_distances) if cd >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    total_items = len(new_solution)\n\n    # Dynamic neighborhood size based on solution quality\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    max_v1 = np.sum(value1_lst)\n    max_v2 = np.sum(value2_lst)\n    quality = (total_v1 / max_v1 + total_v2 / max_v2) / 2 if (max_v1 + max_v2) > 0 else 0.5\n    subset_size = max(1, int(total_items * (1 - quality) * 0.4))  # Larger subset for lower quality\n\n    # Hybrid local search: subset flip + dominance-aware swap\n    subset_indices = np.random.choice(total_items, size=subset_size, replace=False)\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement combining objective gains and marginal contributions\n        w1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n        w2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n        improvement = w1 * delta_v1 + w2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Dominance-aware swap: prioritize items that improve both objectives\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Select items that improve both objectives\n            improving_indices = [i for i in valid_indices if\n                               (new_solution[i] == 0 and value1_lst[i] > 0 and value2_lst[i] > 0) or\n                               (new_solution[i] == 1 and value1_lst[i] < 0 and value2_lst[i] < 0)]\n            if improving_indices:\n                idx = random.choice(improving_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n            else:\n                idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm prioritizes solutions with high normalized objective values by selecting from the top 30% of candidates, then applies a hybrid local search that dynamically adjusts weights between objectives based on the current solution's dominance. It explores a 30% random subset of items, flipping those that most improve a weighted sum of objectives while ensuring feasibility, with a fallback to diversity-aware flips using marginal contributions and item inclusion rates in the archive.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive neighborhood exploration\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Diversity-aware flip based on marginal contribution and solution diversity\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions and diversity scores\n            contributions = []\n            diversity_scores = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n                # Diversity score: higher if item is not frequently included in archive\n                inclusion_rate = sum(sol[i] for sol, _ in archive) / len(archive)\n                diversity_scores.append(1 - inclusion_rate)\n\n            # Combine contribution and diversity with temperature parameter\n            temperature = 0.7  # Controls exploration/exploitation trade-off\n            combined_scores = [c * (1 - temperature) + d * temperature for c, d in zip(contributions, diversity_scores)]\n\n            if sum(combined_scores) > 0:\n                probabilities = [c / sum(combined_scores) for c in combined_scores]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 35,
        "algorithm": "The algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values and low dominance counts, then applies a hybrid local search that dynamically adjusts weights based on objective ratios and item diversity, exploring neighborhoods through weighted improvements and adaptive swaps while ensuring feasibility through capacity checks. The method balances greedy improvements with diversification by considering both individual item flips and strategic swaps, using diversity-aware weights to guide the search toward high-quality, non-dominated solutions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution with highest sum of normalized objectives and lowest dominance count\n        normalized_scores = []\n        dominance_counts = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n\n            # Calculate dominance count (number of solutions that dominate this one)\n            dominance = 0\n            for _, other_obj in archive:\n                if (other_obj[0] >= obj[0] and other_obj[1] > obj[1]) or (other_obj[0] > obj[0] and other_obj[1] >= obj[1]):\n                    dominance += 1\n            dominance_counts.append(dominance)\n\n        # Select top 20% of solutions with lowest dominance counts\n        sorted_indices = sorted(range(len(normalized_scores)), key=lambda i: (-normalized_scores[i], dominance_counts[i]))\n        candidate_indices = sorted_indices[:max(1, int(0.2 * len(archive)))]\n        selected_idx = random.choice(candidate_indices)\n        base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on objective ratios and solution diversity\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    ratio_v1 = total_v1 / (total_v2 + 1e-6) if total_v2 > 0 else 1.0\n    ratio_v2 = total_v2 / (total_v1 + 1e-6) if total_v1 > 0 else 1.0\n\n    # Calculate diversity-aware weights\n    item_diversity = np.zeros(len(new_solution))\n    for i in range(len(new_solution)):\n        inclusion_rate = sum(sol[i] for sol, _ in archive) / len(archive)\n        item_diversity[i] = 1 - inclusion_rate\n\n    weight_v1 = ratio_v2 * (1 + item_diversity)\n    weight_v2 = ratio_v1 * (1 + item_diversity)\n\n    # Hybrid neighborhood exploration\n    subset_size = max(1, int(len(new_solution) * 0.4))  # 40% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement with diversity bonus\n        improvement = (weight_v1[idx] * delta_v1 + weight_v2[idx] * delta_v2) * (1 + 0.5 * item_diversity[idx])\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Adaptive swap operation for diversification\n        valid_pairs = []\n        for i in range(len(new_solution)):\n            if new_solution[i] == 1:\n                for j in range(len(new_solution)):\n                    if new_solution[j] == 0 and weight_lst[j] <= weight_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        valid_pairs.append((i, j))\n\n        if valid_pairs:\n            # Select pair with highest combined weighted improvement\n            best_pair = None\n            best_pair_improvement = -float('inf')\n\n            for i, j in valid_pairs:\n                delta_v1 = value1_lst[j] - value1_lst[i]\n                delta_v2 = value2_lst[j] - value2_lst[i]\n                improvement = weight_v1[j] * delta_v1 + weight_v2[j] * delta_v2\n\n                if improvement > best_pair_improvement:\n                    best_pair_improvement = improvement\n                    best_pair = (i, j)\n\n            if best_pair:\n                i, j = best_pair\n                new_solution[i] = 0\n                new_solution[j] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.9089630759814984,
            3.7565543055534363
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution with highest sum of normalized objectives and lowest dominance count\n        normalized_scores = []\n        dominance_counts = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n\n            # Calculate dominance count (number of solutions that dominate this one)\n            dominance = 0\n            for _, other_obj in archive:\n                if (other_obj[0] >= obj[0] and other_obj[1] > obj[1]) or (other_obj[0] > obj[0] and other_obj[1] >= obj[1]):\n                    dominance += 1\n            dominance_counts.append(dominance)\n\n        # Select top 20% of solutions with lowest dominance counts\n        sorted_indices = sorted(range(len(normalized_scores)), key=lambda i: (-normalized_scores[i], dominance_counts[i]))\n        candidate_indices = sorted_indices[:max(1, int(0.2 * len(archive)))]\n        selected_idx = random.choice(candidate_indices)\n        base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on objective ratios and solution diversity\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    ratio_v1 = total_v1 / (total_v2 + 1e-6) if total_v2 > 0 else 1.0\n    ratio_v2 = total_v2 / (total_v1 + 1e-6) if total_v1 > 0 else 1.0\n\n    # Calculate diversity-aware weights\n    item_diversity = np.zeros(len(new_solution))\n    for i in range(len(new_solution)):\n        inclusion_rate = sum(sol[i] for sol, _ in archive) / len(archive)\n        item_diversity[i] = 1 - inclusion_rate\n\n    weight_v1 = ratio_v2 * (1 + item_diversity)\n    weight_v2 = ratio_v1 * (1 + item_diversity)\n\n    # Hybrid neighborhood exploration\n    subset_size = max(1, int(len(new_solution) * 0.4))  # 40% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement with diversity bonus\n        improvement = (weight_v1[idx] * delta_v1 + weight_v2[idx] * delta_v2) * (1 + 0.5 * item_diversity[idx])\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Adaptive swap operation for diversification\n        valid_pairs = []\n        for i in range(len(new_solution)):\n            if new_solution[i] == 1:\n                for j in range(len(new_solution)):\n                    if new_solution[j] == 0 and weight_lst[j] <= weight_lst[i] and current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                        valid_pairs.append((i, j))\n\n        if valid_pairs:\n            # Select pair with highest combined weighted improvement\n            best_pair = None\n            best_pair_improvement = -float('inf')\n\n            for i, j in valid_pairs:\n                delta_v1 = value1_lst[j] - value1_lst[i]\n                delta_v2 = value2_lst[j] - value2_lst[i]\n                improvement = weight_v1[j] * delta_v1 + weight_v2[j] * delta_v2\n\n                if improvement > best_pair_improvement:\n                    best_pair_improvement = improvement\n                    best_pair = (i, j)\n\n            if best_pair:\n                i, j = best_pair\n                new_solution[i] = 0\n                new_solution[j] = 1\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm prioritizes solutions with high normalized objective values by selecting from the top 30% of candidates, then applies a hybrid local search that dynamically adjusts weights between objectives based on the current solution's dominance. It explores a 30% random subset of items, flipping those that most improve a weighted sum of objectives while ensuring feasibility, with a fallback to diversity-aware flips using marginal contributions and item inclusion rates in the archive.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive neighborhood exploration\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Diversity-aware flip based on marginal contribution and solution diversity\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions and diversity scores\n            contributions = []\n            diversity_scores = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n                # Diversity score: higher if item is not frequently included in archive\n                inclusion_rate = sum(sol[i] for sol, _ in archive) / len(archive)\n                diversity_scores.append(1 - inclusion_rate)\n\n            # Combine contribution and diversity with temperature parameter\n            temperature = 0.7  # Controls exploration/exploitation trade-off\n            combined_scores = [c * (1 - temperature) + d * temperature for c, d in zip(contributions, diversity_scores)]\n\n            if sum(combined_scores) > 0:\n                probabilities = [c / sum(combined_scores) for c in combined_scores]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 36,
        "algorithm": "The algorithm selects promising solutions from the top 50% of non-dominated candidates and applies a hybrid local search that dynamically adjusts objective weights based on their relative importance in the Pareto front, exploring a 40% random subset of items with a focus on marginal contributions while ensuring feasibility, and falls back to diversity-aware flips based on item frequency in the archive.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Calculate dominance ranks for each solution\n        dominance_counts = []\n        for i, (sol_i, obj_i) in enumerate(archive):\n            count = 0\n            for j, (sol_j, obj_j) in enumerate(archive):\n                if i != j and obj_j[0] >= obj_i[0] and obj_j[1] >= obj_i[1] and (obj_j[0] > obj_i[0] or obj_j[1] > obj_i[1]):\n                    count += 1\n            dominance_counts.append(count)\n\n        # Select top 50% of solutions with lowest dominance count (most non-dominated)\n        threshold = np.percentile(dominance_counts, 50)\n        candidates = [sol for (sol, _), count in zip(archive, dominance_counts) if count <= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on relative importance in Pareto front\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    if total_v1 == 0 or total_v2 == 0:\n        weight_v1 = weight_v2 = 0.5\n    else:\n        # Calculate relative importance in Pareto front\n        pareto_front = [obj for _, obj in archive]\n        v1_range = max(obj[0] for obj in pareto_front) - min(obj[0] for obj in pareto_front)\n        v2_range = max(obj[1] for obj in pareto_front) - min(obj[1] for obj in pareto_front)\n        weight_v1 = v2_range / (v1_range + v2_range + 1e-6)\n        weight_v2 = v1_range / (v1_range + v2_range + 1e-6)\n\n    # Adaptive neighborhood exploration (40% of items)\n    subset_size = max(1, int(len(new_solution) * 0.4))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Diversity-aware flip based on item frequency in archive\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate item frequencies in archive\n            item_frequencies = np.zeros(len(new_solution))\n            for sol, _ in archive:\n                item_frequencies += sol\n\n            # Prefer items that are less frequent in the archive\n            diversity_scores = 1 - (item_frequencies / len(archive))\n            valid_diversity = diversity_scores[valid_indices]\n\n            if np.sum(valid_diversity) > 0:\n                probabilities = valid_diversity / np.sum(valid_diversity)\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.6930273783608492,
            1.5446853935718536
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Calculate dominance ranks for each solution\n        dominance_counts = []\n        for i, (sol_i, obj_i) in enumerate(archive):\n            count = 0\n            for j, (sol_j, obj_j) in enumerate(archive):\n                if i != j and obj_j[0] >= obj_i[0] and obj_j[1] >= obj_i[1] and (obj_j[0] > obj_i[0] or obj_j[1] > obj_i[1]):\n                    count += 1\n            dominance_counts.append(count)\n\n        # Select top 50% of solutions with lowest dominance count (most non-dominated)\n        threshold = np.percentile(dominance_counts, 50)\n        candidates = [sol for (sol, _), count in zip(archive, dominance_counts) if count <= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on relative importance in Pareto front\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    if total_v1 == 0 or total_v2 == 0:\n        weight_v1 = weight_v2 = 0.5\n    else:\n        # Calculate relative importance in Pareto front\n        pareto_front = [obj for _, obj in archive]\n        v1_range = max(obj[0] for obj in pareto_front) - min(obj[0] for obj in pareto_front)\n        v2_range = max(obj[1] for obj in pareto_front) - min(obj[1] for obj in pareto_front)\n        weight_v1 = v2_range / (v1_range + v2_range + 1e-6)\n        weight_v2 = v1_range / (v1_range + v2_range + 1e-6)\n\n    # Adaptive neighborhood exploration (40% of items)\n    subset_size = max(1, int(len(new_solution) * 0.4))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Diversity-aware flip based on item frequency in archive\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate item frequencies in archive\n            item_frequencies = np.zeros(len(new_solution))\n            for sol, _ in archive:\n                item_frequencies += sol\n\n            # Prefer items that are less frequent in the archive\n            diversity_scores = 1 - (item_frequencies / len(archive))\n            valid_diversity = diversity_scores[valid_indices]\n\n            if np.sum(valid_diversity) > 0:\n                probabilities = valid_diversity / np.sum(valid_diversity)\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 3 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm prioritizes solutions with high normalized objective values by selecting from the top 30% of candidates, then applies a hybrid local search that dynamically adjusts weights between objectives based on the current solution's dominance. It explores a 30% random subset of items, flipping those that most improve a weighted sum of objectives while ensuring feasibility, with a fallback to diversity-aware flips using marginal contributions and item inclusion rates in the archive.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive neighborhood exploration\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Diversity-aware flip based on marginal contribution and solution diversity\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions and diversity scores\n            contributions = []\n            diversity_scores = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n                # Diversity score: higher if item is not frequently included in archive\n                inclusion_rate = sum(sol[i] for sol, _ in archive) / len(archive)\n                diversity_scores.append(1 - inclusion_rate)\n\n            # Combine contribution and diversity with temperature parameter\n            temperature = 0.7  # Controls exploration/exploitation trade-off\n            combined_scores = [c * (1 - temperature) + d * temperature for c, d in zip(contributions, diversity_scores)]\n\n            if sum(combined_scores) > 0:\n                probabilities = [c / sum(combined_scores) for c in combined_scores]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s a concise, high-impact heuristic design for the BI-KP:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 37,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Calculate crowding distance for each solution\n        objectives = [obj for _, obj in archive]\n        v1_sorted = sorted([obj[0] for obj in objectives])\n        v2_sorted = sorted([obj[1] for obj in objectives])\n\n        crowding_distances = []\n        for i in range(len(archive)):\n            if i == 0 or i == len(archive) - 1:\n                crowding_distances.append(float('inf'))\n            else:\n                v1_dist = (v1_sorted[i+1] - v1_sorted[i-1]) / (max(v1_sorted) - min(v1_sorted) + 1e-6)\n                v2_dist = (v2_sorted[i+1] - v2_sorted[i-1]) / (max(v2_sorted) - min(v2_sorted) + 1e-6)\n                crowding_distances.append(v1_dist + v2_dist)\n\n        # Select top 40% solutions by crowding distance and choose randomly among them\n        threshold = np.percentile(crowding_distances, 60)\n        candidates = [sol for (sol, _), dist in zip(archive, crowding_distances) if dist >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive neighborhood size based on solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(weight_lst) + 1e-6)\n    neighborhood_size = max(1, int(len(new_solution) * (0.3 + 0.2 * (1 - solution_quality))))\n    subset_indices = np.random.choice(len(new_solution), size=neighborhood_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Precompute potential contributions for feasibility checks\n    potential_contributions = []\n    for idx in range(len(new_solution)):\n        if new_solution[idx] == 1:\n            potential_contributions.append((-value1_lst[idx], -value2_lst[idx], -weight_lst[idx]))\n        else:\n            potential_contributions.append((value1_lst[idx], value2_lst[idx], weight_lst[idx]))\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight + potential_contributions[idx][2]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = potential_contributions[idx][0]\n        delta_v2 = potential_contributions[idx][1]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Diversity-aware flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight + potential_contributions[i][2] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + potential_contributions[i][2] <= capacity)]\n\n        if valid_indices:\n            # Calculate combined scores of contribution and diversity\n            contributions = []\n            diversity_scores = []\n            for i in valid_indices:\n                contribution = weight_v1 * potential_contributions[i][0] + weight_v2 * potential_contributions[i][1]\n                contributions.append(contribution)\n\n                # Diversity score: higher if item is not frequently included in archive\n                inclusion_rate = sum(sol[i] for sol, _ in archive) / len(archive)\n                diversity_scores.append(1 - inclusion_rate)\n\n            # Adaptive temperature based on solution quality\n            temperature = 0.3 + 0.4 * (1 - solution_quality)\n            combined_scores = [c * (1 - temperature) + d * temperature for c, d in zip(contributions, diversity_scores)]\n\n            if sum(combined_scores) > 0:\n                probabilities = [c / sum(combined_scores) for c in combined_scores]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.883032251620399,
            1.2218582034111023
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Calculate crowding distance for each solution\n        objectives = [obj for _, obj in archive]\n        v1_sorted = sorted([obj[0] for obj in objectives])\n        v2_sorted = sorted([obj[1] for obj in objectives])\n\n        crowding_distances = []\n        for i in range(len(archive)):\n            if i == 0 or i == len(archive) - 1:\n                crowding_distances.append(float('inf'))\n            else:\n                v1_dist = (v1_sorted[i+1] - v1_sorted[i-1]) / (max(v1_sorted) - min(v1_sorted) + 1e-6)\n                v2_dist = (v2_sorted[i+1] - v2_sorted[i-1]) / (max(v2_sorted) - min(v2_sorted) + 1e-6)\n                crowding_distances.append(v1_dist + v2_dist)\n\n        # Select top 40% solutions by crowding distance and choose randomly among them\n        threshold = np.percentile(crowding_distances, 60)\n        candidates = [sol for (sol, _), dist in zip(archive, crowding_distances) if dist >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive neighborhood size based on solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(weight_lst) + 1e-6)\n    neighborhood_size = max(1, int(len(new_solution) * (0.3 + 0.2 * (1 - solution_quality))))\n    subset_indices = np.random.choice(len(new_solution), size=neighborhood_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Precompute potential contributions for feasibility checks\n    potential_contributions = []\n    for idx in range(len(new_solution)):\n        if new_solution[idx] == 1:\n            potential_contributions.append((-value1_lst[idx], -value2_lst[idx], -weight_lst[idx]))\n        else:\n            potential_contributions.append((value1_lst[idx], value2_lst[idx], weight_lst[idx]))\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight + potential_contributions[idx][2]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = potential_contributions[idx][0]\n        delta_v2 = potential_contributions[idx][1]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Diversity-aware flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight + potential_contributions[i][2] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + potential_contributions[i][2] <= capacity)]\n\n        if valid_indices:\n            # Calculate combined scores of contribution and diversity\n            contributions = []\n            diversity_scores = []\n            for i in valid_indices:\n                contribution = weight_v1 * potential_contributions[i][0] + weight_v2 * potential_contributions[i][1]\n                contributions.append(contribution)\n\n                # Diversity score: higher if item is not frequently included in archive\n                inclusion_rate = sum(sol[i] for sol, _ in archive) / len(archive)\n                diversity_scores.append(1 - inclusion_rate)\n\n            # Adaptive temperature based on solution quality\n            temperature = 0.3 + 0.4 * (1 - solution_quality)\n            combined_scores = [c * (1 - temperature) + d * temperature for c, d in zip(contributions, diversity_scores)]\n\n            if sum(combined_scores) > 0:\n                probabilities = [c / sum(combined_scores) for c in combined_scores]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 4 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing those with high objective diversity (via crowding distance) and applies a hybrid local search combining item swaps and subset replacements to generate a feasible neighbor solution while ensuring weight constraints are not violated. It first evaluates solutions based on their crowding distances to identify promising candidates, then performs targeted swaps and subset operations to explore the neighborhood while maintaining feasibility. The algorithm balances exploration and exploitation by focusing on high-diversity regions of the Pareto front while dynamically adjusting the search based on the current solution's characteristics.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high objective diversity and low crowding distance)\n    # Calculate crowding distances for solutions in the archive\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n    objectives_sorted = objectives[sorted_indices]\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance (most promising for improvement)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Randomly swap items if feasible\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Strategy 2: Replace a subset of items with new ones if feasible\n    subset_size = min(3, n_items // 4)\n    if subset_size > 0:\n        # Remove a subset of items\n        remove_indices = np.where(new_solution == 1)[0]\n        if len(remove_indices) > 0:\n            remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Add a subset of new items\n        add_indices = np.where(new_solution == 0)[0]\n        if len(add_indices) > 0:\n            add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive selection - prioritize solutions with high crowding distance and normalized objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance with normalized objective diversity\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_diversity = np.zeros(len(archive))\n    for i, (v1, v2) in enumerate(objectives):\n        normalized_diversity[i] = (v1 / max_v1 + v2 / max_v2) if max_v1 > 0 and max_v2 > 0 else 0\n\n    selection_scores = crowding_distances * normalized_diversity\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search - adaptive subset flips with objective-weighted improvements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic neighborhood size based on solution quality\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Strategy 1: Adaptive subset flip with objective-weighted selection\n    for _ in range(neighborhood_size):\n        # Select a random subset of items to consider\n        subset_size = min(3, max(1, int(n_items * 0.2)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            # Try flipping the item\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            # Calculate new weight\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            # Check feasibility\n            if new_weight > capacity:\n                continue\n\n            # Calculate objective improvements\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-weighted improvement\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Strategy 2: Objective-weighted item swaps if no improvement found\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    # Objective-weighted swap decision\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design for the BI-KP, optimized for high-quality, feasible neighbors:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 38,
        "algorithm": "The algorithm selects high-crowding solutions from the archive (prioritizing those with the most \"room\" for improvement) and applies a dynamic local search that flips item subsets based on weighted improvements, ensuring feasibility through precomputed weight checks. It uses adaptive neighborhood sizes and objective-weighted swaps when no direct improvements are found, balancing exploration and exploitation in the Pareto front.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive selection - prioritize high-crowding solutions\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search - dynamic subset flips with objective-weighted improvements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.2)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Objective-weighted swaps if no improvement found\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        break\n\n    return new_solution\n\n",
        "score": [
            -0.7528915405485983,
            2.3339934051036835
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive selection - prioritize high-crowding solutions\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search - dynamic subset flips with objective-weighted improvements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.2)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Objective-weighted swaps if no improvement found\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        break\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a non-dominated solution from the archive using a Pareto-ranked crowding-distance metric, then applies an adaptive hybrid local search that combines subset flips and objective-weighted swaps to explore high-quality neighbors while ensuring feasibility. It prioritizes items with higher combined value-to-weight ratios and dynamically adjusts neighborhood size based on solution quality and capacity utilization. The search alternates between flipping subsets of items and performing targeted swaps when no improvements are found in the first phase.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high Pareto-ranked crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance with Pareto ranking\n    pareto_rank = np.zeros(len(archive))\n    for i in range(len(archive)):\n        dominated = False\n        for j in range(len(archive)):\n            if i != j and objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1]):\n                dominated = True\n                break\n        pareto_rank[i] = 0 if dominated else 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Adaptive hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic neighborhood size based on solution quality\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Strategy 1: Adaptive subset flip with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(3, max(1, int(n_items * 0.2)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Strategy 2: Objective-weighted item swaps if no improvement found\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThis algorithm selects a promising solution from the archive by prioritizing those with high value ratios (value1/value2), then applies a weighted local search where items are flipped based on their potential to improve a dynamically adjusted weighted combination of the two objectives. If no immediate improvement is found, it probabilistically flips items based on their potential contribution, ensuring feasibility by checking weight constraints.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the ratio of objectives to prioritize diverse solutions\n        ratios = []\n        for _, obj in archive:\n            ratio = obj[0] / (obj[1] + 1e-6) if obj[1] > 0 else float('inf')\n            ratios.append(ratio)\n        # Select top 20% of solutions and choose randomly among them\n        threshold = np.percentile(ratios, 80)\n        candidates = [sol for (sol, _), r in zip(archive, ratios) if r >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's ratio\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    ratio = total_v1 / (total_v2 + 1e-6) if total_v2 > 0 else float('inf')\n    weight_v1 = 0.7 if ratio > 1 else 0.3\n    weight_v2 = 1 - weight_v1\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially flip\n    # 2. Evaluate potential improvements with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions with adjusted weights\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[idx]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design for the BI-KP, optimized for high-quality, feasible neighbors:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 39,
        "algorithm": "The heuristic algorithm selects a solution from the archive using a weighted crowding-distance metric (prioritizing non-dominated solutions with higher crowding distances), then applies an adaptive two-phase local search: first flipping items in a targeted subset based on dynamic value-weight ratios (weighted by the solution's current objective balance), and if no improvement is found, performing probabilistic objective-weighted swaps to explore better neighbors while always ensuring feasibility. The method dynamically adjusts exploration based on the solution's current capacity utilization and objective balance.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest weighted crowding-distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Weight crowding distances by Pareto rank (non-dominated solutions get higher weight)\n    pareto_rank = np.zeros(len(archive))\n    for i in range(len(archive)):\n        dominated = False\n        for j in range(len(archive)):\n            if i != j and objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1]):\n                dominated = True\n                break\n        pareto_rank[i] = 1 if not dominated else 0.5  # Non-dominated get higher weight\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Adaptive two-phase local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Phase 1: Targeted subset flips based on dynamic value-weight ratio\n    subset_size = max(3, int(n_items * 0.15))\n    subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Calculate current solution's objective balance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    ratio = total_v1 / (total_v2 + 1e-6) if total_v2 > 0 else float('inf')\n    weight_v1 = 0.6 if ratio > 1 else 0.4\n    weight_v2 = 1 - weight_v1\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering objective balance\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n        normalized_improvement = improvement / (weight_lst[idx] + 1e-6)\n\n        if normalized_improvement > best_improvement:\n            best_improvement = normalized_improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Phase 2: Probabilistic objective-weighted swaps\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            # Calculate swap potential based on weighted objectives\n            swap_potentials = []\n            for out_idx in out_items:\n                for in_idx in in_items:\n                    new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                    if new_weight <= capacity:\n                        delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                        delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                        potential = weight_v1 * delta_v1 + weight_v2 * delta_v2\n                        swap_potentials.append((potential, out_idx, in_idx))\n\n            if swap_potentials:\n                # Select swap with highest potential\n                best_potential, best_out, best_in = max(swap_potentials, key=lambda x: x[0])\n                if best_potential > 0:\n                    new_solution[best_out] = 0\n                    new_solution[best_in] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.7609199274775342,
            1.1375239193439484
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest weighted crowding-distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Weight crowding distances by Pareto rank (non-dominated solutions get higher weight)\n    pareto_rank = np.zeros(len(archive))\n    for i in range(len(archive)):\n        dominated = False\n        for j in range(len(archive)):\n            if i != j and objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1]):\n                dominated = True\n                break\n        pareto_rank[i] = 1 if not dominated else 0.5  # Non-dominated get higher weight\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Adaptive two-phase local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Phase 1: Targeted subset flips based on dynamic value-weight ratio\n    subset_size = max(3, int(n_items * 0.15))\n    subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Calculate current solution's objective balance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    ratio = total_v1 / (total_v2 + 1e-6) if total_v2 > 0 else float('inf')\n    weight_v1 = 0.6 if ratio > 1 else 0.4\n    weight_v2 = 1 - weight_v1\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering objective balance\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n        normalized_improvement = improvement / (weight_lst[idx] + 1e-6)\n\n        if normalized_improvement > best_improvement:\n            best_improvement = normalized_improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Phase 2: Probabilistic objective-weighted swaps\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            # Calculate swap potential based on weighted objectives\n            swap_potentials = []\n            for out_idx in out_items:\n                for in_idx in in_items:\n                    new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                    if new_weight <= capacity:\n                        delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                        delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                        potential = weight_v1 * delta_v1 + weight_v2 * delta_v2\n                        swap_potentials.append((potential, out_idx, in_idx))\n\n            if swap_potentials:\n                # Select swap with highest potential\n                best_potential, best_out, best_in = max(swap_potentials, key=lambda x: x[0])\n                if best_potential > 0:\n                    new_solution[best_out] = 0\n                    new_solution[best_in] = 1\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThis algorithm selects a promising solution from the archive by prioritizing those with high value ratios (value1/value2), then applies a weighted local search where items are flipped based on their potential to improve a dynamically adjusted weighted combination of the two objectives. If no immediate improvement is found, it probabilistically flips items based on their potential contribution, ensuring feasibility by checking weight constraints.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the ratio of objectives to prioritize diverse solutions\n        ratios = []\n        for _, obj in archive:\n            ratio = obj[0] / (obj[1] + 1e-6) if obj[1] > 0 else float('inf')\n            ratios.append(ratio)\n        # Select top 20% of solutions and choose randomly among them\n        threshold = np.percentile(ratios, 80)\n        candidates = [sol for (sol, _), r in zip(archive, ratios) if r >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's ratio\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    ratio = total_v1 / (total_v2 + 1e-6) if total_v2 > 0 else float('inf')\n    weight_v1 = 0.7 if ratio > 1 else 0.3\n    weight_v2 = 1 - weight_v1\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially flip\n    # 2. Evaluate potential improvements with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions with adjusted weights\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[idx]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design for the BI-KP, optimized for high-quality, feasible neighbors:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 40,
        "algorithm": "The algorithm selects a solution from the archive using a weighted crowding-distance metric to prioritize high-diversity candidates, then applies a hybrid local search that dynamically adjusts neighborhood size (20-50% of items) and uses objective-weighted flips (prioritizing either value1 or value2 based on current balance) to generate feasible neighbors, with a fallback to greedy feasibility checks when no improvements are found.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution using weighted crowding-distance\n    crowding_distances = []\n    for sol, obj in archive:\n        total_v1 = np.sum(value1_lst[sol == 1])\n        total_v2 = np.sum(value2_lst[sol == 1])\n        # Simplified crowding distance (for illustration)\n        crowding_dist = total_v1 + total_v2\n        crowding_distances.append(crowding_dist)\n\n    if len(crowding_distances) > 1:\n        sorted_indices = np.argsort(crowding_distances)[::-1]\n        top_indices = sorted_indices[:max(1, len(sorted_indices) // 2)]\n        selected_idx = random.choice(top_indices)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size (20-50% of items)\n    neighborhood_size = random.randint(int(len(new_solution) * 0.2), int(len(new_solution) * 0.5))\n    subset_indices = np.random.choice(len(new_solution), size=neighborhood_size, replace=False)\n\n    # Objective-weighted improvements\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    ratio = total_v1 / (total_v2 + 1e-6) if total_v2 > 0 else float('inf')\n    weight_v1 = 0.7 if ratio > 1 else 0.3\n    weight_v2 = 1 - weight_v1\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy feasibility check: flip the least impactful item if no improvement\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Flip the item with minimal impact on objectives\n            impacts = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                impact = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                impacts.append(abs(impact))\n            if impacts:\n                idx = valid_indices[np.argmin(impacts)]\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8210200988784186,
            1.3254239857196808
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a solution using weighted crowding-distance\n    crowding_distances = []\n    for sol, obj in archive:\n        total_v1 = np.sum(value1_lst[sol == 1])\n        total_v2 = np.sum(value2_lst[sol == 1])\n        # Simplified crowding distance (for illustration)\n        crowding_dist = total_v1 + total_v2\n        crowding_distances.append(crowding_dist)\n\n    if len(crowding_distances) > 1:\n        sorted_indices = np.argsort(crowding_distances)[::-1]\n        top_indices = sorted_indices[:max(1, len(sorted_indices) // 2)]\n        selected_idx = random.choice(top_indices)\n    else:\n        selected_idx = 0\n\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size (20-50% of items)\n    neighborhood_size = random.randint(int(len(new_solution) * 0.2), int(len(new_solution) * 0.5))\n    subset_indices = np.random.choice(len(new_solution), size=neighborhood_size, replace=False)\n\n    # Objective-weighted improvements\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    ratio = total_v1 / (total_v2 + 1e-6) if total_v2 > 0 else float('inf')\n    weight_v1 = 0.7 if ratio > 1 else 0.3\n    weight_v2 = 1 - weight_v1\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy feasibility check: flip the least impactful item if no improvement\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Flip the item with minimal impact on objectives\n            impacts = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                impact = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                impacts.append(abs(impact))\n            if impacts:\n                idx = valid_indices[np.argmin(impacts)]\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        This algorithm selects a promising solution from the archive by prioritizing those with high value ratios (value1/value2), then applies a weighted local search where items are flipped based on their potential to improve a dynamically adjusted weighted combination of the two objectives. If no immediate improvement is found, it probabilistically flips items based on their potential contribution, ensuring feasibility by checking weight constraints.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the ratio of objectives to prioritize diverse solutions\n        ratios = []\n        for _, obj in archive:\n            ratio = obj[0] / (obj[1] + 1e-6) if obj[1] > 0 else float('inf')\n            ratios.append(ratio)\n        # Select top 20% of solutions and choose randomly among them\n        threshold = np.percentile(ratios, 80)\n        candidates = [sol for (sol, _), r in zip(archive, ratios) if r >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's ratio\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    ratio = total_v1 / (total_v2 + 1e-6) if total_v2 > 0 else float('inf')\n    weight_v1 = 0.7 if ratio > 1 else 0.3\n    weight_v2 = 1 - weight_v1\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially flip\n    # 2. Evaluate potential improvements with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions with adjusted weights\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[idx]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 41,
        "algorithm": "The algorithm selects a promising solution from the archive using a non-linear dominance-based approach, then applies a dynamic multi-objective local search that combines greedy improvement with probabilistic exploration, balancing objectives with adaptive weights based on capacity utilization and solution diversity. It prioritizes high-utility items (top 15%) and flips their inclusion/exclusion probabilistically, ensuring feasibility while emphasizing better-performing items. The utility function dynamically adjusts weights between objectives (60%/40% or 40%/60%) based on current capacity usage, favoring value1 when underutilized and value2 when nearing capacity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Non-linear dominance-based selection\n        dominance_scores = []\n        for sol, obj in archive:\n            total_v1, total_v2 = obj\n            # Calculate dominance score based on non-linear combination\n            dominance = (total_v1 ** 0.7) * (total_v2 ** 0.3) / (np.sum(weight_lst[sol == 1]) + 1e-6)\n            dominance_scores.append(dominance)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(dominance_scores, 70)\n        candidates = [sol for (sol, _), d in zip(archive, dominance_scores) if d >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on capacity utilization\n    capacity_utilization = current_weight / capacity\n    weight_v1 = 0.6 if capacity_utilization < 0.7 else 0.4\n    weight_v2 = 1 - weight_v1\n\n    # Novel local search strategy:\n    # 1. Calculate utility scores for all items\n    # 2. Select top-k items based on utility\n    # 3. Apply probabilistic flipping based on utility and weight constraints\n\n    # Calculate utility scores\n    utility_scores = []\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # For included items, calculate utility of removal\n            utility = - (weight_v1 * value1_lst[i] + weight_v2 * value2_lst[i])\n        else:\n            # For excluded items, calculate utility of addition if feasible\n            if current_weight + weight_lst[i] <= capacity:\n                utility = weight_v1 * value1_lst[i] + weight_v2 * value2_lst[i]\n            else:\n                utility = -float('inf')  # Not feasible to add\n\n        utility_scores.append(utility)\n\n    # Select top 15% of items with highest utility\n    k = max(1, int(len(new_solution) * 0.15))\n    top_indices = np.argsort(utility_scores)[-k:]\n\n    # Apply probabilistic flipping\n    for idx in top_indices:\n        # Calculate probability based on normalized utility\n        max_utility = max(utility_scores)\n        if max_utility > 0:\n            prob = (utility_scores[idx] / max_utility) ** 2  # Squared to emphasize better options\n        else:\n            prob = 0.5  # Default probability if no positive utility\n\n        if random.random() < prob:\n            new_solution[idx] = 1 - new_solution[idx]\n            # Update current weight\n            if new_solution[idx] == 1:\n                current_weight += weight_lst[idx]\n            else:\n                current_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8999875148015601,
            1.8893332481384277
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Non-linear dominance-based selection\n        dominance_scores = []\n        for sol, obj in archive:\n            total_v1, total_v2 = obj\n            # Calculate dominance score based on non-linear combination\n            dominance = (total_v1 ** 0.7) * (total_v2 ** 0.3) / (np.sum(weight_lst[sol == 1]) + 1e-6)\n            dominance_scores.append(dominance)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(dominance_scores, 70)\n        candidates = [sol for (sol, _), d in zip(archive, dominance_scores) if d >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on capacity utilization\n    capacity_utilization = current_weight / capacity\n    weight_v1 = 0.6 if capacity_utilization < 0.7 else 0.4\n    weight_v2 = 1 - weight_v1\n\n    # Novel local search strategy:\n    # 1. Calculate utility scores for all items\n    # 2. Select top-k items based on utility\n    # 3. Apply probabilistic flipping based on utility and weight constraints\n\n    # Calculate utility scores\n    utility_scores = []\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # For included items, calculate utility of removal\n            utility = - (weight_v1 * value1_lst[i] + weight_v2 * value2_lst[i])\n        else:\n            # For excluded items, calculate utility of addition if feasible\n            if current_weight + weight_lst[i] <= capacity:\n                utility = weight_v1 * value1_lst[i] + weight_v2 * value2_lst[i]\n            else:\n                utility = -float('inf')  # Not feasible to add\n\n        utility_scores.append(utility)\n\n    # Select top 15% of items with highest utility\n    k = max(1, int(len(new_solution) * 0.15))\n    top_indices = np.argsort(utility_scores)[-k:]\n\n    # Apply probabilistic flipping\n    for idx in top_indices:\n        # Calculate probability based on normalized utility\n        max_utility = max(utility_scores)\n        if max_utility > 0:\n            prob = (utility_scores[idx] / max_utility) ** 2  # Squared to emphasize better options\n        else:\n            prob = 0.5  # Default probability if no positive utility\n\n        if random.random() < prob:\n            new_solution[idx] = 1 - new_solution[idx]\n            # Update current weight\n            if new_solution[idx] == 1:\n                current_weight += weight_lst[idx]\n            else:\n                current_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        This algorithm selects a promising solution from the archive by prioritizing those with high value ratios (value1/value2), then applies a weighted local search where items are flipped based on their potential to improve a dynamically adjusted weighted combination of the two objectives. If no immediate improvement is found, it probabilistically flips items based on their potential contribution, ensuring feasibility by checking weight constraints.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the ratio of objectives to prioritize diverse solutions\n        ratios = []\n        for _, obj in archive:\n            ratio = obj[0] / (obj[1] + 1e-6) if obj[1] > 0 else float('inf')\n            ratios.append(ratio)\n        # Select top 20% of solutions and choose randomly among them\n        threshold = np.percentile(ratios, 80)\n        candidates = [sol for (sol, _), r in zip(archive, ratios) if r >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's ratio\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    ratio = total_v1 / (total_v2 + 1e-6) if total_v2 > 0 else float('inf')\n    weight_v1 = 0.7 if ratio > 1 else 0.3\n    weight_v2 = 1 - weight_v1\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially flip\n    # 2. Evaluate potential improvements with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions with adjusted weights\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[idx]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 42,
        "algorithm": "The algorithm selects promising solutions from the archive by prioritizing those with high value ratios (value1/value2) and applies a two-phase local search: first exploring large-scale flips (30% of items) and then refining with small-scale flips (10% of items), dynamically adjusting weights to balance the two objectives. It ensures feasibility by rejecting weight-exceeding flips and uses probabilistic selection for small improvements when no clear candidate exists.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the ratio of objectives to prioritize diverse solutions\n        ratios = []\n        for _, obj in archive:\n            ratio = obj[0] / (obj[1] + 1e-6) if obj[1] > 0 else float('inf')\n            ratios.append(ratio)\n        # Select top 20% of solutions and choose randomly among them\n        threshold = np.percentile(ratios, 80)\n        candidates = [sol for (sol, _), r in zip(archive, ratios) if r >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's ratio\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    ratio = total_v1 / (total_v2 + 1e-6) if total_v2 > 0 else float('inf')\n    weight_v1 = 0.6 if ratio > 1 else 0.4\n    weight_v2 = 1 - weight_v1\n\n    # Multi-phase local search strategy:\n    # Phase 1: Large-scale changes (flip multiple items)\n    # Phase 2: Small-scale refinement (flip single items)\n\n    # Phase 1: Large-scale changes\n    phase1_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    phase1_indices = np.random.choice(len(new_solution), size=phase1_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in phase1_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n        current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 2: Small-scale refinement\n    subset_size = max(1, int(len(new_solution) * 0.1))  # 10% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                       (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                       (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions with adjusted weights\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.6974198528538408,
            0.8143457770347595
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the ratio of objectives to prioritize diverse solutions\n        ratios = []\n        for _, obj in archive:\n            ratio = obj[0] / (obj[1] + 1e-6) if obj[1] > 0 else float('inf')\n            ratios.append(ratio)\n        # Select top 20% of solutions and choose randomly among them\n        threshold = np.percentile(ratios, 80)\n        candidates = [sol for (sol, _), r in zip(archive, ratios) if r >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's ratio\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    ratio = total_v1 / (total_v2 + 1e-6) if total_v2 > 0 else float('inf')\n    weight_v1 = 0.6 if ratio > 1 else 0.4\n    weight_v2 = 1 - weight_v1\n\n    # Multi-phase local search strategy:\n    # Phase 1: Large-scale changes (flip multiple items)\n    # Phase 2: Small-scale refinement (flip single items)\n\n    # Phase 1: Large-scale changes\n    phase1_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    phase1_indices = np.random.choice(len(new_solution), size=phase1_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in phase1_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n        current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Phase 2: Small-scale refinement\n    subset_size = max(1, int(len(new_solution) * 0.1))  # 10% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                       (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                       (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions with adjusted weights\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 3 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThis algorithm selects a promising solution from the archive by prioritizing those with high value ratios (value1/value2), then applies a weighted local search where items are flipped based on their potential to improve a dynamically adjusted weighted combination of the two objectives. If no immediate improvement is found, it probabilistically flips items based on their potential contribution, ensuring feasibility by checking weight constraints.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the ratio of objectives to prioritize diverse solutions\n        ratios = []\n        for _, obj in archive:\n            ratio = obj[0] / (obj[1] + 1e-6) if obj[1] > 0 else float('inf')\n            ratios.append(ratio)\n        # Select top 20% of solutions and choose randomly among them\n        threshold = np.percentile(ratios, 80)\n        candidates = [sol for (sol, _), r in zip(archive, ratios) if r >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's ratio\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    ratio = total_v1 / (total_v2 + 1e-6) if total_v2 > 0 else float('inf')\n    weight_v1 = 0.7 if ratio > 1 else 0.3\n    weight_v2 = 1 - weight_v1\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially flip\n    # 2. Evaluate potential improvements with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions with adjusted weights\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[idx]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design for the BI-KP, optimized for high-quality, feasible neighbors:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 43,
        "algorithm": "The algorithm selects a promising solution from an archive using crowding distance to prioritize diverse solutions, then applies a hybrid local search with dynamic neighborhood size (20-50% of items) and objective-weighted flips. If no improvement is found, it performs a probabilistic flip based on potential contributions while ensuring feasibility through weight checks. The method balances exploration and exploitation by dynamically adjusting weights and neighborhood size based on the current solution's objective ratios.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Calculate crowding distance for each solution\n        solutions = [sol for sol, _ in archive]\n        objectives = np.array([obj for _, obj in archive])\n        crowding_distances = np.zeros(len(archive))\n\n        for m in range(2):  # For each objective\n            sorted_indices = np.argsort(objectives[:, m])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(archive)-1):\n                if objectives[sorted_indices[i+1], m] == objectives[sorted_indices[i-1], m]:\n                    continue\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], m] - objectives[sorted_indices[i-1], m]) / (objectives[sorted_indices[-1], m] - objectives[sorted_indices[0], m] + 1e-6)\n\n        # Select top 30% solutions based on crowding distance\n        threshold = np.percentile(crowding_distances, 70)\n        candidates = [sol for sol, dist in zip(solutions, crowding_distances) if dist >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's ratio\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    ratio = total_v1 / (total_v2 + 1e-6) if total_v2 > 0 else float('inf')\n    weight_v1 = 0.6 if ratio > 1 else 0.4\n    weight_v2 = 1 - weight_v1\n\n    # Dynamic neighborhood size (20-50% of items)\n    neighborhood_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - ratio / (ratio + 1e-6)))))\n    subset_indices = np.random.choice(len(new_solution), size=neighborhood_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.831789880468639,
            1.040635585784912
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Calculate crowding distance for each solution\n        solutions = [sol for sol, _ in archive]\n        objectives = np.array([obj for _, obj in archive])\n        crowding_distances = np.zeros(len(archive))\n\n        for m in range(2):  # For each objective\n            sorted_indices = np.argsort(objectives[:, m])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(archive)-1):\n                if objectives[sorted_indices[i+1], m] == objectives[sorted_indices[i-1], m]:\n                    continue\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], m] - objectives[sorted_indices[i-1], m]) / (objectives[sorted_indices[-1], m] - objectives[sorted_indices[0], m] + 1e-6)\n\n        # Select top 30% solutions based on crowding distance\n        threshold = np.percentile(crowding_distances, 70)\n        candidates = [sol for sol, dist in zip(solutions, crowding_distances) if dist >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's ratio\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    ratio = total_v1 / (total_v2 + 1e-6) if total_v2 > 0 else float('inf')\n    weight_v1 = 0.6 if ratio > 1 else 0.4\n    weight_v2 = 1 - weight_v1\n\n    # Dynamic neighborhood size (20-50% of items)\n    neighborhood_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - ratio / (ratio + 1e-6)))))\n    subset_indices = np.random.choice(len(new_solution), size=neighborhood_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 4 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing those with high objective diversity (via crowding distance) and applies a hybrid local search combining item swaps and subset replacements to generate a feasible neighbor solution while ensuring weight constraints are not violated. It first evaluates solutions based on their crowding distances to identify promising candidates, then performs targeted swaps and subset operations to explore the neighborhood while maintaining feasibility. The algorithm balances exploration and exploitation by focusing on high-diversity regions of the Pareto front while dynamically adjusting the search based on the current solution's characteristics.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high objective diversity and low crowding distance)\n    # Calculate crowding distances for solutions in the archive\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n    objectives_sorted = objectives[sorted_indices]\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance (most promising for improvement)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Randomly swap items if feasible\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Strategy 2: Replace a subset of items with new ones if feasible\n    subset_size = min(3, n_items // 4)\n    if subset_size > 0:\n        # Remove a subset of items\n        remove_indices = np.where(new_solution == 1)[0]\n        if len(remove_indices) > 0:\n            remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Add a subset of new items\n        add_indices = np.where(new_solution == 0)[0]\n        if len(add_indices) > 0:\n            add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive selection - prioritize solutions with high crowding distance and normalized objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance with normalized objective diversity\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_diversity = np.zeros(len(archive))\n    for i, (v1, v2) in enumerate(objectives):\n        normalized_diversity[i] = (v1 / max_v1 + v2 / max_v2) if max_v1 > 0 and max_v2 > 0 else 0\n\n    selection_scores = crowding_distances * normalized_diversity\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search - adaptive subset flips with objective-weighted improvements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic neighborhood size based on solution quality\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Strategy 1: Adaptive subset flip with objective-weighted selection\n    for _ in range(neighborhood_size):\n        # Select a random subset of items to consider\n        subset_size = min(3, max(1, int(n_items * 0.2)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            # Try flipping the item\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            # Calculate new weight\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            # Check feasibility\n            if new_weight > capacity:\n                continue\n\n            # Calculate objective improvements\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-weighted improvement\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Strategy 2: Objective-weighted item swaps if no improvement found\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    # Objective-weighted swap decision\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 44,
        "algorithm": "The algorithm selects a promising solution from the archive using Pareto-ranked crowding-distance metrics, then applies a hybrid local search combining adaptive subset flips and objective-weighted swaps to generate a neighbor solution while dynamically adjusting the neighborhood size based on solution quality and feasibility. It prioritizes high-crowding-distance solutions for exploration and uses objective-weighted improvements to guide flips, falling back to strategic swaps when no flips yield improvements. The neighborhood size adapts to the current solution's weight relative to capacity, balancing exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Pareto-ranked selection with crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine with Pareto rank (higher rank = more promising)\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1  # Non-dominated (frontier)\n\n    # Select by weighted crowding distance and Pareto rank\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search with adaptive operators\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic neighborhood size (larger for low-crowding solutions)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Strategy 1: Adaptive subset flips with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-weighted improvement (normalized by weight)\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Strategy 2: Objective-weighted swaps if no improvement\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    # Objective-weighted swap decision\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n",
        "score": [
            -0.769524318436092,
            0.6605959534645081
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Pareto-ranked selection with crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine with Pareto rank (higher rank = more promising)\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1  # Non-dominated (frontier)\n\n    # Select by weighted crowding distance and Pareto rank\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search with adaptive operators\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic neighborhood size (larger for low-crowding solutions)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Strategy 1: Adaptive subset flips with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-weighted improvement (normalized by weight)\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Strategy 2: Objective-weighted swaps if no improvement\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    # Objective-weighted swap decision\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines Pareto ranking with crowding-distance selection to prioritize diverse, high-quality solutions from the archive, then applies an adaptive cluster-based local search that dynamically adjusts flip cluster sizes based on weight utilization and objective weights, ensuring feasibility through precomputed item contributions and move validation. It balances exploration/exploitation by flipping items within clusters while favoring moves that improve both objectives proportionally, falling back to random valid flips if no improvement is found.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max(obj[0] for _, obj in pareto_front) - min(obj[0] for _, obj in pareto_front) + 1e-6) + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max(obj[1] for _, obj in pareto_front) - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                crowding_distances.append(dist)\n\n            # Select solution with highest crowding distance\n            max_dist_idx = np.argmax(crowding_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive cluster-based item flips\n    cluster_size = max(1, int(len(new_solution) * 0.1 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 45,
        "algorithm": "The algorithm combines adaptive Pareto-front clustering with weighted path relinking to generate neighbors, prioritizing solutions from the largest cluster and dynamically adjusting flip probabilities based on item contributions to both objectives while ensuring feasibility through precomputed weight validation and greedy fallback moves.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Cluster solutions by Pareto dominance\n        clusters = []\n        for sol, obj in archive:\n            assigned = False\n            for cluster in clusters:\n                if all(any(obj[i] >= c_obj[i] for i in range(2)) and not all(obj[i] >= c_obj[i] for i in range(2)) for _, c_obj in cluster):\n                    cluster.append((sol, obj))\n                    assigned = True\n                    break\n            if not assigned:\n                clusters.append([(sol, obj)])\n\n        # Select base solution from largest cluster\n        largest_cluster = max(clusters, key=len)\n        base_solution = random.choice(largest_cluster)[0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Path relinking with weighted contributions\n    target_solution = random.choice([sol for sol, _ in largest_cluster if not np.array_equal(sol, base_solution)])\n    diff_indices = np.where(base_solution != target_solution)[0]\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in diff_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy diversity-aware flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Prefer items with high contribution to underrepresented objective\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.90410744453192,
            1.4575309455394745
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Cluster solutions by Pareto dominance\n        clusters = []\n        for sol, obj in archive:\n            assigned = False\n            for cluster in clusters:\n                if all(any(obj[i] >= c_obj[i] for i in range(2)) and not all(obj[i] >= c_obj[i] for i in range(2)) for _, c_obj in cluster):\n                    cluster.append((sol, obj))\n                    assigned = True\n                    break\n            if not assigned:\n                clusters.append([(sol, obj)])\n\n        # Select base solution from largest cluster\n        largest_cluster = max(clusters, key=len)\n        base_solution = random.choice(largest_cluster)[0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Path relinking with weighted contributions\n    target_solution = random.choice([sol for sol, _ in largest_cluster if not np.array_equal(sol, base_solution)])\n    diff_indices = np.where(base_solution != target_solution)[0]\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in diff_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy diversity-aware flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Prefer items with high contribution to underrepresented objective\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing those with high objective diversity (via crowding distance) and applies a hybrid local search combining item swaps and subset replacements to generate a feasible neighbor solution while ensuring weight constraints are not violated. It first evaluates solutions based on their crowding distances to identify promising candidates, then performs targeted swaps and subset operations to explore the neighborhood while maintaining feasibility. The algorithm balances exploration and exploitation by focusing on high-diversity regions of the Pareto front while dynamically adjusting the search based on the current solution's characteristics.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high objective diversity and low crowding distance)\n    # Calculate crowding distances for solutions in the archive\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n    objectives_sorted = objectives[sorted_indices]\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance (most promising for improvement)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Randomly swap items if feasible\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Strategy 2: Replace a subset of items with new ones if feasible\n    subset_size = min(3, n_items // 4)\n    if subset_size > 0:\n        # Remove a subset of items\n        remove_indices = np.where(new_solution == 1)[0]\n        if len(remove_indices) > 0:\n            remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Add a subset of new items\n        add_indices = np.where(new_solution == 0)[0]\n        if len(add_indices) > 0:\n            add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines Pareto ranking with crowding-distance selection to prioritize diverse, high-quality solutions from the archive, then applies an adaptive cluster-based local search that dynamically adjusts flip cluster sizes based on weight utilization and objective weights, ensuring feasibility through precomputed item contributions and move validation. It balances exploration/exploitation by flipping items within clusters while favoring moves that improve both objectives proportionally, falling back to random valid flips if no improvement is found.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max(obj[0] for _, obj in pareto_front) - min(obj[0] for _, obj in pareto_front) + 1e-6) + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max(obj[1] for _, obj in pareto_front) - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                crowding_distances.append(dist)\n\n            # Select solution with highest crowding distance\n            max_dist_idx = np.argmax(crowding_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive cluster-based item flips\n    cluster_size = max(1, int(len(new_solution) * 0.1 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 46,
        "algorithm": "This algorithm combines Pareto dominance filtering with adaptive neighborhood diversification to generate high-quality neighbors. It selects solutions based on crowding distance from the Pareto front and uses dynamic cluster flips prioritized by weight-balanced item exchanges, adjusting flip sizes based on objective trade-offs and weight utilization, while falling back to greedy random flips if no improvement is found. The algorithm ensures feasibility by precomputing weight contributions and dynamically adjusting cluster sizes to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n        selected_idx = np.argmax(crowding_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Adaptive neighborhood diversification\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Dynamic cluster size based on weight utilization\n    cluster_size = max(1, int(len(new_solution) * 0.2 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Evaluate cluster flips\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy random flip if no improvement\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8215018372748498,
            0.588567852973938
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n        selected_idx = np.argmax(crowding_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Adaptive neighborhood diversification\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Dynamic cluster size based on weight utilization\n    cluster_size = max(1, int(len(new_solution) * 0.2 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Evaluate cluster flips\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy random flip if no improvement\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines Pareto ranking with crowding-distance selection to prioritize diverse, high-quality solutions from the archive, then applies an adaptive cluster-based local search that dynamically adjusts flip cluster sizes based on weight utilization and objective weights, ensuring feasibility through precomputed item contributions and move validation. It balances exploration/exploitation by flipping items within clusters while favoring moves that improve both objectives proportionally, falling back to random valid flips if no improvement is found.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max(obj[0] for _, obj in pareto_front) - min(obj[0] for _, obj in pareto_front) + 1e-6) + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max(obj[1] for _, obj in pareto_front) - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                crowding_distances.append(dist)\n\n            # Select solution with highest crowding distance\n            max_dist_idx = np.argmax(crowding_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive cluster-based item flips\n    cluster_size = max(1, int(len(new_solution) * 0.1 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 47,
        "algorithm": "The algorithm selects a solution from the archive using a hybrid of Pareto dominance and value-to-weight ratio analysis, then applies a dynamic cluster-based local search with adaptive flip probabilities that prioritize both objectives based on their alignment, ensuring feasibility through weighted random sampling and move validation. It dynamically adjusts cluster size and flip weights based on weight utilization and objective alignment, favoring moves that improve both objectives proportionally while maintaining solution feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: Pareto dominance + value-to-weight ratio\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate value-to-weight ratios\n            ratios = []\n            for sol, obj in pareto_front:\n                total_weight = np.sum(weight_lst[sol == 1])\n                ratio = (obj[0]/(total_weight + 1e-6) + obj[1]/(total_weight + 1e-6)) / 2\n                ratios.append(ratio)\n\n            # Select solution with highest ratio\n            max_ratio_idx = np.argmax(ratios)\n            base_solution = pareto_front[max_ratio_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic cluster size based on weight utilization and objective alignment\n    weight_util = current_weight / capacity\n    cluster_size = max(1, int(len(new_solution) * (0.2 - 0.1 * weight_util)))\n\n    # Objective alignment factor\n    if current_v1 > 0 and current_v2 > 0:\n        alignment = min(current_v1, current_v2) / max(current_v1, current_v2)\n    else:\n        alignment = 0.5\n\n    # Weighted random sampling of cluster indices\n    weights = np.ones(len(new_solution))\n    weights[new_solution == 1] *= (1 + alignment)  # Favor items already in solution\n    weights[new_solution == 0] *= (1 - weight_util)  # Favor adding items when underutilized\n\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False, p=weights/np.sum(weights))\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Adaptive improvement calculation\n        improvement = (delta_v1 * (1 - alignment) + delta_v2 * alignment) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Weighted random flip if no improvement found\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            weights = np.ones(len(valid_indices))\n            for i, idx in enumerate(valid_indices):\n                if new_solution[idx] == 1:\n                    weights[i] *= (1 + alignment)\n                else:\n                    weights[i] *= (1 - weight_util)\n            idx = np.random.choice(valid_indices, p=weights/np.sum(weights))\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8611247396581494,
            0.8239341378211975
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: Pareto dominance + value-to-weight ratio\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate value-to-weight ratios\n            ratios = []\n            for sol, obj in pareto_front:\n                total_weight = np.sum(weight_lst[sol == 1])\n                ratio = (obj[0]/(total_weight + 1e-6) + obj[1]/(total_weight + 1e-6)) / 2\n                ratios.append(ratio)\n\n            # Select solution with highest ratio\n            max_ratio_idx = np.argmax(ratios)\n            base_solution = pareto_front[max_ratio_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic cluster size based on weight utilization and objective alignment\n    weight_util = current_weight / capacity\n    cluster_size = max(1, int(len(new_solution) * (0.2 - 0.1 * weight_util)))\n\n    # Objective alignment factor\n    if current_v1 > 0 and current_v2 > 0:\n        alignment = min(current_v1, current_v2) / max(current_v1, current_v2)\n    else:\n        alignment = 0.5\n\n    # Weighted random sampling of cluster indices\n    weights = np.ones(len(new_solution))\n    weights[new_solution == 1] *= (1 + alignment)  # Favor items already in solution\n    weights[new_solution == 0] *= (1 - weight_util)  # Favor adding items when underutilized\n\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False, p=weights/np.sum(weights))\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Adaptive improvement calculation\n        improvement = (delta_v1 * (1 - alignment) + delta_v2 * alignment) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Weighted random flip if no improvement found\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            weights = np.ones(len(valid_indices))\n            for i, idx in enumerate(valid_indices):\n                if new_solution[idx] == 1:\n                    weights[i] *= (1 + alignment)\n                else:\n                    weights[i] *= (1 - weight_util)\n            idx = np.random.choice(valid_indices, p=weights/np.sum(weights))\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines Pareto ranking with crowding-distance selection to prioritize diverse, high-quality solutions from the archive, then applies an adaptive cluster-based local search that dynamically adjusts flip cluster sizes based on weight utilization and objective weights, ensuring feasibility through precomputed item contributions and move validation. It balances exploration/exploitation by flipping items within clusters while favoring moves that improve both objectives proportionally, falling back to random valid flips if no improvement is found.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max(obj[0] for _, obj in pareto_front) - min(obj[0] for _, obj in pareto_front) + 1e-6) + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max(obj[1] for _, obj in pareto_front) - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                crowding_distances.append(dist)\n\n            # Select solution with highest crowding distance\n            max_dist_idx = np.argmax(crowding_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive cluster-based item flips\n    cluster_size = max(1, int(len(new_solution) * 0.1 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 48,
        "algorithm": "The algorithm selects a promising solution from the archive by prioritizing those with high weight utilization and balanced objectives, then applies a weighted random walk neighborhood exploration with dynamic cluster size to generate a feasible neighbor solution, favoring moves that improve both objectives proportionally to their current weights. The method ensures feasibility by precomputing valid moves and falls back to high-weight item flips when no improvements are found.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solutions with high weight utilization and balanced objectives\n        scores = []\n        for sol, obj in archive:\n            total_weight = np.sum(weight_lst[sol == 1])\n            density = total_weight / capacity\n            balance = 1 - abs(obj[0] - obj[1]) / (obj[0] + obj[1] + 1e-6)\n            scores.append(density * balance)\n        selected_idx = np.argmax(scores)\n        base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic cluster size based on solution density\n    density = current_weight / capacity\n    cluster_size = max(1, int(len(new_solution) * (0.2 + 0.1 * (1 - density))))\n\n    # Weighted random walk for neighborhood exploration\n    weights = np.zeros(len(new_solution))\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            weights[i] = weight_lst[i] * (value1_lst[i] + value2_lst[i])\n        else:\n            weights[i] = (capacity - current_weight + weight_lst[i]) * (value1_lst[i] + value2_lst[i]) if (current_weight + weight_lst[i] <= capacity) else 0\n\n    if np.sum(weights) > 0:\n        weights = weights / np.sum(weights)\n        cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False, p=weights)\n    else:\n        cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement calculation\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n        weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a high-weight valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Prefer higher weight items\n            weights = np.array([weight_lst[i] for i in valid_indices])\n            weights = weights / np.sum(weights)\n            idx = np.random.choice(valid_indices, p=weights)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8493817113862324,
            2.3725355565547943
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solutions with high weight utilization and balanced objectives\n        scores = []\n        for sol, obj in archive:\n            total_weight = np.sum(weight_lst[sol == 1])\n            density = total_weight / capacity\n            balance = 1 - abs(obj[0] - obj[1]) / (obj[0] + obj[1] + 1e-6)\n            scores.append(density * balance)\n        selected_idx = np.argmax(scores)\n        base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic cluster size based on solution density\n    density = current_weight / capacity\n    cluster_size = max(1, int(len(new_solution) * (0.2 + 0.1 * (1 - density))))\n\n    # Weighted random walk for neighborhood exploration\n    weights = np.zeros(len(new_solution))\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            weights[i] = weight_lst[i] * (value1_lst[i] + value2_lst[i])\n        else:\n            weights[i] = (capacity - current_weight + weight_lst[i]) * (value1_lst[i] + value2_lst[i]) if (current_weight + weight_lst[i] <= capacity) else 0\n\n    if np.sum(weights) > 0:\n        weights = weights / np.sum(weights)\n        cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False, p=weights)\n    else:\n        cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement calculation\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n        weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a high-weight valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Prefer higher weight items\n            weights = np.array([weight_lst[i] for i in valid_indices])\n            weights = weights / np.sum(weights)\n            idx = np.random.choice(valid_indices, p=weights)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 3 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto ranking with crowding-distance selection to prioritize diverse, high-quality solutions from the archive, then applies an adaptive cluster-based local search that dynamically adjusts flip cluster sizes based on weight utilization and objective weights, ensuring feasibility through precomputed item contributions and move validation. It balances exploration/exploitation by flipping items within clusters while favoring moves that improve both objectives proportionally, falling back to random valid flips if no improvement is found.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max(obj[0] for _, obj in pareto_front) - min(obj[0] for _, obj in pareto_front) + 1e-6) + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max(obj[1] for _, obj in pareto_front) - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                crowding_distances.append(dist)\n\n            # Select solution with highest crowding distance\n            max_dist_idx = np.argmax(crowding_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive cluster-based item flips\n    cluster_size = max(1, int(len(new_solution) * 0.1 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 49,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and weighted crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min(obj[0] for _, obj in pareto_front) + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8389475151262152,
            0.49354496598243713
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and weighted crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min(obj[0] for _, obj in pareto_front) + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 4 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive selection - prioritize solutions with high crowding distance and normalized objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance with normalized objective diversity\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_diversity = np.zeros(len(archive))\n    for i, (v1, v2) in enumerate(objectives):\n        normalized_diversity[i] = (v1 / max_v1 + v2 / max_v2) if max_v1 > 0 and max_v2 > 0 else 0\n\n    selection_scores = crowding_distances * normalized_diversity\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search - adaptive subset flips with objective-weighted improvements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic neighborhood size based on solution quality\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Strategy 1: Adaptive subset flip with objective-weighted selection\n    for _ in range(neighborhood_size):\n        # Select a random subset of items to consider\n        subset_size = min(3, max(1, int(n_items * 0.2)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            # Try flipping the item\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            # Calculate new weight\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            # Check feasibility\n            if new_weight > capacity:\n                continue\n\n            # Calculate objective improvements\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-weighted improvement\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Strategy 2: Objective-weighted item swaps if no improvement found\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    # Objective-weighted swap decision\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using Pareto-ranked crowding-distance metrics, then applies a hybrid local search combining adaptive subset flips and objective-weighted swaps to generate a neighbor solution while dynamically adjusting the neighborhood size based on solution quality and feasibility. It prioritizes high-crowding-distance solutions for exploration and uses objective-weighted improvements to guide flips, falling back to strategic swaps when no flips yield improvements. The neighborhood size adapts to the current solution's weight relative to capacity, balancing exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Pareto-ranked selection with crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine with Pareto rank (higher rank = more promising)\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1  # Non-dominated (frontier)\n\n    # Select by weighted crowding distance and Pareto rank\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search with adaptive operators\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic neighborhood size (larger for low-crowding solutions)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Strategy 1: Adaptive subset flips with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-weighted improvement (normalized by weight)\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Strategy 2: Objective-weighted swaps if no improvement\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    # Objective-weighted swap decision\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 50,
        "algorithm": "The algorithm selects a promising solution from the archive using Pareto-ranked crowding-distance metrics, then applies a hybrid local search combining adaptive subset flips and objective-weighted swaps, dynamically adjusting neighborhood size based on solution quality and feasibility to generate high-quality, feasible neighbor solutions. It prioritizes solutions with high crowding distance (diversity) and Pareto rank (non-dominated status), then uses weighted improvements in both objectives to guide local moves while ensuring feasibility through careful weight checks. The neighborhood size adapts to the remaining capacity, balancing exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto-ranked selection with crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search with adaptive operators\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Adaptive subset flips with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Objective-weighted swaps if no improvement\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n",
        "score": [
            -0.9015902242482594,
            0.6143663823604584
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto-ranked selection with crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search with adaptive operators\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Adaptive subset flips with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Objective-weighted swaps if no improvement\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and weighted crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min(obj[0] for _, obj in pareto_front) + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 51,
        "algorithm": "The algorithm combines adaptive solution selection (via Pareto ranking and weighted crowding distance) with a dynamic local search strategy that probabilistically accepts flips based on multi-objective improvements, dynamically adjusting neighborhood size and flip probabilities to balance exploration/exploitation while ensuring feasibility through precomputed feasible moves. It prioritizes solutions on the Pareto front with high crowding distances and uses objective-weighted improvements to guide the search, with temperature-based selection to escape local optima. The key variables are dynamically weighted based on current solution quality, and neighborhood size adapts to the remaining capacity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and weighted crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min(obj[0] for _, obj in pareto_front) + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (10-40% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Multi-objective simulated annealing with dynamic flip probability\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Precompute feasible flips\n    feasible_indices = []\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        feasible_indices.append(idx)\n\n    if feasible_indices:\n        # Evaluate all feasible flips\n        improvements = []\n        for idx in feasible_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n            improvements.append(improvement)\n\n        # Select with probability based on improvement and current temperature\n        if max(improvements) > 0:\n            # Dynamic temperature (higher for more crowded solutions)\n            temperature = 1.0 - (max(improvements) / (max(improvements) + np.mean(improvements) + 1e-6))\n            probabilities = [np.exp(imp / temperature) for imp in improvements]\n            probabilities = [p / sum(probabilities) for p in probabilities]\n            selected_idx = np.random.choice(feasible_indices, p=probabilities)\n        else:\n            selected_idx = random.choice(feasible_indices)\n\n        new_solution[selected_idx] = 1 - new_solution[selected_idx]\n    else:\n        # If no feasible flips, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            new_solution[random.choice(valid_indices)] = 1 - new_solution[random.choice(valid_indices)]\n\n    return new_solution\n\n",
        "score": [
            -0.8750883974479665,
            0.5188322365283966
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and weighted crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min(obj[0] for _, obj in pareto_front) + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (10-40% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Multi-objective simulated annealing with dynamic flip probability\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Precompute feasible flips\n    feasible_indices = []\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        feasible_indices.append(idx)\n\n    if feasible_indices:\n        # Evaluate all feasible flips\n        improvements = []\n        for idx in feasible_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n            improvements.append(improvement)\n\n        # Select with probability based on improvement and current temperature\n        if max(improvements) > 0:\n            # Dynamic temperature (higher for more crowded solutions)\n            temperature = 1.0 - (max(improvements) / (max(improvements) + np.mean(improvements) + 1e-6))\n            probabilities = [np.exp(imp / temperature) for imp in improvements]\n            probabilities = [p / sum(probabilities) for p in probabilities]\n            selected_idx = np.random.choice(feasible_indices, p=probabilities)\n        else:\n            selected_idx = random.choice(feasible_indices)\n\n        new_solution[selected_idx] = 1 - new_solution[selected_idx]\n    else:\n        # If no feasible flips, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            new_solution[random.choice(valid_indices)] = 1 - new_solution[random.choice(valid_indices)]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a solution from the archive using a hybrid of Pareto dominance and value-to-weight ratio analysis, then applies a dynamic cluster-based local search with adaptive flip probabilities that prioritize both objectives based on their alignment, ensuring feasibility through weighted random sampling and move validation. It dynamically adjusts cluster size and flip weights based on weight utilization and objective alignment, favoring moves that improve both objectives proportionally while maintaining solution feasibility.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: Pareto dominance + value-to-weight ratio\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate value-to-weight ratios\n            ratios = []\n            for sol, obj in pareto_front:\n                total_weight = np.sum(weight_lst[sol == 1])\n                ratio = (obj[0]/(total_weight + 1e-6) + obj[1]/(total_weight + 1e-6)) / 2\n                ratios.append(ratio)\n\n            # Select solution with highest ratio\n            max_ratio_idx = np.argmax(ratios)\n            base_solution = pareto_front[max_ratio_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic cluster size based on weight utilization and objective alignment\n    weight_util = current_weight / capacity\n    cluster_size = max(1, int(len(new_solution) * (0.2 - 0.1 * weight_util)))\n\n    # Objective alignment factor\n    if current_v1 > 0 and current_v2 > 0:\n        alignment = min(current_v1, current_v2) / max(current_v1, current_v2)\n    else:\n        alignment = 0.5\n\n    # Weighted random sampling of cluster indices\n    weights = np.ones(len(new_solution))\n    weights[new_solution == 1] *= (1 + alignment)  # Favor items already in solution\n    weights[new_solution == 0] *= (1 - weight_util)  # Favor adding items when underutilized\n\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False, p=weights/np.sum(weights))\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Adaptive improvement calculation\n        improvement = (delta_v1 * (1 - alignment) + delta_v2 * alignment) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Weighted random flip if no improvement found\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            weights = np.ones(len(valid_indices))\n            for i, idx in enumerate(valid_indices):\n                if new_solution[idx] == 1:\n                    weights[i] *= (1 + alignment)\n                else:\n                    weights[i] *= (1 - weight_util)\n            idx = np.random.choice(valid_indices, p=weights/np.sum(weights))\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and weighted crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min(obj[0] for _, obj in pareto_front) + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 52,
        "algorithm": "This algorithm combines adaptive Pareto selection with a novel \"objective-aligned cluster flip\" operator, where solutions are first filtered for Pareto efficiency, then selected based on crowding distance and objective balance, followed by dynamic cluster flips prioritizing items that proportionally improve both objectives, with neighborhood size and flip probability adapting to weight utilization. The method ensures feasibility through precomputed contributions and feasibility checks, with a fallback to random flips when no improvements are found.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Filter Pareto-efficient solutions\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distance and objective alignment\n            weighted_scores = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min(obj[0] for _, obj in pareto_front) + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                alignment = min(obj[0], obj[1]) / (max(obj[0], obj[1]) + 1e-6) if (obj[0] + obj[1]) > 0 else 0.5\n                weighted_score = 0.6 * (dist_v1 + dist_v2) + 0.4 * alignment\n                weighted_scores.append(weighted_score)\n\n            # Select solution with highest weighted score\n            max_score_idx = np.argmax(weighted_scores)\n            base_solution = pareto_front[max_score_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic cluster size and flip probability\n    weight_util = current_weight / capacity\n    cluster_size = max(1, int(len(new_solution) * (0.3 - 0.15 * weight_util)))\n    flip_prob = 0.5 + 0.4 * (1 - weight_util)\n\n    # Objective-aligned cluster flip\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        if np.random.rand() < flip_prob:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-aligned improvement calculation\n            alignment = min(current_v1 + delta_v1, current_v2 + delta_v2) / (max(current_v1 + delta_v1, current_v2 + delta_v2) + 1e-6) if (current_v1 + delta_v1 + current_v2 + delta_v2) > 0 else 0.5\n            improvement = (delta_v1 + delta_v2) * alignment\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Fallback: random flip with feasibility check\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = np.random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9040799819417309,
            0.6377159357070923
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Filter Pareto-efficient solutions\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distance and objective alignment\n            weighted_scores = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min(obj[0] for _, obj in pareto_front) + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                alignment = min(obj[0], obj[1]) / (max(obj[0], obj[1]) + 1e-6) if (obj[0] + obj[1]) > 0 else 0.5\n                weighted_score = 0.6 * (dist_v1 + dist_v2) + 0.4 * alignment\n                weighted_scores.append(weighted_score)\n\n            # Select solution with highest weighted score\n            max_score_idx = np.argmax(weighted_scores)\n            base_solution = pareto_front[max_score_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic cluster size and flip probability\n    weight_util = current_weight / capacity\n    cluster_size = max(1, int(len(new_solution) * (0.3 - 0.15 * weight_util)))\n    flip_prob = 0.5 + 0.4 * (1 - weight_util)\n\n    # Objective-aligned cluster flip\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        if np.random.rand() < flip_prob:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-aligned improvement calculation\n            alignment = min(current_v1 + delta_v1, current_v2 + delta_v2) / (max(current_v1 + delta_v1, current_v2 + delta_v2) + 1e-6) if (current_v1 + delta_v1 + current_v2 + delta_v2) > 0 else 0.5\n            improvement = (delta_v1 + delta_v2) * alignment\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Fallback: random flip with feasibility check\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = np.random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        None\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and weighted crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min(obj[0] for _, obj in pareto_front) + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 53,
        "algorithm": "This heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8688276949567951,
            0.37794408202171326
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        None\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and weighted crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min(obj[0] for _, obj in pareto_front) + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 54,
        "algorithm": "This algorithm selects a promising solution from the archive using objective space partitioning (quadrants) to focus on high-potential regions, then applies a hybrid local search with adaptive objective weighting and probabilistic flip operations to generate neighbors while ensuring feasibility. It prioritizes solutions in the most populated quadrant, dynamically adjusts neighborhood size and objective weights based on solution density and position, and uses both greedy improvement and probabilistic selection to explore the search space effectively.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Objective space partitioning selection\n        objectives = np.array([obj for _, obj in archive])\n        v1_min, v1_max = np.min(objectives[:, 0]), np.max(objectives[:, 0])\n        v2_min, v2_max = np.min(objectives[:, 1]), np.max(objectives[:, 1])\n\n        # Divide objective space into 4 quadrants\n        q1 = objectives[(objectives[:, 0] >= (v1_min + v1_max)/2) & (objectives[:, 1] >= (v2_min + v2_max)/2)]\n        q2 = objectives[(objectives[:, 0] < (v1_min + v1_max)/2) & (objectives[:, 1] >= (v2_min + v2_max)/2)]\n        q3 = objectives[(objectives[:, 0] < (v1_min + v1_max)/2) & (objectives[:, 1] < (v2_min + v2_max)/2)]\n        q4 = objectives[(objectives[:, 0] >= (v1_min + v1_max)/2) & (objectives[:, 1] < (v2_min + v2_max)/2)]\n\n        # Select quadrant with most solutions (if tie, random)\n        quadrant_counts = [len(q1), len(q2), len(q3), len(q4)]\n        max_count = max(quadrant_counts)\n        selected_quadrants = [i for i, cnt in enumerate(quadrant_counts) if cnt == max_count]\n        selected_q = random.choice(selected_quadrants)\n\n        if selected_q == 0:\n            candidates = [sol for sol, obj in archive if obj[0] >= (v1_min + v1_max)/2 and obj[1] >= (v2_min + v2_max)/2]\n        elif selected_q == 1:\n            candidates = [sol for sol, obj in archive if obj[0] < (v1_min + v1_max)/2 and obj[1] >= (v2_min + v2_max)/2]\n        elif selected_q == 2:\n            candidates = [sol for sol, obj in archive if obj[0] < (v1_min + v1_max)/2 and obj[1] < (v2_min + v2_max)/2]\n        else:\n            candidates = [sol for sol, obj in archive if obj[0] >= (v1_min + v1_max)/2 and obj[1] < (v2_min + v2_max)/2]\n\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive neighborhood size based on solution density\n    total_items = len(new_solution)\n    density = current_weight / capacity\n    neighborhood_size = max(1, int(total_items * (0.3 + 0.2 * density)))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Dynamic objective weighting based on solution's position in objective space\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    v1_range = np.max(value1_lst) - np.min(value1_lst)\n    v2_range = np.max(value2_lst) - np.min(value2_lst)\n    weight_v1 = (total_v1 - np.min(value1_lst)) / (v1_range + 1e-6) if v1_range > 0 else 0.5\n    weight_v2 = (total_v2 - np.min(value2_lst)) / (v2_range + 1e-6) if v2_range > 0 else 0.5\n    weight_v1 = weight_v1 / (weight_v1 + weight_v2 + 1e-6)\n    weight_v2 = 1 - weight_v1\n\n    # Hybrid local search with adaptive objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on normalized potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * (potential_v1 / (v1_range + 1e-6)) + weight_v2 * (potential_v2 / (v2_range + 1e-6))\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8781607167301745,
            1.2968321442604065
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Objective space partitioning selection\n        objectives = np.array([obj for _, obj in archive])\n        v1_min, v1_max = np.min(objectives[:, 0]), np.max(objectives[:, 0])\n        v2_min, v2_max = np.min(objectives[:, 1]), np.max(objectives[:, 1])\n\n        # Divide objective space into 4 quadrants\n        q1 = objectives[(objectives[:, 0] >= (v1_min + v1_max)/2) & (objectives[:, 1] >= (v2_min + v2_max)/2)]\n        q2 = objectives[(objectives[:, 0] < (v1_min + v1_max)/2) & (objectives[:, 1] >= (v2_min + v2_max)/2)]\n        q3 = objectives[(objectives[:, 0] < (v1_min + v1_max)/2) & (objectives[:, 1] < (v2_min + v2_max)/2)]\n        q4 = objectives[(objectives[:, 0] >= (v1_min + v1_max)/2) & (objectives[:, 1] < (v2_min + v2_max)/2)]\n\n        # Select quadrant with most solutions (if tie, random)\n        quadrant_counts = [len(q1), len(q2), len(q3), len(q4)]\n        max_count = max(quadrant_counts)\n        selected_quadrants = [i for i, cnt in enumerate(quadrant_counts) if cnt == max_count]\n        selected_q = random.choice(selected_quadrants)\n\n        if selected_q == 0:\n            candidates = [sol for sol, obj in archive if obj[0] >= (v1_min + v1_max)/2 and obj[1] >= (v2_min + v2_max)/2]\n        elif selected_q == 1:\n            candidates = [sol for sol, obj in archive if obj[0] < (v1_min + v1_max)/2 and obj[1] >= (v2_min + v2_max)/2]\n        elif selected_q == 2:\n            candidates = [sol for sol, obj in archive if obj[0] < (v1_min + v1_max)/2 and obj[1] < (v2_min + v2_max)/2]\n        else:\n            candidates = [sol for sol, obj in archive if obj[0] >= (v1_min + v1_max)/2 and obj[1] < (v2_min + v2_max)/2]\n\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive neighborhood size based on solution density\n    total_items = len(new_solution)\n    density = current_weight / capacity\n    neighborhood_size = max(1, int(total_items * (0.3 + 0.2 * density)))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Dynamic objective weighting based on solution's position in objective space\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    v1_range = np.max(value1_lst) - np.min(value1_lst)\n    v2_range = np.max(value2_lst) - np.min(value2_lst)\n    weight_v1 = (total_v1 - np.min(value1_lst)) / (v1_range + 1e-6) if v1_range > 0 else 0.5\n    weight_v2 = (total_v2 - np.min(value2_lst)) / (v2_range + 1e-6) if v2_range > 0 else 0.5\n    weight_v1 = weight_v1 / (weight_v1 + weight_v2 + 1e-6)\n    weight_v2 = 1 - weight_v1\n\n    # Hybrid local search with adaptive objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on normalized potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * (potential_v1 / (v1_range + 1e-6)) + weight_v2 * (potential_v2 / (v2_range + 1e-6))\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 4 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and weighted crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min(obj[0] for _, obj in pareto_front) + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto ranking with crowding-distance selection to prioritize diverse, high-quality solutions from the archive, then applies an adaptive cluster-based local search that dynamically adjusts flip cluster sizes based on weight utilization and objective weights, ensuring feasibility through precomputed item contributions and move validation. It balances exploration/exploitation by flipping items within clusters while favoring moves that improve both objectives proportionally, falling back to random valid flips if no improvement is found.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max(obj[0] for _, obj in pareto_front) - min(obj[0] for _, obj in pareto_front) + 1e-6) + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max(obj[1] for _, obj in pareto_front) - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                crowding_distances.append(dist)\n\n            # Select solution with highest crowding distance\n            max_dist_idx = np.argmax(crowding_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive cluster-based item flips\n    cluster_size = max(1, int(len(new_solution) * 0.1 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 55,
        "algorithm": "The algorithm first identifies the Pareto front from the archive and selects a base solution using a weighted crowding-distance metric to prioritize high-diversity solutions. It then applies a hybrid local search that dynamically adjusts the neighborhood size based on the solution's weight utilization and objective weights, evaluating flips within the neighborhood while ensuring feasibility. If no improving move is found, it performs a probabilistic flip based on potential weighted contributions, dynamically adjusting objective weights to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Identify Pareto front\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            min_v1 = min(obj[0] for _, obj in pareto_front)\n            min_v2 = min(obj[1] for _, obj in pareto_front)\n\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min_v1 + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min_v2 + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8923749103075588,
            0.4989009499549866
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Identify Pareto front\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            min_v1 = min(obj[0] for _, obj in pareto_front)\n            min_v2 = min(obj[1] for _, obj in pareto_front)\n\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min_v1 + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min_v2 + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 5 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive selection - prioritize solutions with high crowding distance and normalized objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance with normalized objective diversity\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_diversity = np.zeros(len(archive))\n    for i, (v1, v2) in enumerate(objectives):\n        normalized_diversity[i] = (v1 / max_v1 + v2 / max_v2) if max_v1 > 0 and max_v2 > 0 else 0\n\n    selection_scores = crowding_distances * normalized_diversity\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search - adaptive subset flips with objective-weighted improvements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic neighborhood size based on solution quality\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Strategy 1: Adaptive subset flip with objective-weighted selection\n    for _ in range(neighborhood_size):\n        # Select a random subset of items to consider\n        subset_size = min(3, max(1, int(n_items * 0.2)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            # Try flipping the item\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            # Calculate new weight\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            # Check feasibility\n            if new_weight > capacity:\n                continue\n\n            # Calculate objective improvements\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-weighted improvement\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Strategy 2: Objective-weighted item swaps if no improvement found\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    # Objective-weighted swap decision\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and weighted crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min(obj[0] for _, obj in pareto_front) + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using Pareto-ranked crowding-distance metrics, then applies a hybrid local search combining adaptive subset flips and objective-weighted swaps, dynamically adjusting neighborhood size based on solution quality and feasibility to generate high-quality, feasible neighbor solutions. It prioritizes solutions with high crowding distance (diversity) and Pareto rank (non-dominated status), then uses weighted improvements in both objectives to guide local moves while ensuring feasibility through careful weight checks. The neighborhood size adapts to the remaining capacity, balancing exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto-ranked selection with crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search with adaptive operators\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Adaptive subset flips with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Objective-weighted swaps if no improvement\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design for the BI-KP, optimized for high-quality, feasible neighbors:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 56,
        "algorithm": "The algorithm selects a solution from the archive using a weighted crowding-distance metric that prioritizes high-diversity regions and Pareto-optimal solutions, then applies a hybrid local search combining adaptive subset flips and objective-weighted swaps, dynamically adjusting neighborhood size and objective weights to generate high-quality, feasible neighbors while balancing improvement across both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution using weighted crowding-distance metric\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Weight crowding distances by Pareto dominance\n    pareto_rank = np.zeros(len(archive))\n    for i, obj in enumerate(objectives):\n        dominated = False\n        for other_obj in objectives:\n            if np.all(other_obj >= obj) and np.any(other_obj > obj):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search with adaptive operators\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive subset flips with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = -float('inf')\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Objective-weighted swaps if no improvement\n    if best_improvement == -float('inf'):\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (weight_v1 * delta_v1 + weight_v2 * delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n",
        "score": [
            -0.7685162708243267,
            1.8663938641548157
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a solution using weighted crowding-distance metric\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Weight crowding distances by Pareto dominance\n    pareto_rank = np.zeros(len(archive))\n    for i, obj in enumerate(objectives):\n        dominated = False\n        for other_obj in objectives:\n            if np.all(other_obj >= obj) and np.any(other_obj > obj):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search with adaptive operators\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive subset flips with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = -float('inf')\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Objective-weighted swaps if no improvement\n    if best_improvement == -float('inf'):\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (weight_v1 * delta_v1 + weight_v2 * delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive using Pareto-ranked crowding-distance metrics, then applies a hybrid local search combining adaptive subset flips and objective-weighted swaps, dynamically adjusting neighborhood size based on solution quality and feasibility to generate high-quality, feasible neighbor solutions. It prioritizes solutions with high crowding distance (diversity) and Pareto rank (non-dominated status), then uses weighted improvements in both objectives to guide local moves while ensuring feasibility through careful weight checks. The neighborhood size adapts to the remaining capacity, balancing exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto-ranked selection with crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search with adaptive operators\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Adaptive subset flips with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Objective-weighted swaps if no improvement\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design for the BI-KP, optimized for high-quality, feasible neighbors:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 57,
        "algorithm": "The algorithm selects a solution from the archive using hypervolume-based prioritization to focus on non-dominated regions, then applies a multi-phase local search: first adaptively flipping subsets of items based on value-to-weight ratios and remaining capacity, followed by targeted swaps that prioritize objective improvements while ensuring feasibility. It dynamically adjusts flip size and swap intensity to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hypervolume-based selection\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    hypervolumes = np.zeros(len(archive))\n    for i, obj in enumerate(objectives):\n        hypervolumes[i] = (max_obj[0] - obj[0]) * (max_obj[1] - obj[1])\n\n    selected_idx = np.argmax(hypervolumes)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-phase local search\n    # Phase 1: Adaptive multi-item flips\n    flip_size = max(1, min(5, int(0.2 * len(weight_lst) * (1 - current_weight / capacity))))\n    for _ in range(3):\n        flip_indices = np.random.choice(len(weight_lst), size=flip_size, replace=False)\n        temp_solution = new_solution.copy()\n        for idx in flip_indices:\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        for idx in flip_indices:\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n        if new_weight <= capacity:\n            new_solution = temp_solution\n            current_weight = new_weight\n\n    # Phase 2: Targeted objective-driven swaps\n    for _ in range(5):\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            # Select item to remove based on objective impact\n            out_idx = np.argmin([value1_lst[i] + value2_lst[i] for i in out_items])\n            out_idx = out_items[out_idx]\n\n            # Select item to add based on objective improvement\n            improvements = []\n            for in_idx in in_items:\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    improvements.append(delta_v1 + delta_v2)\n                else:\n                    improvements.append(-np.inf)\n\n            if any(imp > 0 for imp in improvements):\n                best_in_idx = np.argmax(improvements)\n                best_in_idx = in_items[best_in_idx]\n                new_solution[out_idx] = 0\n                new_solution[best_in_idx] = 1\n                current_weight = current_weight - weight_lst[out_idx] + weight_lst[best_in_idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8892713386550599,
            3.1528317630290985
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hypervolume-based selection\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    hypervolumes = np.zeros(len(archive))\n    for i, obj in enumerate(objectives):\n        hypervolumes[i] = (max_obj[0] - obj[0]) * (max_obj[1] - obj[1])\n\n    selected_idx = np.argmax(hypervolumes)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Multi-phase local search\n    # Phase 1: Adaptive multi-item flips\n    flip_size = max(1, min(5, int(0.2 * len(weight_lst) * (1 - current_weight / capacity))))\n    for _ in range(3):\n        flip_indices = np.random.choice(len(weight_lst), size=flip_size, replace=False)\n        temp_solution = new_solution.copy()\n        for idx in flip_indices:\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        for idx in flip_indices:\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n        if new_weight <= capacity:\n            new_solution = temp_solution\n            current_weight = new_weight\n\n    # Phase 2: Targeted objective-driven swaps\n    for _ in range(5):\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            # Select item to remove based on objective impact\n            out_idx = np.argmin([value1_lst[i] + value2_lst[i] for i in out_items])\n            out_idx = out_items[out_idx]\n\n            # Select item to add based on objective improvement\n            improvements = []\n            for in_idx in in_items:\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    improvements.append(delta_v1 + delta_v2)\n                else:\n                    improvements.append(-np.inf)\n\n            if any(imp > 0 for imp in improvements):\n                best_in_idx = np.argmax(improvements)\n                best_in_idx = in_items[best_in_idx]\n                new_solution[out_idx] = 0\n                new_solution[best_in_idx] = 1\n                current_weight = current_weight - weight_lst[out_idx] + weight_lst[best_in_idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive using Pareto-ranked crowding-distance metrics, then applies a hybrid local search combining adaptive subset flips and objective-weighted swaps, dynamically adjusting neighborhood size based on solution quality and feasibility to generate high-quality, feasible neighbor solutions. It prioritizes solutions with high crowding distance (diversity) and Pareto rank (non-dominated status), then uses weighted improvements in both objectives to guide local moves while ensuring feasibility through careful weight checks. The neighborhood size adapts to the remaining capacity, balancing exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto-ranked selection with crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search with adaptive operators\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Adaptive subset flips with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Objective-weighted swaps if no improvement\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design for the BI-KP, optimized for high-quality, feasible neighbors:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 58,
        "algorithm": "The algorithm combines adaptive Pareto-aware selection (prioritizing solutions with high crowding distance and Pareto rank) with a hybrid local search that alternates between probabilistic subset flips (weighted by marginal gains in both objectives) and feasibility-preserving swaps, dynamically adjusting neighborhood size based on remaining capacity to ensure high-quality, diverse, and feasible neighbors. It emphasizes marginal improvement over both objectives while maintaining feasibility through careful weight tracking and constrained random swaps. The selection process prioritizes solutions that are both non-dominated and well-distributed in the objective space, while the local search intelligently explores the neighborhood to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive Pareto-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Value-weighted diversification\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Feasibility-preserving swaps\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n",
        "score": [
            -0.8973063653077944,
            0.6381535828113556
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive Pareto-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Value-weighted diversification\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Feasibility-preserving swaps\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive using Pareto-ranked crowding-distance metrics, then applies a hybrid local search combining adaptive subset flips and objective-weighted swaps, dynamically adjusting neighborhood size based on solution quality and feasibility to generate high-quality, feasible neighbor solutions. It prioritizes solutions with high crowding distance (diversity) and Pareto rank (non-dominated status), then uses weighted improvements in both objectives to guide local moves while ensuring feasibility through careful weight checks. The neighborhood size adapts to the remaining capacity, balancing exploration and exploitation.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto-ranked selection with crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search with adaptive operators\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Adaptive subset flips with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Objective-weighted swaps if no improvement\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 59,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid metric combining Pareto dominance and hypervolume contribution, then applies an objective-biased mutation operator that probabilistically flips items based on their marginal contributions to both objectives, dynamically adjusting mutation rate and neighborhood size according to solution quality and remaining capacity. The selection prioritizes non-dominated solutions with high hypervolume contribution, while the mutation emphasizes items with significant marginal value-to-weight ratios, ensuring feasibility by always checking capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hybrid selection: Pareto dominance + hypervolume contribution\n    objectives = np.array([obj for _, obj in archive])\n    dominance_counts = np.zeros(len(archive))\n    hypervolume_contributions = np.zeros(len(archive))\n\n    # Calculate dominance counts and hypervolume contributions\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j:\n                if (objectives[i, 0] >= objectives[j, 0] and objectives[i, 1] >= objectives[j, 1] and\n                    (objectives[i, 0] > objectives[j, 0] or objectives[i, 1] > objectives[j, 1])):\n                    dominance_counts[i] += 1\n\n    # Simple hypervolume contribution approximation (area improvement)\n    max_obj = np.max(objectives, axis=0)\n    for i in range(len(archive)):\n        if i == 0 or i == len(archive)-1:\n            hypervolume_contributions[i] = 1.0\n        else:\n            hypervolume_contributions[i] = (objectives[i, 0] - objectives[i-1, 0]) * (objectives[i+1, 1] - objectives[i, 1])\n\n    # Combine metrics (inverse dominance + hypervolume)\n    selection_scores = (1.0 / (dominance_counts + 1)) * hypervolume_contributions\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Objective-biased mutation operator\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Calculate remaining capacity and solution quality\n    remaining_capacity = capacity - current_weight\n    solution_quality = (np.sum(value1_lst[base_solution == 1]) + np.sum(value2_lst[base_solution == 1])) / (capacity + 1e-6)\n\n    # Dynamic mutation rate and neighborhood size\n    mutation_rate = min(0.5, max(0.1, 0.3 * (1 - solution_quality)))\n    neighborhood_size = min(5, max(1, int(n_items * 0.2 * (remaining_capacity / capacity))))\n\n    for _ in range(neighborhood_size):\n        # Calculate marginal contributions for each item\n        marginal_contributions = np.zeros(n_items)\n        for i in range(n_items):\n            if base_solution[i] == 0 and (current_weight + weight_lst[i]) <= capacity:\n                marginal_contributions[i] = (value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-6)\n            elif base_solution[i] == 1:\n                marginal_contributions[i] = -(value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-6)\n\n        # Normalize and apply mutation probabilities\n        if np.sum(np.abs(marginal_contributions)) > 0:\n            mutation_probs = np.abs(marginal_contributions) / np.sum(np.abs(marginal_contributions))\n        else:\n            mutation_probs = np.ones(n_items) / n_items\n\n        # Select items to flip\n        flip_indices = np.random.choice(n_items, size=min(n_items, max(1, int(n_items * mutation_rate))),\n                                      p=mutation_probs, replace=False)\n\n        # Apply flips with feasibility check\n        for idx in flip_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight <= capacity:\n                new_solution = temp_solution\n                current_weight = new_weight\n\n    return new_solution\n\n",
        "score": [
            -0.8223878586425644,
            1.2856266498565674
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hybrid selection: Pareto dominance + hypervolume contribution\n    objectives = np.array([obj for _, obj in archive])\n    dominance_counts = np.zeros(len(archive))\n    hypervolume_contributions = np.zeros(len(archive))\n\n    # Calculate dominance counts and hypervolume contributions\n    for i in range(len(archive)):\n        for j in range(len(archive)):\n            if i != j:\n                if (objectives[i, 0] >= objectives[j, 0] and objectives[i, 1] >= objectives[j, 1] and\n                    (objectives[i, 0] > objectives[j, 0] or objectives[i, 1] > objectives[j, 1])):\n                    dominance_counts[i] += 1\n\n    # Simple hypervolume contribution approximation (area improvement)\n    max_obj = np.max(objectives, axis=0)\n    for i in range(len(archive)):\n        if i == 0 or i == len(archive)-1:\n            hypervolume_contributions[i] = 1.0\n        else:\n            hypervolume_contributions[i] = (objectives[i, 0] - objectives[i-1, 0]) * (objectives[i+1, 1] - objectives[i, 1])\n\n    # Combine metrics (inverse dominance + hypervolume)\n    selection_scores = (1.0 / (dominance_counts + 1)) * hypervolume_contributions\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Objective-biased mutation operator\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Calculate remaining capacity and solution quality\n    remaining_capacity = capacity - current_weight\n    solution_quality = (np.sum(value1_lst[base_solution == 1]) + np.sum(value2_lst[base_solution == 1])) / (capacity + 1e-6)\n\n    # Dynamic mutation rate and neighborhood size\n    mutation_rate = min(0.5, max(0.1, 0.3 * (1 - solution_quality)))\n    neighborhood_size = min(5, max(1, int(n_items * 0.2 * (remaining_capacity / capacity))))\n\n    for _ in range(neighborhood_size):\n        # Calculate marginal contributions for each item\n        marginal_contributions = np.zeros(n_items)\n        for i in range(n_items):\n            if base_solution[i] == 0 and (current_weight + weight_lst[i]) <= capacity:\n                marginal_contributions[i] = (value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-6)\n            elif base_solution[i] == 1:\n                marginal_contributions[i] = -(value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-6)\n\n        # Normalize and apply mutation probabilities\n        if np.sum(np.abs(marginal_contributions)) > 0:\n            mutation_probs = np.abs(marginal_contributions) / np.sum(np.abs(marginal_contributions))\n        else:\n            mutation_probs = np.ones(n_items) / n_items\n\n        # Select items to flip\n        flip_indices = np.random.choice(n_items, size=min(n_items, max(1, int(n_items * mutation_rate))),\n                                      p=mutation_probs, replace=False)\n\n        # Apply flips with feasibility check\n        for idx in flip_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight <= capacity:\n                new_solution = temp_solution\n                current_weight = new_weight\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a promising solution from the archive using Pareto-ranked crowding-distance metrics, then applies a hybrid local search combining adaptive subset flips and objective-weighted swaps, dynamically adjusting neighborhood size based on solution quality and feasibility to generate high-quality, feasible neighbor solutions. It prioritizes solutions with high crowding distance (diversity) and Pareto rank (non-dominated status), then uses weighted improvements in both objectives to guide local moves while ensuring feasibility through careful weight checks. The neighborhood size adapts to the remaining capacity, balancing exploration and exploitation.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto-ranked selection with crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search with adaptive operators\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Adaptive subset flips with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Objective-weighted swaps if no improvement\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 60,
        "algorithm": "The algorithm selects a promising solution from the archive using hypervolume contribution and diversity metrics, then applies a hybrid local search with dynamic neighborhood size adjustment, prioritizing objective-weighted swap and flip operations to balance improvements across both objectives while ensuring feasibility. It dynamically adjusts the neighborhood size based on solution quality and capacity, and uses a novel improvement metric to guide flip operations, favoring those that maximize the product of value improvements relative to weight changes.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hypervolume-based selection with diversity\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    hypervolumes = np.zeros(len(archive))\n    for i, obj in enumerate(objectives):\n        hypervolumes[i] = (max_obj[0] - obj[0]) * (max_obj[1] - obj[1])\n\n    diversity_scores = np.zeros(len(archive))\n    for i in range(len(archive)):\n        distances = np.sum(np.abs(objectives - objectives[i]), axis=1)\n        diversity_scores[i] = np.mean(np.sort(distances)[:5])\n\n    selection_scores = hypervolumes * diversity_scores\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search with dynamic neighborhood\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(10, max(2, int(n_items * 0.2 * (capacity - current_weight) / capacity)))\n\n    # Dynamic objective-weighted swaps and flips\n    for _ in range(neighborhood_size):\n        operation_type = np.random.choice(['swap', 'flip'], p=[0.6, 0.4])\n\n        if operation_type == 'swap':\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n\n        else:  # flip operation\n            flip_indices = np.random.choice(n_items, size=min(3, n_items), replace=False)\n            best_improvement = 0\n            best_candidate = None\n\n            for idx in flip_indices:\n                temp_solution = new_solution.copy()\n                temp_solution[idx] = 1 - temp_solution[idx]\n\n                new_weight = current_weight\n                if temp_solution[idx] == 1:\n                    new_weight += weight_lst[idx]\n                else:\n                    new_weight -= weight_lst[idx]\n\n                if new_weight > capacity:\n                    continue\n\n                delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n                delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n                improvement = (delta_v1 * delta_v2) / (weight_lst[idx] + 1e-6)\n\n                if improvement > best_improvement:\n                    best_improvement = improvement\n                    best_candidate = temp_solution\n\n            if best_candidate is not None:\n                new_solution = best_candidate\n                current_weight = np.sum(weight_lst[new_solution == 1])\n\n    return new_solution\n\n",
        "score": [
            -0.8922021088093526,
            1.2740332186222076
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Hypervolume-based selection with diversity\n    objectives = np.array([obj for _, obj in archive])\n    max_obj = np.max(objectives, axis=0)\n    hypervolumes = np.zeros(len(archive))\n    for i, obj in enumerate(objectives):\n        hypervolumes[i] = (max_obj[0] - obj[0]) * (max_obj[1] - obj[1])\n\n    diversity_scores = np.zeros(len(archive))\n    for i in range(len(archive)):\n        distances = np.sum(np.abs(objectives - objectives[i]), axis=1)\n        diversity_scores[i] = np.mean(np.sort(distances)[:5])\n\n    selection_scores = hypervolumes * diversity_scores\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search with dynamic neighborhood\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(10, max(2, int(n_items * 0.2 * (capacity - current_weight) / capacity)))\n\n    # Dynamic objective-weighted swaps and flips\n    for _ in range(neighborhood_size):\n        operation_type = np.random.choice(['swap', 'flip'], p=[0.6, 0.4])\n\n        if operation_type == 'swap':\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n\n        else:  # flip operation\n            flip_indices = np.random.choice(n_items, size=min(3, n_items), replace=False)\n            best_improvement = 0\n            best_candidate = None\n\n            for idx in flip_indices:\n                temp_solution = new_solution.copy()\n                temp_solution[idx] = 1 - temp_solution[idx]\n\n                new_weight = current_weight\n                if temp_solution[idx] == 1:\n                    new_weight += weight_lst[idx]\n                else:\n                    new_weight -= weight_lst[idx]\n\n                if new_weight > capacity:\n                    continue\n\n                delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n                delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n                improvement = (delta_v1 * delta_v2) / (weight_lst[idx] + 1e-6)\n\n                if improvement > best_improvement:\n                    best_improvement = improvement\n                    best_candidate = temp_solution\n\n            if best_candidate is not None:\n                new_solution = best_candidate\n                current_weight = np.sum(weight_lst[new_solution == 1])\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 4 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using Pareto-ranked crowding-distance metrics, then applies a hybrid local search combining adaptive subset flips and objective-weighted swaps, dynamically adjusting neighborhood size based on solution quality and feasibility to generate high-quality, feasible neighbor solutions. It prioritizes solutions with high crowding distance (diversity) and Pareto rank (non-dominated status), then uses weighted improvements in both objectives to guide local moves while ensuring feasibility through careful weight checks. The neighborhood size adapts to the remaining capacity, balancing exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto-ranked selection with crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search with adaptive operators\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Adaptive subset flips with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Objective-weighted swaps if no improvement\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto ranking with crowding-distance selection to prioritize diverse, high-quality solutions from the archive, then applies an adaptive cluster-based local search that dynamically adjusts flip cluster sizes based on weight utilization and objective weights, ensuring feasibility through precomputed item contributions and move validation. It balances exploration/exploitation by flipping items within clusters while favoring moves that improve both objectives proportionally, falling back to random valid flips if no improvement is found.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max(obj[0] for _, obj in pareto_front) - min(obj[0] for _, obj in pareto_front) + 1e-6) + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max(obj[1] for _, obj in pareto_front) - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                crowding_distances.append(dist)\n\n            # Select solution with highest crowding distance\n            max_dist_idx = np.argmax(crowding_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive cluster-based item flips\n    cluster_size = max(1, int(len(new_solution) * 0.1 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design for the BI-KP, optimized for high-quality, feasible neighbors:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 61,
        "algorithm": "The algorithm first selects a promising solution from the archive using a weighted combination of Pareto rank and crowding distance, then applies a hybrid local search that dynamically adjusts neighborhood sizes based on weight utilization and objective weights, prioritizing flipping subsets of items with high potential improvements in both objectives while ensuring feasibility through precomputed item contributions. If no improving move is found, it performs probabilistic flips based on potential contributions to maintain diversity, dynamically adjusting the neighborhood size to escape local optima and explore new regions of the search space. The algorithm balances exploration and exploitation by adaptively weighting objectives and adjusting neighborhood sizes, while always maintaining feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto-ranked selection with weighted crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search with adaptive subset flips and weighted improvements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(10, max(2, int(n_items * 0.2 * (1 - current_weight / capacity))))\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for _ in range(neighborhood_size):\n        subset_size = max(2, min(10, int(n_items * 0.3 * (1 - current_weight / capacity))))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(n_items) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9165812833708862,
            1.2528109848499298
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto-ranked selection with weighted crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search with adaptive subset flips and weighted improvements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(10, max(2, int(n_items * 0.2 * (1 - current_weight / capacity))))\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for _ in range(neighborhood_size):\n        subset_size = max(2, min(10, int(n_items * 0.3 * (1 - current_weight / capacity))))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(n_items) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 7 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive selection - prioritize solutions with high crowding distance and normalized objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance with normalized objective diversity\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_diversity = np.zeros(len(archive))\n    for i, (v1, v2) in enumerate(objectives):\n        normalized_diversity[i] = (v1 / max_v1 + v2 / max_v2) if max_v1 > 0 and max_v2 > 0 else 0\n\n    selection_scores = crowding_distances * normalized_diversity\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search - adaptive subset flips with objective-weighted improvements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic neighborhood size based on solution quality\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Strategy 1: Adaptive subset flip with objective-weighted selection\n    for _ in range(neighborhood_size):\n        # Select a random subset of items to consider\n        subset_size = min(3, max(1, int(n_items * 0.2)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            # Try flipping the item\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            # Calculate new weight\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            # Check feasibility\n            if new_weight > capacity:\n                continue\n\n            # Calculate objective improvements\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-weighted improvement\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Strategy 2: Objective-weighted item swaps if no improvement found\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    # Objective-weighted swap decision\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using Pareto-ranked crowding-distance metrics, then applies a hybrid local search combining adaptive subset flips and objective-weighted swaps, dynamically adjusting neighborhood size based on solution quality and feasibility to generate high-quality, feasible neighbor solutions. It prioritizes solutions with high crowding distance (diversity) and Pareto rank (non-dominated status), then uses weighted improvements in both objectives to guide local moves while ensuring feasibility through careful weight checks. The neighborhood size adapts to the remaining capacity, balancing exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto-ranked selection with crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search with adaptive operators\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Adaptive subset flips with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Objective-weighted swaps if no improvement\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThis algorithm combines adaptive Pareto selection with a novel \"objective-aligned cluster flip\" operator, where solutions are first filtered for Pareto efficiency, then selected based on crowding distance and objective balance, followed by dynamic cluster flips prioritizing items that proportionally improve both objectives, with neighborhood size and flip probability adapting to weight utilization. The method ensures feasibility through precomputed contributions and feasibility checks, with a fallback to random flips when no improvements are found.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Filter Pareto-efficient solutions\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distance and objective alignment\n            weighted_scores = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min(obj[0] for _, obj in pareto_front) + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                alignment = min(obj[0], obj[1]) / (max(obj[0], obj[1]) + 1e-6) if (obj[0] + obj[1]) > 0 else 0.5\n                weighted_score = 0.6 * (dist_v1 + dist_v2) + 0.4 * alignment\n                weighted_scores.append(weighted_score)\n\n            # Select solution with highest weighted score\n            max_score_idx = np.argmax(weighted_scores)\n            base_solution = pareto_front[max_score_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic cluster size and flip probability\n    weight_util = current_weight / capacity\n    cluster_size = max(1, int(len(new_solution) * (0.3 - 0.15 * weight_util)))\n    flip_prob = 0.5 + 0.4 * (1 - weight_util)\n\n    # Objective-aligned cluster flip\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        if np.random.rand() < flip_prob:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-aligned improvement calculation\n            alignment = min(current_v1 + delta_v1, current_v2 + delta_v2) / (max(current_v1 + delta_v1, current_v2 + delta_v2) + 1e-6) if (current_v1 + delta_v1 + current_v2 + delta_v2) > 0 else 0.5\n            improvement = (delta_v1 + delta_v2) * alignment\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Fallback: random flip with feasibility check\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = np.random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThis heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nThe algorithm first identifies the Pareto front from the archive and selects a base solution using a weighted crowding-distance metric to prioritize high-diversity solutions. It then applies a hybrid local search that dynamically adjusts the neighborhood size based on the solution's weight utilization and objective weights, evaluating flips within the neighborhood while ensuring feasibility. If no improving move is found, it performs a probabilistic flip based on potential weighted contributions, dynamically adjusting objective weights to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Identify Pareto front\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            min_v1 = min(obj[0] for _, obj in pareto_front)\n            min_v2 = min(obj[1] for _, obj in pareto_front)\n\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min_v1 + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min_v2 + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design for the BI-KP, optimized for high-quality, feasible neighbors:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 62,
        "algorithm": "The algorithm selects promising solutions from the archive using crowding-distance-based Pareto front selection, then applies a hybrid local search that adaptively flips item selections based on weighted objective improvements while ensuring feasibility, with dynamic neighborhood sizes and probabilistic selection for exploration. It prioritizes solutions with high crowding distances, balances objective trade-offs through dynamic weights, and ensures feasibility through careful weight checks and probabilistic flips.\n\n*(Key design ideas: crowding-distance selection, adaptive neighborhood sizes, objective-weighted improvements, probabilistic feasibility checks, and dynamic objective weights.)*",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive Pareto-front selection with crowding-distance weighting\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select top 30% of solutions by crowding distance\n    threshold = np.percentile(crowding_distances, 70)\n    candidates = [sol for (sol, _), dist in zip(archive, crowding_distances) if dist >= threshold]\n    base_solution = random.choice(candidates).copy() if candidates else archive[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size (20-50% of items) based on remaining capacity\n    neighborhood_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(len(new_solution), size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights (inverse of current objective ratios)\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search: adaptive subset flips with objective-weighted improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with feasibility check\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Select flip based on potential weighted contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9197964225919173,
            0.8178737759590149
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive Pareto-front selection with crowding-distance weighting\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select top 30% of solutions by crowding distance\n    threshold = np.percentile(crowding_distances, 70)\n    candidates = [sol for (sol, _), dist in zip(archive, crowding_distances) if dist >= threshold]\n    base_solution = random.choice(candidates).copy() if candidates else archive[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size (20-50% of items) based on remaining capacity\n    neighborhood_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(len(new_solution), size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights (inverse of current objective ratios)\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search: adaptive subset flips with objective-weighted improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with feasibility check\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Select flip based on potential weighted contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines adaptive Pareto-front clustering with weighted path relinking to generate neighbors, prioritizing solutions from the largest cluster and dynamically adjusting flip probabilities based on item contributions to both objectives while ensuring feasibility through precomputed weight validation and greedy fallback moves.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Cluster solutions by Pareto dominance\n        clusters = []\n        for sol, obj in archive:\n            assigned = False\n            for cluster in clusters:\n                if all(any(obj[i] >= c_obj[i] for i in range(2)) and not all(obj[i] >= c_obj[i] for i in range(2)) for _, c_obj in cluster):\n                    cluster.append((sol, obj))\n                    assigned = True\n                    break\n            if not assigned:\n                clusters.append([(sol, obj)])\n\n        # Select base solution from largest cluster\n        largest_cluster = max(clusters, key=len)\n        base_solution = random.choice(largest_cluster)[0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Path relinking with weighted contributions\n    target_solution = random.choice([sol for sol, _ in largest_cluster if not np.array_equal(sol, base_solution)])\n    diff_indices = np.where(base_solution != target_solution)[0]\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in diff_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy diversity-aware flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Prefer items with high contribution to underrepresented objective\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design for the BI-KP, optimized for high-quality, feasible neighbors:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 63,
        "algorithm": "The algorithm selects a promising solution from the archive by clustering Pareto-dominated solutions and prioritizing those with high crowding distance, then performs a dynamic neighborhood search that balances improvements in both objectives using weighted flips, while ensuring feasibility through precomputed contributions and a probabilistic fallback strategy. It adaptively adjusts neighborhood size and objective weights based on solution quality, favoring items that contribute more to underrepresented objectives. The approach combines Pareto-based clustering, crowding distance, and weighted objective improvements with a greedy fallback for exploration.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Cluster solutions by Pareto dominance\n        clusters = []\n        for sol, obj in archive:\n            assigned = False\n            for cluster in clusters:\n                if all(any(obj[i] >= c_obj[i] for i in range(2)) and not all(obj[i] >= c_obj[i] for i in range(2)) for _, c_obj in cluster):\n                    cluster.append((sol, obj))\n                    assigned = True\n                    break\n            if not assigned:\n                clusters.append([(sol, obj)])\n\n        # Select base solution from largest cluster with highest crowding distance\n        largest_cluster = max(clusters, key=len)\n        # Calculate crowding distance for solutions in the largest cluster\n        crowding_distances = []\n        for sol, obj in largest_cluster:\n            distances = []\n            for i in range(2):\n                sorted_objs = sorted([c_obj[i] for _, c_obj in largest_cluster])\n                if len(sorted_objs) < 2:\n                    distances.append(0)\n                else:\n                    if obj[i] == min(sorted_objs):\n                        distances.append(float('inf'))\n                    elif obj[i] == max(sorted_objs):\n                        distances.append(0)\n                    else:\n                        idx = sorted_objs.index(obj[i])\n                        distances.append((sorted_objs[idx+1] - sorted_objs[idx-1]) / (sorted_objs[-1] - sorted_objs[0] + 1e-6))\n            crowding_distances.append(sum(distances))\n\n        # Select solution with highest crowding distance\n        base_solution = max(zip([sol for sol, _ in largest_cluster], crowding_distances), key=lambda x: x[1])[0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate objective weights based on current solution's balance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Dynamic neighborhood size based on solution quality\n    neighborhood_size = min(10, max(1, int(0.3 * len(new_solution))))  # 30% of items, but at least 1 and at most 10\n\n    # Evaluate potential improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for _ in range(neighborhood_size):\n        # Select a random item to flip\n        idx = random.randint(0, len(new_solution) - 1)\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements and crowding distance as the selection criterion\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy diversity-aware flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Prefer items with high contribution to underrepresented objective\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.7912959966249464,
            0.9158434569835663
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Cluster solutions by Pareto dominance\n        clusters = []\n        for sol, obj in archive:\n            assigned = False\n            for cluster in clusters:\n                if all(any(obj[i] >= c_obj[i] for i in range(2)) and not all(obj[i] >= c_obj[i] for i in range(2)) for _, c_obj in cluster):\n                    cluster.append((sol, obj))\n                    assigned = True\n                    break\n            if not assigned:\n                clusters.append([(sol, obj)])\n\n        # Select base solution from largest cluster with highest crowding distance\n        largest_cluster = max(clusters, key=len)\n        # Calculate crowding distance for solutions in the largest cluster\n        crowding_distances = []\n        for sol, obj in largest_cluster:\n            distances = []\n            for i in range(2):\n                sorted_objs = sorted([c_obj[i] for _, c_obj in largest_cluster])\n                if len(sorted_objs) < 2:\n                    distances.append(0)\n                else:\n                    if obj[i] == min(sorted_objs):\n                        distances.append(float('inf'))\n                    elif obj[i] == max(sorted_objs):\n                        distances.append(0)\n                    else:\n                        idx = sorted_objs.index(obj[i])\n                        distances.append((sorted_objs[idx+1] - sorted_objs[idx-1]) / (sorted_objs[-1] - sorted_objs[0] + 1e-6))\n            crowding_distances.append(sum(distances))\n\n        # Select solution with highest crowding distance\n        base_solution = max(zip([sol for sol, _ in largest_cluster], crowding_distances), key=lambda x: x[1])[0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate objective weights based on current solution's balance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Dynamic neighborhood size based on solution quality\n    neighborhood_size = min(10, max(1, int(0.3 * len(new_solution))))  # 30% of items, but at least 1 and at most 10\n\n    # Evaluate potential improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for _ in range(neighborhood_size):\n        # Select a random item to flip\n        idx = random.randint(0, len(new_solution) - 1)\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements and crowding distance as the selection criterion\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy diversity-aware flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Prefer items with high contribution to underrepresented objective\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and weighted crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min(obj[0] for _, obj in pareto_front) + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines adaptive Pareto-front clustering with weighted path relinking to generate neighbors, prioritizing solutions from the largest cluster and dynamically adjusting flip probabilities based on item contributions to both objectives while ensuring feasibility through precomputed weight validation and greedy fallback moves.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Cluster solutions by Pareto dominance\n        clusters = []\n        for sol, obj in archive:\n            assigned = False\n            for cluster in clusters:\n                if all(any(obj[i] >= c_obj[i] for i in range(2)) and not all(obj[i] >= c_obj[i] for i in range(2)) for _, c_obj in cluster):\n                    cluster.append((sol, obj))\n                    assigned = True\n                    break\n            if not assigned:\n                clusters.append([(sol, obj)])\n\n        # Select base solution from largest cluster\n        largest_cluster = max(clusters, key=len)\n        base_solution = random.choice(largest_cluster)[0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Path relinking with weighted contributions\n    target_solution = random.choice([sol for sol, _ in largest_cluster if not np.array_equal(sol, base_solution)])\n    diff_indices = np.where(base_solution != target_solution)[0]\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in diff_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy diversity-aware flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Prefer items with high contribution to underrepresented objective\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design for the BI-KP, optimized for high-quality, feasible neighbors:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 64,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution with highest weighted objective diversity\n        weighted_diversities = []\n        for sol, obj in archive:\n            v1, v2 = obj\n            diversity = (v1 / (v2 + 1e-6)) + (v2 / (v1 + 1e-6))\n            weighted_diversities.append(diversity)\n        base_solution = archive[np.argmax(weighted_diversities)][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive neighborhood selection based on solution balance\n    total_items = len(new_solution)\n    balance = np.sum(value1_lst[new_solution == 1]) / (np.sum(value2_lst[new_solution == 1]) + 1e-6)\n    neighborhood_size = max(1, int(total_items * (0.3 + 0.2 * (1 - min(balance, 1/balance)))))\n\n    # Dynamic objective weighting based on solution's current imbalance\n    if balance > 1:\n        weight_v1, weight_v2 = 0.7, 0.3\n    elif balance < 1:\n        weight_v1, weight_v2 = 0.3, 0.7\n    else:\n        weight_v1, weight_v2 = 0.5, 0.5\n\n    # Multi-flip local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for _ in range(min(3, neighborhood_size)):\n        temp_solution = new_solution.copy()\n        # Select multiple items to flip (1-3)\n        flip_count = np.random.randint(1, min(4, neighborhood_size))\n        flip_indices = np.random.choice(total_items, size=flip_count, replace=False)\n\n        new_weight = current_weight\n        for idx in flip_indices:\n            if temp_solution[idx] == 1:\n                new_weight -= weight_lst[idx]\n            else:\n                new_weight += weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        improvement = 0\n        for idx in flip_indices:\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 0 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 0 else -value2_lst[idx]\n            improvement += weight_v1 * delta_v1 + weight_v2 * delta_v2\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution.copy()\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Feasibility-preserving random flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.46915393694341895,
            0.4114633798599243
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution with highest weighted objective diversity\n        weighted_diversities = []\n        for sol, obj in archive:\n            v1, v2 = obj\n            diversity = (v1 / (v2 + 1e-6)) + (v2 / (v1 + 1e-6))\n            weighted_diversities.append(diversity)\n        base_solution = archive[np.argmax(weighted_diversities)][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive neighborhood selection based on solution balance\n    total_items = len(new_solution)\n    balance = np.sum(value1_lst[new_solution == 1]) / (np.sum(value2_lst[new_solution == 1]) + 1e-6)\n    neighborhood_size = max(1, int(total_items * (0.3 + 0.2 * (1 - min(balance, 1/balance)))))\n\n    # Dynamic objective weighting based on solution's current imbalance\n    if balance > 1:\n        weight_v1, weight_v2 = 0.7, 0.3\n    elif balance < 1:\n        weight_v1, weight_v2 = 0.3, 0.7\n    else:\n        weight_v1, weight_v2 = 0.5, 0.5\n\n    # Multi-flip local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for _ in range(min(3, neighborhood_size)):\n        temp_solution = new_solution.copy()\n        # Select multiple items to flip (1-3)\n        flip_count = np.random.randint(1, min(4, neighborhood_size))\n        flip_indices = np.random.choice(total_items, size=flip_count, replace=False)\n\n        new_weight = current_weight\n        for idx in flip_indices:\n            if temp_solution[idx] == 1:\n                new_weight -= weight_lst[idx]\n            else:\n                new_weight += weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        improvement = 0\n        for idx in flip_indices:\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 0 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 0 else -value2_lst[idx]\n            improvement += weight_v1 * delta_v1 + weight_v2 * delta_v2\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution.copy()\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Feasibility-preserving random flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines adaptive Pareto-front clustering with weighted path relinking to generate neighbors, prioritizing solutions from the largest cluster and dynamically adjusting flip probabilities based on item contributions to both objectives while ensuring feasibility through precomputed weight validation and greedy fallback moves.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Cluster solutions by Pareto dominance\n        clusters = []\n        for sol, obj in archive:\n            assigned = False\n            for cluster in clusters:\n                if all(any(obj[i] >= c_obj[i] for i in range(2)) and not all(obj[i] >= c_obj[i] for i in range(2)) for _, c_obj in cluster):\n                    cluster.append((sol, obj))\n                    assigned = True\n                    break\n            if not assigned:\n                clusters.append([(sol, obj)])\n\n        # Select base solution from largest cluster\n        largest_cluster = max(clusters, key=len)\n        base_solution = random.choice(largest_cluster)[0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Path relinking with weighted contributions\n    target_solution = random.choice([sol for sol, _ in largest_cluster if not np.array_equal(sol, base_solution)])\n    diff_indices = np.where(base_solution != target_solution)[0]\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in diff_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy diversity-aware flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Prefer items with high contribution to underrepresented objective\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 65,
        "algorithm": "This algorithm selects a solution from the archive by clustering non-dominated solutions and choosing from the largest cluster, then applies a temperature-controlled stochastic local search that flips item selections based on their weighted contribution to both objectives, dynamically balancing exploration and exploitation while ensuring feasibility. The adaptive objective weights prioritize underrepresented objectives, and the temperature parameter controls the randomness of the selection process.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Cluster solutions by dominance and select from largest cluster\n        clusters = []\n        for sol, obj in archive:\n            assigned = False\n            for cluster in clusters:\n                if all(any(obj[i] >= c_obj[i] for i in range(2)) and not all(obj[i] >= c_obj[i] for i in range(2)) for _, c_obj in cluster):\n                    cluster.append((sol, obj))\n                    assigned = True\n                    break\n            if not assigned:\n                clusters.append([(sol, obj)])\n\n        largest_cluster = max(clusters, key=len)\n        base_solution = random.choice(largest_cluster)[0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate adaptive objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Temperature for stochastic control\n    temperature = 0.7 * (1 - len(new_solution) / len(weight_lst))\n\n    # Objective-guided flip mechanism\n    valid_indices = [i for i in range(len(new_solution)) if\n                    (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                    (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n\n    if valid_indices:\n        # Calculate flip scores\n        scores = []\n        for i in valid_indices:\n            delta_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n            delta_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n            score = weight_v1 * delta_v1 + weight_v2 * delta_v2\n            scores.append(score)\n\n        # Apply temperature-controlled stochastic selection\n        if sum(scores) > 0:\n            probabilities = [np.exp(s / temperature) / sum(np.exp(s / temperature) for s in scores) for s in scores]\n            idx = np.random.choice(valid_indices, p=probabilities)\n        else:\n            idx = random.choice(valid_indices)\n\n        new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9268215584089741,
            1.0663727223873138
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Cluster solutions by dominance and select from largest cluster\n        clusters = []\n        for sol, obj in archive:\n            assigned = False\n            for cluster in clusters:\n                if all(any(obj[i] >= c_obj[i] for i in range(2)) and not all(obj[i] >= c_obj[i] for i in range(2)) for _, c_obj in cluster):\n                    cluster.append((sol, obj))\n                    assigned = True\n                    break\n            if not assigned:\n                clusters.append([(sol, obj)])\n\n        largest_cluster = max(clusters, key=len)\n        base_solution = random.choice(largest_cluster)[0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate adaptive objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Temperature for stochastic control\n    temperature = 0.7 * (1 - len(new_solution) / len(weight_lst))\n\n    # Objective-guided flip mechanism\n    valid_indices = [i for i in range(len(new_solution)) if\n                    (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                    (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n\n    if valid_indices:\n        # Calculate flip scores\n        scores = []\n        for i in valid_indices:\n            delta_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n            delta_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n            score = weight_v1 * delta_v1 + weight_v2 * delta_v2\n            scores.append(score)\n\n        # Apply temperature-controlled stochastic selection\n        if sum(scores) > 0:\n            probabilities = [np.exp(s / temperature) / sum(np.exp(s / temperature) for s in scores) for s in scores]\n            idx = np.random.choice(valid_indices, p=probabilities)\n        else:\n            idx = random.choice(valid_indices)\n\n        new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines adaptive Pareto-front clustering with weighted path relinking to generate neighbors, prioritizing solutions from the largest cluster and dynamically adjusting flip probabilities based on item contributions to both objectives while ensuring feasibility through precomputed weight validation and greedy fallback moves.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Cluster solutions by Pareto dominance\n        clusters = []\n        for sol, obj in archive:\n            assigned = False\n            for cluster in clusters:\n                if all(any(obj[i] >= c_obj[i] for i in range(2)) and not all(obj[i] >= c_obj[i] for i in range(2)) for _, c_obj in cluster):\n                    cluster.append((sol, obj))\n                    assigned = True\n                    break\n            if not assigned:\n                clusters.append([(sol, obj)])\n\n        # Select base solution from largest cluster\n        largest_cluster = max(clusters, key=len)\n        base_solution = random.choice(largest_cluster)[0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Path relinking with weighted contributions\n    target_solution = random.choice([sol for sol, _ in largest_cluster if not np.array_equal(sol, base_solution)])\n    diff_indices = np.where(base_solution != target_solution)[0]\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in diff_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy diversity-aware flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Prefer items with high contribution to underrepresented objective\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 66,
        "algorithm": "This algorithm combines NSGA-II's crowding distance selection with adaptive mutation rates to prioritize solutions on the Pareto front and generate neighbors by probabilistically flipping items based on their marginal contributions to both objectives, while ensuring feasibility through a two-phase validation process that removes low-contribution items when capacity is exceeded. Solutions with higher crowding distances are favored for selection, and mutation probabilities are dynamically adjusted based on each item's impact on both objectives, with a fallback mechanism to remove items when feasibility is violated.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Calculate crowding distances for all solutions\n        solutions = [sol for sol, _ in archive]\n        objectives = [obj for _, obj in archive]\n\n        # Normalize objectives\n        v1_min, v1_max = min(obj[0] for obj in objectives), max(obj[0] for obj in objectives)\n        v2_min, v2_max = min(obj[1] for obj in objectives), max(obj[1] for obj in objectives)\n\n        normalized = []\n        for obj in objectives:\n            norm_v1 = (obj[0] - v1_min) / (v1_max - v1_min + 1e-6)\n            norm_v2 = (obj[1] - v2_min) / (v2_max - v2_min + 1e-6)\n            normalized.append((norm_v1, norm_v2))\n\n        # Sort by both objectives\n        sorted_v1 = sorted(range(len(normalized)), key=lambda i: normalized[i][0])\n        sorted_v2 = sorted(range(len(normalized)), key=lambda i: normalized[i][1])\n\n        # Calculate crowding distances\n        crowding = [0.0] * len(normalized)\n        for i in range(2):\n            sorted_idx = sorted_v1 if i == 0 else sorted_v2\n            crowding[sorted_idx[0]] = float('inf')\n            crowding[sorted_idx[-1]] = float('inf')\n            for j in range(1, len(normalized)-1):\n                crowding[sorted_idx[j]] += (normalized[sorted_idx[j+1]][i] - normalized[sorted_idx[j-1]][i])\n\n        # Select solution with highest crowding distance\n        max_crowding_idx = max(range(len(crowding)), key=lambda i: crowding[i])\n        base_solution = solutions[max_crowding_idx].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate mutation probabilities based on marginal contributions\n    mutation_probs = []\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # Probability to remove item\n            delta_v1 = -value1_lst[i]\n            delta_v2 = -value2_lst[i]\n            prob = (delta_v1 / (total_v1 + 1e-6) + delta_v2 / (total_v2 + 1e-6)) / 2\n        else:\n            # Probability to add item\n            if weight_lst[i] > remaining_capacity:\n                prob = 0.0\n            else:\n                delta_v1 = value1_lst[i]\n                delta_v2 = value2_lst[i]\n                prob = (delta_v1 / (total_v1 + 1e-6) + delta_v2 / (total_v2 + 1e-6)) / 2\n\n        mutation_probs.append(prob)\n\n    # Normalize probabilities\n    total_prob = sum(mutation_probs)\n    if total_prob > 0:\n        mutation_probs = [p / total_prob for p in mutation_probs]\n    else:\n        mutation_probs = [1.0 / len(mutation_probs)] * len(mutation_probs)\n\n    # Select item to flip\n    idx = np.random.choice(len(new_solution), p=mutation_probs)\n    new_solution[idx] = 1 - new_solution[idx]\n\n    # Validate feasibility\n    new_weight = np.sum(weight_lst[new_solution == 1])\n    if new_weight > capacity:\n        # Fallback: remove items with lowest marginal contribution\n        while new_weight > capacity:\n            remove_candidates = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n            if not remove_candidates:\n                break\n            # Find item with lowest marginal contribution\n            min_contribution = float('inf')\n            best_idx = -1\n            for i in remove_candidates:\n                delta_v1 = -value1_lst[i]\n                delta_v2 = -value2_lst[i]\n                contribution = (delta_v1 / (total_v1 + 1e-6) + delta_v2 / (total_v2 + 1e-6)) / 2\n                if contribution < min_contribution:\n                    min_contribution = contribution\n                    best_idx = i\n            if best_idx != -1:\n                new_solution[best_idx] = 0\n                new_weight -= weight_lst[best_idx]\n            else:\n                break\n\n    return new_solution\n\n",
        "score": [
            -0.7723137524882135,
            0.8807046115398407
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Calculate crowding distances for all solutions\n        solutions = [sol for sol, _ in archive]\n        objectives = [obj for _, obj in archive]\n\n        # Normalize objectives\n        v1_min, v1_max = min(obj[0] for obj in objectives), max(obj[0] for obj in objectives)\n        v2_min, v2_max = min(obj[1] for obj in objectives), max(obj[1] for obj in objectives)\n\n        normalized = []\n        for obj in objectives:\n            norm_v1 = (obj[0] - v1_min) / (v1_max - v1_min + 1e-6)\n            norm_v2 = (obj[1] - v2_min) / (v2_max - v2_min + 1e-6)\n            normalized.append((norm_v1, norm_v2))\n\n        # Sort by both objectives\n        sorted_v1 = sorted(range(len(normalized)), key=lambda i: normalized[i][0])\n        sorted_v2 = sorted(range(len(normalized)), key=lambda i: normalized[i][1])\n\n        # Calculate crowding distances\n        crowding = [0.0] * len(normalized)\n        for i in range(2):\n            sorted_idx = sorted_v1 if i == 0 else sorted_v2\n            crowding[sorted_idx[0]] = float('inf')\n            crowding[sorted_idx[-1]] = float('inf')\n            for j in range(1, len(normalized)-1):\n                crowding[sorted_idx[j]] += (normalized[sorted_idx[j+1]][i] - normalized[sorted_idx[j-1]][i])\n\n        # Select solution with highest crowding distance\n        max_crowding_idx = max(range(len(crowding)), key=lambda i: crowding[i])\n        base_solution = solutions[max_crowding_idx].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate marginal contributions\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate mutation probabilities based on marginal contributions\n    mutation_probs = []\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            # Probability to remove item\n            delta_v1 = -value1_lst[i]\n            delta_v2 = -value2_lst[i]\n            prob = (delta_v1 / (total_v1 + 1e-6) + delta_v2 / (total_v2 + 1e-6)) / 2\n        else:\n            # Probability to add item\n            if weight_lst[i] > remaining_capacity:\n                prob = 0.0\n            else:\n                delta_v1 = value1_lst[i]\n                delta_v2 = value2_lst[i]\n                prob = (delta_v1 / (total_v1 + 1e-6) + delta_v2 / (total_v2 + 1e-6)) / 2\n\n        mutation_probs.append(prob)\n\n    # Normalize probabilities\n    total_prob = sum(mutation_probs)\n    if total_prob > 0:\n        mutation_probs = [p / total_prob for p in mutation_probs]\n    else:\n        mutation_probs = [1.0 / len(mutation_probs)] * len(mutation_probs)\n\n    # Select item to flip\n    idx = np.random.choice(len(new_solution), p=mutation_probs)\n    new_solution[idx] = 1 - new_solution[idx]\n\n    # Validate feasibility\n    new_weight = np.sum(weight_lst[new_solution == 1])\n    if new_weight > capacity:\n        # Fallback: remove items with lowest marginal contribution\n        while new_weight > capacity:\n            remove_candidates = [i for i in range(len(new_solution)) if new_solution[i] == 1]\n            if not remove_candidates:\n                break\n            # Find item with lowest marginal contribution\n            min_contribution = float('inf')\n            best_idx = -1\n            for i in remove_candidates:\n                delta_v1 = -value1_lst[i]\n                delta_v2 = -value2_lst[i]\n                contribution = (delta_v1 / (total_v1 + 1e-6) + delta_v2 / (total_v2 + 1e-6)) / 2\n                if contribution < min_contribution:\n                    min_contribution = contribution\n                    best_idx = i\n            if best_idx != -1:\n                new_solution[best_idx] = 0\n                new_weight -= weight_lst[best_idx]\n            else:\n                break\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 4 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines adaptive Pareto-front clustering with weighted path relinking to generate neighbors, prioritizing solutions from the largest cluster and dynamically adjusting flip probabilities based on item contributions to both objectives while ensuring feasibility through precomputed weight validation and greedy fallback moves.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Cluster solutions by Pareto dominance\n        clusters = []\n        for sol, obj in archive:\n            assigned = False\n            for cluster in clusters:\n                if all(any(obj[i] >= c_obj[i] for i in range(2)) and not all(obj[i] >= c_obj[i] for i in range(2)) for _, c_obj in cluster):\n                    cluster.append((sol, obj))\n                    assigned = True\n                    break\n            if not assigned:\n                clusters.append([(sol, obj)])\n\n        # Select base solution from largest cluster\n        largest_cluster = max(clusters, key=len)\n        base_solution = random.choice(largest_cluster)[0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Path relinking with weighted contributions\n    target_solution = random.choice([sol for sol, _ in largest_cluster if not np.array_equal(sol, base_solution)])\n    diff_indices = np.where(base_solution != target_solution)[0]\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in diff_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy diversity-aware flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Prefer items with high contribution to underrepresented objective\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto ranking with crowding-distance selection to prioritize diverse, high-quality solutions from the archive, then applies an adaptive cluster-based local search that dynamically adjusts flip cluster sizes based on weight utilization and objective weights, ensuring feasibility through precomputed item contributions and move validation. It balances exploration/exploitation by flipping items within clusters while favoring moves that improve both objectives proportionally, falling back to random valid flips if no improvement is found.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max(obj[0] for _, obj in pareto_front) - min(obj[0] for _, obj in pareto_front) + 1e-6) + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max(obj[1] for _, obj in pareto_front) - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                crowding_distances.append(dist)\n\n            # Select solution with highest crowding distance\n            max_dist_idx = np.argmax(crowding_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive cluster-based item flips\n    cluster_size = max(1, int(len(new_solution) * 0.1 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design for the BI-KP, optimized for high-quality, feasible neighbors:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 67,
        "algorithm": "This algorithm selects a promising solution from the archive using a weighted crowding-distance metric to prioritize high-potential candidates, then applies a hybrid local search that dynamically adjusts neighborhood size (10-30% of items) based on capacity utilization. It evaluates flipping subsets of items with objective-weighted improvements, falling back to probabilistic flips if no improvement is found, while always ensuring feasibility by validating moves. The dynamic objective weights (weight_v1, weight_v2) prioritize under-represented objectives, and the probabilistic fallback ensures exploration when no clear improvement exists.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Weighted crowding-distance selection\n        crowding_weights = []\n        for sol, obj in archive:\n            # Calculate normalized objectives\n            norm_v1 = obj[0] / (max(obj[0] for _, obj in archive) + 1e-6)\n            norm_v2 = obj[1] / (max(obj[1] for _, obj in archive) + 1e-6)\n            # Weighted crowding score\n            crowding_weights.append(norm_v1 * norm_v2)\n\n        # Select top 30% of solutions by weighted crowding\n        threshold = np.percentile(crowding_weights, 70)\n        candidates = [sol for (sol, _), score in zip(archive, crowding_weights) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size (10-30% of items)\n    subset_size = max(1, int(len(new_solution) * (0.1 + 0.2 * (1 - current_weight / capacity))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8122509306407175,
            0.9820201098918915
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Weighted crowding-distance selection\n        crowding_weights = []\n        for sol, obj in archive:\n            # Calculate normalized objectives\n            norm_v1 = obj[0] / (max(obj[0] for _, obj in archive) + 1e-6)\n            norm_v2 = obj[1] / (max(obj[1] for _, obj in archive) + 1e-6)\n            # Weighted crowding score\n            crowding_weights.append(norm_v1 * norm_v2)\n\n        # Select top 30% of solutions by weighted crowding\n        threshold = np.percentile(crowding_weights, 70)\n        candidates = [sol for (sol, _), score in zip(archive, crowding_weights) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size (10-30% of items)\n    subset_size = max(1, int(len(new_solution) * (0.1 + 0.2 * (1 - current_weight / capacity))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 7 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive selection - prioritize solutions with high crowding distance and normalized objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance with normalized objective diversity\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_diversity = np.zeros(len(archive))\n    for i, (v1, v2) in enumerate(objectives):\n        normalized_diversity[i] = (v1 / max_v1 + v2 / max_v2) if max_v1 > 0 and max_v2 > 0 else 0\n\n    selection_scores = crowding_distances * normalized_diversity\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search - adaptive subset flips with objective-weighted improvements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic neighborhood size based on solution quality\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Strategy 1: Adaptive subset flip with objective-weighted selection\n    for _ in range(neighborhood_size):\n        # Select a random subset of items to consider\n        subset_size = min(3, max(1, int(n_items * 0.2)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            # Try flipping the item\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            # Calculate new weight\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            # Check feasibility\n            if new_weight > capacity:\n                continue\n\n            # Calculate objective improvements\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-weighted improvement\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Strategy 2: Objective-weighted item swaps if no improvement found\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    # Objective-weighted swap decision\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using Pareto-ranked crowding-distance metrics, then applies a hybrid local search combining adaptive subset flips and objective-weighted swaps, dynamically adjusting neighborhood size based on solution quality and feasibility to generate high-quality, feasible neighbor solutions. It prioritizes solutions with high crowding distance (diversity) and Pareto rank (non-dominated status), then uses weighted improvements in both objectives to guide local moves while ensuring feasibility through careful weight checks. The neighborhood size adapts to the remaining capacity, balancing exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto-ranked selection with crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search with adaptive operators\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Adaptive subset flips with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Objective-weighted swaps if no improvement\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThis algorithm combines adaptive Pareto selection with a novel \"objective-aligned cluster flip\" operator, where solutions are first filtered for Pareto efficiency, then selected based on crowding distance and objective balance, followed by dynamic cluster flips prioritizing items that proportionally improve both objectives, with neighborhood size and flip probability adapting to weight utilization. The method ensures feasibility through precomputed contributions and feasibility checks, with a fallback to random flips when no improvements are found.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Filter Pareto-efficient solutions\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distance and objective alignment\n            weighted_scores = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min(obj[0] for _, obj in pareto_front) + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                alignment = min(obj[0], obj[1]) / (max(obj[0], obj[1]) + 1e-6) if (obj[0] + obj[1]) > 0 else 0.5\n                weighted_score = 0.6 * (dist_v1 + dist_v2) + 0.4 * alignment\n                weighted_scores.append(weighted_score)\n\n            # Select solution with highest weighted score\n            max_score_idx = np.argmax(weighted_scores)\n            base_solution = pareto_front[max_score_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic cluster size and flip probability\n    weight_util = current_weight / capacity\n    cluster_size = max(1, int(len(new_solution) * (0.3 - 0.15 * weight_util)))\n    flip_prob = 0.5 + 0.4 * (1 - weight_util)\n\n    # Objective-aligned cluster flip\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        if np.random.rand() < flip_prob:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-aligned improvement calculation\n            alignment = min(current_v1 + delta_v1, current_v2 + delta_v2) / (max(current_v1 + delta_v1, current_v2 + delta_v2) + 1e-6) if (current_v1 + delta_v1 + current_v2 + delta_v2) > 0 else 0.5\n            improvement = (delta_v1 + delta_v2) * alignment\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Fallback: random flip with feasibility check\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = np.random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThis heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nThe algorithm first identifies the Pareto front from the archive and selects a base solution using a weighted crowding-distance metric to prioritize high-diversity solutions. It then applies a hybrid local search that dynamically adjusts the neighborhood size based on the solution's weight utilization and objective weights, evaluating flips within the neighborhood while ensuring feasibility. If no improving move is found, it performs a probabilistic flip based on potential weighted contributions, dynamically adjusting objective weights to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Identify Pareto front\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            min_v1 = min(obj[0] for _, obj in pareto_front)\n            min_v2 = min(obj[1] for _, obj in pareto_front)\n\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min_v1 + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min_v2 + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors. *(Word count: 100)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 68,
        "algorithm": "This algorithm selects a base solution using a hybrid approach of Pareto dominance (70%) and crowding distance (30%) to balance exploration and exploitation, then applies a dynamic neighborhood search with adaptive objective weighting, probabilistic flips, and feasibility-aware cluster swaps to generate high-quality neighbors while ensuring capacity constraints are met. The method prioritizes solutions with better trade-offs between objectives and uses weighted improvements to guide local search, with fallback cluster swaps for diversification when no beneficial flips are found.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% crowding distance\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                base_solution = random.choice(pareto_front)[0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Crowding distance selection\n            objectives = np.array([obj for _, obj in archive])\n            crowding_distances = np.zeros(len(archive))\n            for obj_idx in range(2):\n                sorted_indices = np.argsort(objectives[:, obj_idx])\n                crowding_distances[sorted_indices[0]] = np.inf\n                crowding_distances[sorted_indices[-1]] = np.inf\n                for i in range(1, len(archive) - 1):\n                    if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                        crowding_distances[sorted_indices[i]] = np.inf\n                    else:\n                        crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx] + 1e-6)\n            selected_idx = np.argmax(crowding_distances)\n            base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size (20-50% of items)\n    neighborhood_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(len(new_solution), size=neighborhood_size, replace=False)\n\n    # Adaptive objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search: weighted flips + cluster swaps\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Phase 1: Weighted flips\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Phase 2: Feasibility-aware cluster swaps\n        cluster_size = max(1, int(neighborhood_size * 0.3))\n        for _ in range(3):  # 3 attempts\n            cluster_indices = np.random.choice(neighborhood_size, size=cluster_size, replace=False)\n            out_items = np.where(new_solution[cluster_indices] == 1)[0]\n            in_items = np.where(new_solution[cluster_indices] == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = cluster_indices[np.random.choice(out_items)]\n                in_idx = cluster_indices[np.random.choice(in_items)]\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (weight_v1 * delta_v1 + weight_v2 * delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        break\n\n    return new_solution\n\n",
        "score": [
            -0.8504667739891978,
            0.5975618660449982
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% crowding distance\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                base_solution = random.choice(pareto_front)[0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Crowding distance selection\n            objectives = np.array([obj for _, obj in archive])\n            crowding_distances = np.zeros(len(archive))\n            for obj_idx in range(2):\n                sorted_indices = np.argsort(objectives[:, obj_idx])\n                crowding_distances[sorted_indices[0]] = np.inf\n                crowding_distances[sorted_indices[-1]] = np.inf\n                for i in range(1, len(archive) - 1):\n                    if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                        crowding_distances[sorted_indices[i]] = np.inf\n                    else:\n                        crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx] + 1e-6)\n            selected_idx = np.argmax(crowding_distances)\n            base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size (20-50% of items)\n    neighborhood_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(len(new_solution), size=neighborhood_size, replace=False)\n\n    # Adaptive objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search: weighted flips + cluster swaps\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Phase 1: Weighted flips\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Phase 2: Feasibility-aware cluster swaps\n        cluster_size = max(1, int(neighborhood_size * 0.3))\n        for _ in range(3):  # 3 attempts\n            cluster_indices = np.random.choice(neighborhood_size, size=cluster_size, replace=False)\n            out_items = np.where(new_solution[cluster_indices] == 1)[0]\n            in_items = np.where(new_solution[cluster_indices] == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = cluster_indices[np.random.choice(out_items)]\n                in_idx = cluster_indices[np.random.choice(in_items)]\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (weight_v1 * delta_v1 + weight_v2 * delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        break\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm first identifies the Pareto front from the archive and selects a base solution using a weighted crowding-distance metric to prioritize high-diversity solutions. It then applies a hybrid local search that dynamically adjusts the neighborhood size based on the solution's weight utilization and objective weights, evaluating flips within the neighborhood while ensuring feasibility. If no improving move is found, it performs a probabilistic flip based on potential weighted contributions, dynamically adjusting objective weights to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Identify Pareto front\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            min_v1 = min(obj[0] for _, obj in pareto_front)\n            min_v2 = min(obj[1] for _, obj in pareto_front)\n\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min_v1 + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min_v2 + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThis algorithm combines Pareto dominance filtering with adaptive neighborhood diversification to generate high-quality neighbors. It selects solutions based on crowding distance from the Pareto front and uses dynamic cluster flips prioritized by weight-balanced item exchanges, adjusting flip sizes based on objective trade-offs and weight utilization, while falling back to greedy random flips if no improvement is found. The algorithm ensures feasibility by precomputing weight contributions and dynamically adjusting cluster sizes to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n        selected_idx = np.argmax(crowding_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Adaptive neighborhood diversification\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Dynamic cluster size based on weight utilization\n    cluster_size = max(1, int(len(new_solution) * 0.2 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Evaluate cluster flips\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy random flip if no improvement\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors. *(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 69,
        "algorithm": "The algorithm uses **adaptive Pareto clustering** to select a base solution from the least crowded cluster, then applies **weighted subset replacement** to improve objectives by dynamically adjusting item selections based on their contribution to each objective, while ensuring feasibility through dynamic cluster sizing and greedy flips when no improvement is found. It prioritizes solutions with lower crowding distance and balances objective improvements using weighted scores.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster solutions by Pareto dominance and crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution from the least crowded cluster\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n        selected_idx = np.argmin(crowding_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Adaptive subset replacement\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Dynamic cluster size based on weight utilization\n    cluster_size = max(1, int(len(new_solution) * 0.3 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Evaluate subset replacement\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy cluster-based flip if no improvement\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8591582648902856,
            0.6172443926334381
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster solutions by Pareto dominance and crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution from the least crowded cluster\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n        selected_idx = np.argmin(crowding_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Adaptive subset replacement\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Dynamic cluster size based on weight utilization\n    cluster_size = max(1, int(len(new_solution) * 0.3 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Evaluate subset replacement\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy cluster-based flip if no improvement\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThis heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThis algorithm combines Pareto dominance filtering with adaptive neighborhood diversification to generate high-quality neighbors. It selects solutions based on crowding distance from the Pareto front and uses dynamic cluster flips prioritized by weight-balanced item exchanges, adjusting flip sizes based on objective trade-offs and weight utilization, while falling back to greedy random flips if no improvement is found. The algorithm ensures feasibility by precomputing weight contributions and dynamically adjusting cluster sizes to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n        selected_idx = np.argmax(crowding_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Adaptive neighborhood diversification\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Dynamic cluster size based on weight utilization\n    cluster_size = max(1, int(len(new_solution) * 0.2 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Evaluate cluster flips\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy random flip if no improvement\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors. *(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 70,
        "algorithm": "The algorithm combines Pareto front clustering with dynamic objective-weighted cluster flips, prioritizing solutions based on crowding distance and hypervolume contributions while adaptively adjusting flip probabilities and cluster sizes to balance exploration and exploitation, using momentum to emphasize high-potential regions of the search space. Key design features include weighted improvement calculations, adaptive cluster sizing, and probabilistic flips with temperature-based selection to ensure feasible neighbor solutions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster solutions based on Pareto dominance and crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest combined crowding distance and hypervolume contribution\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster flip with momentum\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization and solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.3 + 0.2 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.9  # Higher momentum for more aggressive search\n\n    # Evaluate cluster flips with momentum\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -1.0478521338732665,
            0.5647115409374237
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster solutions based on Pareto dominance and crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest combined crowding distance and hypervolume contribution\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster flip with momentum\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization and solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.3 + 0.2 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.9  # Higher momentum for more aggressive search\n\n    # Evaluate cluster flips with momentum\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThis heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThis algorithm combines Pareto dominance filtering with adaptive neighborhood diversification to generate high-quality neighbors. It selects solutions based on crowding distance from the Pareto front and uses dynamic cluster flips prioritized by weight-balanced item exchanges, adjusting flip sizes based on objective trade-offs and weight utilization, while falling back to greedy random flips if no improvement is found. The algorithm ensures feasibility by precomputing weight contributions and dynamically adjusting cluster sizes to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n        selected_idx = np.argmax(crowding_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Adaptive neighborhood diversification\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Dynamic cluster size based on weight utilization\n    cluster_size = max(1, int(len(new_solution) * 0.2 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Evaluate cluster flips\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy random flip if no improvement\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors. *(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 70,
        "algorithm": "The algorithm combines Pareto front clustering with dynamic objective-weighted cluster flips, prioritizing solutions based on crowding distance and hypervolume contributions while adaptively adjusting flip probabilities and cluster sizes to balance exploration and exploitation, using momentum to emphasize high-potential regions of the search space. Key design features include weighted improvement calculations, adaptive cluster sizing, and probabilistic flips with temperature-based selection to ensure feasible neighbor solutions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster solutions based on Pareto dominance and crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest combined crowding distance and hypervolume contribution\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster flip with momentum\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization and solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.3 + 0.2 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.9  # Higher momentum for more aggressive search\n\n    # Evaluate cluster flips with momentum\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -1.0478521338732665,
            0.5647115409374237
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster solutions based on Pareto dominance and crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest combined crowding distance and hypervolume contribution\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster flip with momentum\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization and solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.3 + 0.2 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.9  # Higher momentum for more aggressive search\n\n    # Evaluate cluster flips with momentum\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        This algorithm combines Pareto dominance filtering with adaptive neighborhood diversification to generate high-quality neighbors. It selects solutions based on crowding distance from the Pareto front and uses dynamic cluster flips prioritized by weight-balanced item exchanges, adjusting flip sizes based on objective trade-offs and weight utilization, while falling back to greedy random flips if no improvement is found. The algorithm ensures feasibility by precomputing weight contributions and dynamically adjusting cluster sizes to balance exploration and exploitation.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n        selected_idx = np.argmax(crowding_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Adaptive neighborhood diversification\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Dynamic cluster size based on weight utilization\n    cluster_size = max(1, int(len(new_solution) * 0.2 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Evaluate cluster flips\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy random flip if no improvement\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 71,
        "algorithm": "This algorithm selects promising solutions from the archive based on dominance rank and objective diversity, then generates neighbors using adaptive cluster-based flips with dynamic cluster sizes adjusted by weight utilization and trade-off metrics. It prioritizes solutions with lower dominance counts and higher diversity, evaluates flips within clusters considering trade-off ratios, and falls back to greedy flips when no improvement is found, ensuring feasibility by always checking weight constraints. The trade-off ratio and cluster size adaptation dynamically balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Calculate dominance ranks and select solutions with lower ranks\n    dominance_counts = np.zeros(len(archive))\n    for i, (sol_i, obj_i) in enumerate(archive):\n        for j, (sol_j, obj_j) in enumerate(archive):\n            if i != j and obj_i[0] <= obj_j[0] and obj_i[1] <= obj_j[1] and (obj_i[0] < obj_j[0] or obj_i[1] < obj_j[1]):\n                dominance_counts[i] += 1\n\n    selected_indices = np.where(dominance_counts == np.min(dominance_counts))[0]\n    if len(selected_indices) == 0:\n        selected_indices = np.arange(len(archive))\n\n    # Step 2: Select solution with highest objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    diversity_scores = np.zeros(len(selected_indices))\n    for i, idx in enumerate(selected_indices):\n        diversity_scores[i] = np.sum(np.sqrt(np.sum((objectives - objectives[idx])**2, axis=1)))\n\n    selected_idx = selected_indices[np.argmax(diversity_scores)]\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Adaptive cluster-based flips with dynamic size adjustment\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    tradeoff_ratio = (total_v1 + total_v2) / (current_weight + 1e-6)\n\n    cluster_size = max(1, int(len(new_solution) * 0.15 * (1 - (current_weight / capacity)**2)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Evaluate cluster flips with tradeoff-aware evaluation\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering tradeoff ratio\n        improvement = (delta_v1 + delta_v2) * (1 + 0.5 * tradeoff_ratio)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy random flip with tradeoff consideration\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Select item with highest (value1 + value2)/weight ratio\n            ratios = [(value1_lst[i] + value2_lst[i]) / weight_lst[i] for i in valid_indices]\n            idx = valid_indices[np.argmax(ratios)]\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.7719822850414426,
            1.159808099269867
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Calculate dominance ranks and select solutions with lower ranks\n    dominance_counts = np.zeros(len(archive))\n    for i, (sol_i, obj_i) in enumerate(archive):\n        for j, (sol_j, obj_j) in enumerate(archive):\n            if i != j and obj_i[0] <= obj_j[0] and obj_i[1] <= obj_j[1] and (obj_i[0] < obj_j[0] or obj_i[1] < obj_j[1]):\n                dominance_counts[i] += 1\n\n    selected_indices = np.where(dominance_counts == np.min(dominance_counts))[0]\n    if len(selected_indices) == 0:\n        selected_indices = np.arange(len(archive))\n\n    # Step 2: Select solution with highest objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    diversity_scores = np.zeros(len(selected_indices))\n    for i, idx in enumerate(selected_indices):\n        diversity_scores[i] = np.sum(np.sqrt(np.sum((objectives - objectives[idx])**2, axis=1)))\n\n    selected_idx = selected_indices[np.argmax(diversity_scores)]\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Adaptive cluster-based flips with dynamic size adjustment\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    tradeoff_ratio = (total_v1 + total_v2) / (current_weight + 1e-6)\n\n    cluster_size = max(1, int(len(new_solution) * 0.15 * (1 - (current_weight / capacity)**2)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Evaluate cluster flips with tradeoff-aware evaluation\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering tradeoff ratio\n        improvement = (delta_v1 + delta_v2) * (1 + 0.5 * tradeoff_ratio)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy random flip with tradeoff consideration\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Select item with highest (value1 + value2)/weight ratio\n            ratios = [(value1_lst[i] + value2_lst[i]) / weight_lst[i] for i in valid_indices]\n            idx = valid_indices[np.argmax(ratios)]\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        This algorithm combines Pareto dominance filtering with adaptive neighborhood diversification to generate high-quality neighbors. It selects solutions based on crowding distance from the Pareto front and uses dynamic cluster flips prioritized by weight-balanced item exchanges, adjusting flip sizes based on objective trade-offs and weight utilization, while falling back to greedy random flips if no improvement is found. The algorithm ensures feasibility by precomputing weight contributions and dynamically adjusting cluster sizes to balance exploration and exploitation.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n        selected_idx = np.argmax(crowding_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Adaptive neighborhood diversification\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Dynamic cluster size based on weight utilization\n    cluster_size = max(1, int(len(new_solution) * 0.2 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Evaluate cluster flips\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy random flip if no improvement\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 72,
        "algorithm": "The algorithm selects a promising solution from the archive based on hypervolume contribution, then applies a hybrid local search that combines cluster flips with dynamic weight balancing, prioritizing items with high marginal utility while ensuring feasibility through capacity checks and fallback strategies. It adaptively adjusts cluster sizes based on weight utilization and objective correlation, and evaluates improvements using weighted marginal utilities to guide the search toward better solutions. The method balances exploration and exploitation by dynamically adjusting flip probabilities and incorporating weighted improvements.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest hypervolume contribution\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        # Calculate hypervolume contributions\n        objectives = np.array([obj for _, obj in pareto_front])\n        ref_point = np.max(objectives, axis=0)\n        contributions = np.zeros(len(pareto_front))\n        for i in range(len(pareto_front)):\n            temp_front = [j for j in range(len(pareto_front)) if j != i]\n            if not temp_front:\n                contributions[i] = np.prod(ref_point - objectives[i])\n            else:\n                temp_objectives = objectives[temp_front]\n                temp_ref = np.max(temp_objectives, axis=0)\n                contributions[i] = np.prod(ref_point - objectives[i]) - np.prod(temp_ref - objectives[i])\n        selected_idx = np.argmax(contributions)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Dynamic cluster size based on weight utilization and objective correlation\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    correlation = np.corrcoef(weight_lst, value1_lst + value2_lst)[0,1]\n    cluster_size = max(1, int(len(new_solution) * 0.15 * (1 - abs(correlation)) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    # Step 3: Objective-weighted local search with marginal utility\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Calculate marginal utilities\n    marginal_v1 = value1_lst / (weight_lst + 1e-6)\n    marginal_v2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal_v1 + marginal_v2) / 2\n\n    # Evaluate cluster flips with weighted improvements\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement with marginal utility factor\n        improvement = (delta_v1 * marginal_v1[idx] + delta_v2 * marginal_v2[idx]) * combined_marginal[idx]\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Weight-balanced random flip\n        weights = (marginal_v1 + marginal_v2) / (weight_lst + 1e-6)\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            weights = weights[valid_indices]\n            weights = weights / (np.sum(weights) + 1e-6)\n            idx = np.random.choice(valid_indices, p=weights)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.801629915014341,
            0.7038775980472565
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest hypervolume contribution\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        # Calculate hypervolume contributions\n        objectives = np.array([obj for _, obj in pareto_front])\n        ref_point = np.max(objectives, axis=0)\n        contributions = np.zeros(len(pareto_front))\n        for i in range(len(pareto_front)):\n            temp_front = [j for j in range(len(pareto_front)) if j != i]\n            if not temp_front:\n                contributions[i] = np.prod(ref_point - objectives[i])\n            else:\n                temp_objectives = objectives[temp_front]\n                temp_ref = np.max(temp_objectives, axis=0)\n                contributions[i] = np.prod(ref_point - objectives[i]) - np.prod(temp_ref - objectives[i])\n        selected_idx = np.argmax(contributions)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Dynamic cluster size based on weight utilization and objective correlation\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    correlation = np.corrcoef(weight_lst, value1_lst + value2_lst)[0,1]\n    cluster_size = max(1, int(len(new_solution) * 0.15 * (1 - abs(correlation)) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    # Step 3: Objective-weighted local search with marginal utility\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Calculate marginal utilities\n    marginal_v1 = value1_lst / (weight_lst + 1e-6)\n    marginal_v2 = value2_lst / (weight_lst + 1e-6)\n    combined_marginal = (marginal_v1 + marginal_v2) / 2\n\n    # Evaluate cluster flips with weighted improvements\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement with marginal utility factor\n        improvement = (delta_v1 * marginal_v1[idx] + delta_v2 * marginal_v2[idx]) * combined_marginal[idx]\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Weight-balanced random flip\n        weights = (marginal_v1 + marginal_v2) / (weight_lst + 1e-6)\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            weights = weights[valid_indices]\n            weights = weights / (np.sum(weights) + 1e-6)\n            idx = np.random.choice(valid_indices, p=weights)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 4 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThis algorithm combines Pareto dominance filtering with adaptive neighborhood diversification to generate high-quality neighbors. It selects solutions based on crowding distance from the Pareto front and uses dynamic cluster flips prioritized by weight-balanced item exchanges, adjusting flip sizes based on objective trade-offs and weight utilization, while falling back to greedy random flips if no improvement is found. The algorithm ensures feasibility by precomputing weight contributions and dynamically adjusting cluster sizes to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n        selected_idx = np.argmax(crowding_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Adaptive neighborhood diversification\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Dynamic cluster size based on weight utilization\n    cluster_size = max(1, int(len(new_solution) * 0.2 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Evaluate cluster flips\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy random flip if no improvement\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto ranking with crowding-distance selection to prioritize diverse, high-quality solutions from the archive, then applies an adaptive cluster-based local search that dynamically adjusts flip cluster sizes based on weight utilization and objective weights, ensuring feasibility through precomputed item contributions and move validation. It balances exploration/exploitation by flipping items within clusters while favoring moves that improve both objectives proportionally, falling back to random valid flips if no improvement is found.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max(obj[0] for _, obj in pareto_front) - min(obj[0] for _, obj in pareto_front) + 1e-6) + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max(obj[1] for _, obj in pareto_front) - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                crowding_distances.append(dist)\n\n            # Select solution with highest crowding distance\n            max_dist_idx = np.argmax(crowding_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive cluster-based item flips\n    cluster_size = max(1, int(len(new_solution) * 0.1 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors. *(Word count: 100)*\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 73,
        "algorithm": "The algorithm first filters non-dominated solutions from the archive and selects a base solution using a weighted crowding-distance metric that balances Pareto dominance and diversity. It then applies a hybrid local search with dynamic neighborhood sizes (20-50% of items) based on weight utilization, using objective-weighted improvement criteria to prioritize flips that enhance both objectives proportionally. If no improvement is found, it performs a probabilistic flip based on potential contributions while ensuring feasibility. The method dynamically adjusts objective weights to balance exploration and exploitation, generating high-quality neighbors that dominate or complement existing solutions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with weighted crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic neighborhood size based on weight utilization\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 4: Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 5: Hybrid local search with cluster flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering both objectives\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9225959738247063,
            0.5165721774101257
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with weighted crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic neighborhood size based on weight utilization\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 4: Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 5: Hybrid local search with cluster flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering both objectives\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 7 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive selection - prioritize solutions with high crowding distance and normalized objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance with normalized objective diversity\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_diversity = np.zeros(len(archive))\n    for i, (v1, v2) in enumerate(objectives):\n        normalized_diversity[i] = (v1 / max_v1 + v2 / max_v2) if max_v1 > 0 and max_v2 > 0 else 0\n\n    selection_scores = crowding_distances * normalized_diversity\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search - adaptive subset flips with objective-weighted improvements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic neighborhood size based on solution quality\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Strategy 1: Adaptive subset flip with objective-weighted selection\n    for _ in range(neighborhood_size):\n        # Select a random subset of items to consider\n        subset_size = min(3, max(1, int(n_items * 0.2)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            # Try flipping the item\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            # Calculate new weight\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            # Check feasibility\n            if new_weight > capacity:\n                continue\n\n            # Calculate objective improvements\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-weighted improvement\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Strategy 2: Objective-weighted item swaps if no improvement found\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    # Objective-weighted swap decision\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using Pareto-ranked crowding-distance metrics, then applies a hybrid local search combining adaptive subset flips and objective-weighted swaps, dynamically adjusting neighborhood size based on solution quality and feasibility to generate high-quality, feasible neighbor solutions. It prioritizes solutions with high crowding distance (diversity) and Pareto rank (non-dominated status), then uses weighted improvements in both objectives to guide local moves while ensuring feasibility through careful weight checks. The neighborhood size adapts to the remaining capacity, balancing exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto-ranked selection with crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search with adaptive operators\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Adaptive subset flips with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Objective-weighted swaps if no improvement\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThis algorithm combines adaptive Pareto selection with a novel \"objective-aligned cluster flip\" operator, where solutions are first filtered for Pareto efficiency, then selected based on crowding distance and objective balance, followed by dynamic cluster flips prioritizing items that proportionally improve both objectives, with neighborhood size and flip probability adapting to weight utilization. The method ensures feasibility through precomputed contributions and feasibility checks, with a fallback to random flips when no improvements are found.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Filter Pareto-efficient solutions\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distance and objective alignment\n            weighted_scores = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min(obj[0] for _, obj in pareto_front) + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                alignment = min(obj[0], obj[1]) / (max(obj[0], obj[1]) + 1e-6) if (obj[0] + obj[1]) > 0 else 0.5\n                weighted_score = 0.6 * (dist_v1 + dist_v2) + 0.4 * alignment\n                weighted_scores.append(weighted_score)\n\n            # Select solution with highest weighted score\n            max_score_idx = np.argmax(weighted_scores)\n            base_solution = pareto_front[max_score_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic cluster size and flip probability\n    weight_util = current_weight / capacity\n    cluster_size = max(1, int(len(new_solution) * (0.3 - 0.15 * weight_util)))\n    flip_prob = 0.5 + 0.4 * (1 - weight_util)\n\n    # Objective-aligned cluster flip\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        if np.random.rand() < flip_prob:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-aligned improvement calculation\n            alignment = min(current_v1 + delta_v1, current_v2 + delta_v2) / (max(current_v1 + delta_v1, current_v2 + delta_v2) + 1e-6) if (current_v1 + delta_v1 + current_v2 + delta_v2) > 0 else 0.5\n            improvement = (delta_v1 + delta_v2) * alignment\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Fallback: random flip with feasibility check\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = np.random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThis heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nThe algorithm first identifies the Pareto front from the archive and selects a base solution using a weighted crowding-distance metric to prioritize high-diversity solutions. It then applies a hybrid local search that dynamically adjusts the neighborhood size based on the solution's weight utilization and objective weights, evaluating flips within the neighborhood while ensuring feasibility. If no improving move is found, it performs a probabilistic flip based on potential weighted contributions, dynamically adjusting objective weights to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Identify Pareto front\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            min_v1 = min(obj[0] for _, obj in pareto_front)\n            min_v2 = min(obj[1] for _, obj in pareto_front)\n\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min_v1 + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min_v2 + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design for the BI-KP, optimized for high-quality, feasible neighbors:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 74,
        "algorithm": "This algorithm selects a solution from the archive using a Pareto-front-based weighted crowding distance metric, then applies a hybrid local search combining adaptive subset flips and objective-weighted swaps, dynamically adjusting neighborhood size and flip probabilities based on solution quality and remaining capacity to generate high-quality, feasible neighbors while prioritizing diversity in value2 (0.4 weight) over value1 (0.6 weight). The method balances exploration and exploitation through dynamic objective weights that adapt to the current solution's value distribution.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto-aware selection with weighted crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if len(pareto_front) > 1:\n        # Calculate weighted crowding distances\n        weighted_distances = []\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        min_v1 = min(obj[0] for _, obj in pareto_front)\n        min_v2 = min(obj[1] for _, obj in pareto_front)\n\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n            dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min_v1 + 1e-6)\n            dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min_v2 + 1e-6)\n            weighted_dist = 0.6 * dist_v1 + 0.4 * dist_v2  # Prioritize diversity in value2\n            weighted_distances.append(weighted_dist)\n\n        # Select solution with highest weighted crowding distance\n        max_dist_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[max_dist_idx][0].copy()\n    else:\n        base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size (20-50% of items) based on remaining capacity\n    total_items = len(new_solution)\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (remaining_capacity / capacity))))\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = 0.4 * (total_v2 / (total_v1 + total_v2 + 1e-6)) if (total_v1 + total_v2) > 0 else 0.4\n    weight_v2 = 0.6 * (total_v1 / (total_v1 + total_v2 + 1e-6)) if (total_v1 + total_v2) > 0 else 0.6\n\n    # Hybrid local search: adaptive subset flips and objective-weighted swaps\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Adaptive subset flips\n    flip_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n    for idx in flip_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Objective-weighted swaps if no improving flips found\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            # Sort items by weighted contribution\n            out_contributions = []\n            in_contributions = []\n\n            for idx in out_items:\n                contribution = weight_v1 * value1_lst[idx] + weight_v2 * value2_lst[idx]\n                out_contributions.append((contribution, idx))\n\n            for idx in in_items:\n                contribution = weight_v1 * value1_lst[idx] + weight_v2 * value2_lst[idx]\n                in_contributions.append((contribution, idx))\n\n            out_contributions.sort(reverse=True)\n            in_contributions.sort(reverse=True)\n\n            # Try top 3 highest-contribution swaps\n            for i in range(min(3, len(out_contributions), len(in_contributions))):\n                out_idx = out_contributions[i][1]\n                in_idx = in_contributions[i][1]\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n                    if improvement > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        break\n\n    return new_solution\n\n",
        "score": [
            -0.7493968023217321,
            0.49805372953414917
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto-aware selection with weighted crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if len(pareto_front) > 1:\n        # Calculate weighted crowding distances\n        weighted_distances = []\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        min_v1 = min(obj[0] for _, obj in pareto_front)\n        min_v2 = min(obj[1] for _, obj in pareto_front)\n\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n            dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min_v1 + 1e-6)\n            dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min_v2 + 1e-6)\n            weighted_dist = 0.6 * dist_v1 + 0.4 * dist_v2  # Prioritize diversity in value2\n            weighted_distances.append(weighted_dist)\n\n        # Select solution with highest weighted crowding distance\n        max_dist_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[max_dist_idx][0].copy()\n    else:\n        base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size (20-50% of items) based on remaining capacity\n    total_items = len(new_solution)\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (remaining_capacity / capacity))))\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = 0.4 * (total_v2 / (total_v1 + total_v2 + 1e-6)) if (total_v1 + total_v2) > 0 else 0.4\n    weight_v2 = 0.6 * (total_v1 / (total_v1 + total_v2 + 1e-6)) if (total_v1 + total_v2) > 0 else 0.6\n\n    # Hybrid local search: adaptive subset flips and objective-weighted swaps\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Adaptive subset flips\n    flip_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n    for idx in flip_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Objective-weighted swaps if no improving flips found\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            # Sort items by weighted contribution\n            out_contributions = []\n            in_contributions = []\n\n            for idx in out_items:\n                contribution = weight_v1 * value1_lst[idx] + weight_v2 * value2_lst[idx]\n                out_contributions.append((contribution, idx))\n\n            for idx in in_items:\n                contribution = weight_v1 * value1_lst[idx] + weight_v2 * value2_lst[idx]\n                in_contributions.append((contribution, idx))\n\n            out_contributions.sort(reverse=True)\n            in_contributions.sort(reverse=True)\n\n            # Try top 3 highest-contribution swaps\n            for i in range(min(3, len(out_contributions), len(in_contributions))):\n                out_idx = out_contributions[i][1]\n                in_idx = in_contributions[i][1]\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n                    if improvement > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        break\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines Pareto front clustering with dynamic objective-weighted cluster flips, prioritizing solutions based on crowding distance and hypervolume contributions while adaptively adjusting flip probabilities and cluster sizes to balance exploration and exploitation, using momentum to emphasize high-potential regions of the search space. Key design features include weighted improvement calculations, adaptive cluster sizing, and probabilistic flips with temperature-based selection to ensure feasible neighbor solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster solutions based on Pareto dominance and crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest combined crowding distance and hypervolume contribution\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster flip with momentum\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization and solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.3 + 0.2 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.9  # Higher momentum for more aggressive search\n\n    # Evaluate cluster flips with momentum\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design for the BI-KP, optimized for high-quality, feasible neighbors:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 75,
        "algorithm": "The algorithm selects a promising solution from the archive based on hypervolume contribution and crowding distance, then generates a neighbor by adaptively flipping a subset of items weighted by their value-to-weight ratios, ensuring feasibility through precomputed constraints. The subset size and flip priorities dynamically adjust based on the solution's quality and remaining capacity.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution based on hypervolume contribution and crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    max_v1 = np.max(objectives[:, 0])\n    max_v2 = np.max(objectives[:, 1])\n    contributions = []\n    for i, (sol, obj) in enumerate(archive):\n        left = archive[i-1][1] if i > 0 else (0, 0)\n        right = archive[i+1][1] if i < len(archive)-1 else (max_v1, max_v2)\n        contribution = (right[0] - left[0]) * (right[1] - left[1])\n        contributions.append(contribution)\n\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    combined_scores = np.array(contributions) + crowding_distances\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Adaptive subset flip based on value-to-weight ratios\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    subset_size = max(1, int(len(new_solution) * (0.2 + 0.3 * solution_quality) * (1 - current_weight / capacity)))\n\n    # Precompute feasible flips\n    valid_indices = []\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity:\n            valid_indices.append(i)\n        elif new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n            valid_indices.append(i)\n\n    if not valid_indices:\n        return new_solution\n\n    # Weighted selection of flip candidates\n    weights = []\n    for i in valid_indices:\n        if new_solution[i] == 1:\n            weights.append((value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-6))\n        else:\n            weights.append((value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-6))\n\n    weights = np.array(weights)\n    weights = weights / np.sum(weights)\n    flip_indices = np.random.choice(valid_indices, size=min(subset_size, len(valid_indices)), replace=False, p=weights)\n\n    for i in flip_indices:\n        new_solution[i] = 1 - new_solution[i]\n\n    return new_solution\n\n",
        "score": [
            -0.9252825761622552,
            1.3603337407112122
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution based on hypervolume contribution and crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    max_v1 = np.max(objectives[:, 0])\n    max_v2 = np.max(objectives[:, 1])\n    contributions = []\n    for i, (sol, obj) in enumerate(archive):\n        left = archive[i-1][1] if i > 0 else (0, 0)\n        right = archive[i+1][1] if i < len(archive)-1 else (max_v1, max_v2)\n        contribution = (right[0] - left[0]) * (right[1] - left[1])\n        contributions.append(contribution)\n\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    combined_scores = np.array(contributions) + crowding_distances\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Adaptive subset flip based on value-to-weight ratios\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    subset_size = max(1, int(len(new_solution) * (0.2 + 0.3 * solution_quality) * (1 - current_weight / capacity)))\n\n    # Precompute feasible flips\n    valid_indices = []\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity:\n            valid_indices.append(i)\n        elif new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n            valid_indices.append(i)\n\n    if not valid_indices:\n        return new_solution\n\n    # Weighted selection of flip candidates\n    weights = []\n    for i in valid_indices:\n        if new_solution[i] == 1:\n            weights.append((value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-6))\n        else:\n            weights.append((value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-6))\n\n    weights = np.array(weights)\n    weights = weights / np.sum(weights)\n    flip_indices = np.random.choice(valid_indices, size=min(subset_size, len(valid_indices)), replace=False, p=weights)\n\n    for i in flip_indices:\n        new_solution[i] = 1 - new_solution[i]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines Pareto front clustering with dynamic objective-weighted cluster flips, prioritizing solutions based on crowding distance and hypervolume contributions while adaptively adjusting flip probabilities and cluster sizes to balance exploration and exploitation, using momentum to emphasize high-potential regions of the search space. Key design features include weighted improvement calculations, adaptive cluster sizing, and probabilistic flips with temperature-based selection to ensure feasible neighbor solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster solutions based on Pareto dominance and crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest combined crowding distance and hypervolume contribution\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster flip with momentum\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization and solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.3 + 0.2 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.9  # Higher momentum for more aggressive search\n\n    # Evaluate cluster flips with momentum\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design for the BI-KP, optimized for high-quality, feasible neighbors:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 76,
        "algorithm": "The algorithm selects a promising solution from the archive based on combined crowding distance and hypervolume contribution, then generates a neighbor by adaptively expanding high-value clusters while ensuring feasibility through weighted flips, with neighborhood size dynamically adjusted based on solution quality and weight utilization. It prioritizes items with high value-to-weight ratios for inclusion/exclusion, balancing exploration and exploitation through momentum-driven flips.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with highest combined crowding distance and hypervolume contribution\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    max_v1 = max(obj[0] for _, obj in archive)\n    max_v2 = max(obj[1] for _, obj in archive)\n    contributions = []\n    for i, (sol, obj) in enumerate(archive):\n        left = archive[i-1][1] if i > 0 else (0, 0)\n        right = archive[i+1][1] if i < len(archive)-1 else (max_v1, max_v2)\n        contribution = (right[0] - left[0]) * (right[1] - left[1])\n        contributions.append(contribution)\n\n    combined_scores = crowding_distances + np.array(contributions)\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Adaptive neighborhood expansion\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Calculate adaptive neighborhood size\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    neighborhood_size = max(1, int(len(new_solution) * (0.2 + 0.3 * solution_quality) * (1 - current_weight / capacity)))\n\n    # Step 3: Expand high-potential clusters\n    in_items = np.where(new_solution == 0)[0]\n    out_items = np.where(new_solution == 1)[0]\n\n    if len(in_items) > 0 and len(out_items) > 0:\n        # Prioritize items with high value-to-weight ratios\n        in_ratios = (value1_lst[in_items] + value2_lst[in_items]) / weight_lst[in_items]\n        out_ratios = (value1_lst[out_items] + value2_lst[out_items]) / weight_lst[out_items]\n\n        # Select top items to flip\n        in_top = in_items[np.argsort(in_ratios)[-min(neighborhood_size, len(in_items)):]]\n        out_top = out_items[np.argsort(out_ratios)[:min(neighborhood_size, len(out_items))]]\n\n        # Perform flips\n        for in_idx in in_top:\n            new_weight = current_weight + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n\n        for out_idx in out_top:\n            new_weight = current_weight - weight_lst[out_idx]\n            if new_weight >= 0:\n                new_solution[out_idx] = 0\n                current_weight = new_weight\n\n    return new_solution\n\n",
        "score": [
            -0.8321134294976401,
            0.6715418696403503
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select solution with highest combined crowding distance and hypervolume contribution\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    max_v1 = max(obj[0] for _, obj in archive)\n    max_v2 = max(obj[1] for _, obj in archive)\n    contributions = []\n    for i, (sol, obj) in enumerate(archive):\n        left = archive[i-1][1] if i > 0 else (0, 0)\n        right = archive[i+1][1] if i < len(archive)-1 else (max_v1, max_v2)\n        contribution = (right[0] - left[0]) * (right[1] - left[1])\n        contributions.append(contribution)\n\n    combined_scores = crowding_distances + np.array(contributions)\n    selected_idx = np.argmax(combined_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    # Step 2: Adaptive neighborhood expansion\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Calculate adaptive neighborhood size\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    neighborhood_size = max(1, int(len(new_solution) * (0.2 + 0.3 * solution_quality) * (1 - current_weight / capacity)))\n\n    # Step 3: Expand high-potential clusters\n    in_items = np.where(new_solution == 0)[0]\n    out_items = np.where(new_solution == 1)[0]\n\n    if len(in_items) > 0 and len(out_items) > 0:\n        # Prioritize items with high value-to-weight ratios\n        in_ratios = (value1_lst[in_items] + value2_lst[in_items]) / weight_lst[in_items]\n        out_ratios = (value1_lst[out_items] + value2_lst[out_items]) / weight_lst[out_items]\n\n        # Select top items to flip\n        in_top = in_items[np.argsort(in_ratios)[-min(neighborhood_size, len(in_items)):]]\n        out_top = out_items[np.argsort(out_ratios)[:min(neighborhood_size, len(out_items))]]\n\n        # Perform flips\n        for in_idx in in_top:\n            new_weight = current_weight + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n\n        for out_idx in out_top:\n            new_weight = current_weight - weight_lst[out_idx]\n            if new_weight >= 0:\n                new_solution[out_idx] = 0\n                current_weight = new_weight\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines Pareto front clustering with dynamic objective-weighted cluster flips, prioritizing solutions based on crowding distance and hypervolume contributions while adaptively adjusting flip probabilities and cluster sizes to balance exploration and exploitation, using momentum to emphasize high-potential regions of the search space. Key design features include weighted improvement calculations, adaptive cluster sizing, and probabilistic flips with temperature-based selection to ensure feasible neighbor solutions.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster solutions based on Pareto dominance and crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest combined crowding distance and hypervolume contribution\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster flip with momentum\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization and solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.3 + 0.2 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.9  # Higher momentum for more aggressive search\n\n    # Evaluate cluster flips with momentum\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 77,
        "algorithm": "This algorithm enhances the provided approach by incorporating adaptive objective weighting, dynamic cluster flipping with momentum, and probabilistic selection based on solution quality and weight utilization, while ensuring feasibility through constrained optimization and temperature-based exploration. The algorithm prioritizes solutions with higher combined normalized objective values, dynamically adjusts cluster sizes based on weight utilization, and uses weighted improvements with momentum to guide local search, falling back to probabilistic flips when no improving moves are found.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Adaptive objective weighting based on solution quality\n    objectives = np.array([obj for _, obj in archive])\n    max_v1, max_v2 = np.max(objectives, axis=0)\n\n    # Calculate solution quality scores\n    quality_scores = []\n    for _, obj in archive:\n        normalized_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        normalized_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        quality_scores.append(0.5 * normalized_v1 + 0.5 * normalized_v2)\n\n    # Select solution with highest quality score\n    selected_idx = np.argmax(quality_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Dynamic cluster flipping with adaptive weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization\n    weight_utilization = current_weight / capacity\n    cluster_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - weight_utilization))))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.7  # Moderate momentum for balanced search\n\n    # Evaluate cluster flips with weighted improvement\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = (weight_v1 * delta_v1 + weight_v2 * delta_v2) * (1 + momentum * (1 - weight_utilization))\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 0.5 + 0.5 * weight_utilization\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8018584254885521,
            0.6183541417121887
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Adaptive objective weighting based on solution quality\n    objectives = np.array([obj for _, obj in archive])\n    max_v1, max_v2 = np.max(objectives, axis=0)\n\n    # Calculate solution quality scores\n    quality_scores = []\n    for _, obj in archive:\n        normalized_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n        normalized_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n        quality_scores.append(0.5 * normalized_v1 + 0.5 * normalized_v2)\n\n    # Select solution with highest quality score\n    selected_idx = np.argmax(quality_scores)\n    base_solution = archive[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Dynamic cluster flipping with adaptive weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization\n    weight_utilization = current_weight / capacity\n    cluster_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - weight_utilization))))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.7  # Moderate momentum for balanced search\n\n    # Evaluate cluster flips with weighted improvement\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = (weight_v1 * delta_v1 + weight_v2 * delta_v2) * (1 + momentum * (1 - weight_utilization))\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 0.5 + 0.5 * weight_utilization\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines Pareto front clustering with dynamic objective-weighted cluster flips, prioritizing solutions based on crowding distance and hypervolume contributions while adaptively adjusting flip probabilities and cluster sizes to balance exploration and exploitation, using momentum to emphasize high-potential regions of the search space. Key design features include weighted improvement calculations, adaptive cluster sizing, and probabilistic flips with temperature-based selection to ensure feasible neighbor solutions.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster solutions based on Pareto dominance and crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest combined crowding distance and hypervolume contribution\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster flip with momentum\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization and solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.3 + 0.2 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.9  # Higher momentum for more aggressive search\n\n    # Evaluate cluster flips with momentum\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 78,
        "algorithm": "The algorithm combines adaptive Pareto dominance filtering with dynamic cluster-based local search, prioritizing solutions based on hypervolume contribution and diversity while using temperature-based selection to balance exploration and exploitation. It dynamically adjusts cluster sizes and flip probabilities based on solution quality and weight utilization, ensuring feasible neighbors through probabilistic validation. The momentum parameter (0.7) and temperature (1-solution_quality) control the trade-off between objective-weighted improvements and overall value maximization.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Adaptive Pareto filtering with diversity consideration\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest hypervolume contribution and diversity\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        diversity_scores = []\n\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n            # Calculate diversity as distance to nearest neighbor\n            distances = [np.sqrt((obj[0]-other_obj[0])**2 + (obj[1]-other_obj[1])**2)\n                        for _, other_obj in pareto_front if other_obj != obj]\n            diversity_scores.append(min(distances) if distances else 1.0)\n\n        combined_scores = np.array(contributions) * np.array(diversity_scores)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster flip with temperature-based selection\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization and diversity\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.2 + 0.3 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.7  # Lower momentum for more balanced search\n\n    # Evaluate cluster flips with temperature-based selection\n    temperature = 1 - solution_quality\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        weighted_improvement = (weight_v1 * delta_v1 + weight_v2 * delta_v2)\n        improvement = momentum * weighted_improvement + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8341909030825101,
            0.647712767124176
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Adaptive Pareto filtering with diversity consideration\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest hypervolume contribution and diversity\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        diversity_scores = []\n\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n            # Calculate diversity as distance to nearest neighbor\n            distances = [np.sqrt((obj[0]-other_obj[0])**2 + (obj[1]-other_obj[1])**2)\n                        for _, other_obj in pareto_front if other_obj != obj]\n            diversity_scores.append(min(distances) if distances else 1.0)\n\n        combined_scores = np.array(contributions) * np.array(diversity_scores)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster flip with temperature-based selection\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization and diversity\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.2 + 0.3 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.7  # Lower momentum for more balanced search\n\n    # Evaluate cluster flips with temperature-based selection\n    temperature = 1 - solution_quality\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        weighted_improvement = (weight_v1 * delta_v1 + weight_v2 * delta_v2)\n        improvement = momentum * weighted_improvement + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 5 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto front clustering with dynamic objective-weighted cluster flips, prioritizing solutions based on crowding distance and hypervolume contributions while adaptively adjusting flip probabilities and cluster sizes to balance exploration and exploitation, using momentum to emphasize high-potential regions of the search space. Key design features include weighted improvement calculations, adaptive cluster sizing, and probabilistic flips with temperature-based selection to ensure feasible neighbor solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster solutions based on Pareto dominance and crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest combined crowding distance and hypervolume contribution\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster flip with momentum\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization and solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.3 + 0.2 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.9  # Higher momentum for more aggressive search\n\n    # Evaluate cluster flips with momentum\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThis algorithm combines Pareto dominance filtering with adaptive neighborhood diversification to generate high-quality neighbors. It selects solutions based on crowding distance from the Pareto front and uses dynamic cluster flips prioritized by weight-balanced item exchanges, adjusting flip sizes based on objective trade-offs and weight utilization, while falling back to greedy random flips if no improvement is found. The algorithm ensures feasibility by precomputing weight contributions and dynamically adjusting cluster sizes to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n        selected_idx = np.argmax(crowding_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Adaptive neighborhood diversification\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Dynamic cluster size based on weight utilization\n    cluster_size = max(1, int(len(new_solution) * 0.2 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Evaluate cluster flips\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy random flip if no improvement\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto ranking with crowding-distance selection to prioritize diverse, high-quality solutions from the archive, then applies an adaptive cluster-based local search that dynamically adjusts flip cluster sizes based on weight utilization and objective weights, ensuring feasibility through precomputed item contributions and move validation. It balances exploration/exploitation by flipping items within clusters while favoring moves that improve both objectives proportionally, falling back to random valid flips if no improvement is found.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max(obj[0] for _, obj in pareto_front) - min(obj[0] for _, obj in pareto_front) + 1e-6) + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max(obj[1] for _, obj in pareto_front) - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                crowding_distances.append(dist)\n\n            # Select solution with highest crowding distance\n            max_dist_idx = np.argmax(crowding_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive cluster-based item flips\n    cluster_size = max(1, int(len(new_solution) * 0.1 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design for the BI-KP, optimized for high-quality, feasible neighbors:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 79,
        "algorithm": "This algorithm combines Pareto dominance filtering with adaptive weighted crowding-distance selection to prioritize high-quality, diverse solutions, then applies a hybrid local search that dynamically adjusts flip cluster sizes based on weight utilization and objective trade-offs, evaluating flips with weighted improvements and falling back to probabilistic flips when no improvement is found. It ensures feasibility by precomputing item contributions and dynamically balances exploration/exploitation through adaptive neighborhood sizes and objective-weighted improvements. The selection prioritizes solutions with better objective trade-offs (weighted by normalized value sums), while the local search dynamically adjusts cluster sizes based on solution quality and weight utilization.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions and select based on weighted crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weighted crowding distance (prioritize solutions with better objective trade-offs)\n        total_v1 = np.sum(value1_lst)\n        total_v2 = np.sum(value2_lst)\n        weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n        weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n        weighted_scores = crowding_distances * (weight_v1 * objectives[:, 0] + weight_v2 * objectives[:, 1])\n        selected_idx = np.argmax(weighted_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Adaptive cluster-based local search\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Dynamic cluster size (adjust based on weight utilization and solution quality)\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.2 + 0.3 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Evaluate cluster flips with weighted improvements\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on contribution-weighted probabilities\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.7912525733658806,
            0.692170262336731
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions and select based on weighted crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weighted crowding distance (prioritize solutions with better objective trade-offs)\n        total_v1 = np.sum(value1_lst)\n        total_v2 = np.sum(value2_lst)\n        weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n        weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n        weighted_scores = crowding_distances * (weight_v1 * objectives[:, 0] + weight_v2 * objectives[:, 1])\n        selected_idx = np.argmax(weighted_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Adaptive cluster-based local search\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Dynamic cluster size (adjust based on weight utilization and solution quality)\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.2 + 0.3 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Evaluate cluster flips with weighted improvements\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on contribution-weighted probabilities\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 5 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThis heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm first identifies the Pareto front from the archive and selects a base solution using a weighted crowding-distance metric to prioritize high-diversity solutions. It then applies a hybrid local search that dynamically adjusts the neighborhood size based on the solution's weight utilization and objective weights, evaluating flips within the neighborhood while ensuring feasibility. If no improving move is found, it performs a probabilistic flip based on potential weighted contributions, dynamically adjusting objective weights to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Identify Pareto front\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            min_v1 = min(obj[0] for _, obj in pareto_front)\n            min_v2 = min(obj[1] for _, obj in pareto_front)\n\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min_v1 + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min_v2 + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto front clustering with dynamic objective-weighted cluster flips, prioritizing solutions based on crowding distance and hypervolume contributions while adaptively adjusting flip probabilities and cluster sizes to balance exploration and exploitation, using momentum to emphasize high-potential regions of the search space. Key design features include weighted improvement calculations, adaptive cluster sizing, and probabilistic flips with temperature-based selection to ensure feasible neighbor solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster solutions based on Pareto dominance and crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest combined crowding distance and hypervolume contribution\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster flip with momentum\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization and solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.3 + 0.2 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.9  # Higher momentum for more aggressive search\n\n    # Evaluate cluster flips with momentum\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm first filters non-dominated solutions from the archive and selects a base solution using a weighted crowding-distance metric that balances Pareto dominance and diversity. It then applies a hybrid local search with dynamic neighborhood sizes (20-50% of items) based on weight utilization, using objective-weighted improvement criteria to prioritize flips that enhance both objectives proportionally. If no improvement is found, it performs a probabilistic flip based on potential contributions while ensuring feasibility. The method dynamically adjusts objective weights to balance exploration and exploitation, generating high-quality neighbors that dominate or complement existing solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with weighted crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic neighborhood size based on weight utilization\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 4: Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 5: Hybrid local search with cluster flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering both objectives\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: Here\u2019s the refined heuristic design for the BI-KP, optimized for high-quality, feasible neighbors:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 80,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a base solution using weighted crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Adaptive neighborhood size (20-50% of items)\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 3: Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 4: Hybrid local search with subset flips and objective-weighted improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering both objectives\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8590842426083138,
            0.47211188077926636
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select a base solution using weighted crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Adaptive neighborhood size (20-50% of items)\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 3: Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 4: Hybrid local search with subset flips and objective-weighted improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering both objectives\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects the most promising solution from the archive (based on average value-to-weight density) and applies a hybrid local search strategy that prioritizes swapping low-value items for high-value ones while ensuring feasibility through adaptive perturbations. If no swaps are feasible, it randomly removes items to free up capacity. The approach balances exploration (via adaptive swaps) and exploitation (via value-to-weight ratios) to improve both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution from the archive\n    # Calculate the density (value/weight) for each objective\n    densities = []\n    for sol, (v1, v2) in archive:\n        included = sol == 1\n        total_weight = np.sum(weight_lst[included])\n        if total_weight == 0:\n            density1 = density2 = 0\n        else:\n            density1 = np.sum(value1_lst[included]) / total_weight\n            density2 = np.sum(value2_lst[included]) / total_weight\n        densities.append((density1 + density2) / 2)  # Average density\n\n    # Select the solution with the highest density (most promising for improvement)\n    best_idx = np.argmax(densities)\n    base_solution = archive[best_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Apply novel local search operator\n    # Hybrid strategy: adaptive item swaps and perturbations\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Adaptive swap: replace a low-value item with a high-value item\n        # Calculate value-to-weight ratio for included items\n        included_ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_ratios)]\n\n        # Calculate value-to-weight ratio for excluded items\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        # Check if swap is feasible\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n        else:\n            # If swap is not feasible, perform a perturbation\n            # Randomly flip a small number of items to find a feasible solution\n            max_perturbations = min(3, len(included_items))\n            for _ in range(max_perturbations):\n                candidate = random.choice(included_items)\n                if current_weight - weight_lst[candidate] <= capacity:\n                    new_solution[candidate] = 0\n                    current_weight -= weight_lst[candidate]\n                    break\n\n    # If no items are included, add the best item if possible\n    elif len(included_items) == 0 and len(excluded_items) > 0:\n        # Add the item with the highest value-to-weight ratio\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        if weight_lst[best_excluded] <= capacity:\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm first filters non-dominated solutions from the archive and selects a base solution using a weighted crowding-distance metric that balances Pareto dominance and diversity. It then applies a hybrid local search with dynamic neighborhood sizes (20-50% of items) based on weight utilization, using objective-weighted improvement criteria to prioritize flips that enhance both objectives proportionally. If no improvement is found, it performs a probabilistic flip based on potential contributions while ensuring feasibility. The method dynamically adjusts objective weights to balance exploration and exploitation, generating high-quality neighbors that dominate or complement existing solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with weighted crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic neighborhood size based on weight utilization\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 4: Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 5: Hybrid local search with cluster flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering both objectives\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design for the BI-KP, optimized for high-quality, feasible neighbors:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 81,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high potential for multi-objective improvement\n    # Calculate the normalized dominance score for each solution\n    dominance_scores = []\n    for sol, (v1, v2) in archive:\n        dominated = 0\n        for _, (other_v1, other_v2) in archive:\n            if (other_v1 >= v1 and other_v2 >= v2) and (other_v1 > v1 or other_v2 > v2):\n                dominated += 1\n        dominance_scores.append(dominated)\n\n    # Select the solution with the lowest dominance (most non-dominated)\n    selected_idx = np.argmin(dominance_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Apply cluster-based local search with adaptive objective weighting\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Calculate current weight and objective weights\n    current_weight = np.sum(weight_lst[included])\n    total_v1 = np.sum(value1_lst[included])\n    total_v2 = np.sum(value2_lst[included])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 3: Dynamic cluster flipping with feasibility checks\n    cluster_size = max(1, min(10, len(included) // 2))\n    if len(included) > 0:\n        # Select a cluster of items to flip\n        cluster_indices = np.random.choice(included, size=cluster_size, replace=False)\n        potential_weight = current_weight - np.sum(weight_lst[cluster_indices])\n\n        # Find best items to add from excluded\n        if len(excluded) > 0:\n            excluded_ratios = (weight_v1 * value1_lst[excluded] + weight_v2 * value2_lst[excluded]) / weight_lst[excluded]\n            best_excluded = excluded[np.argsort(excluded_ratios)[-min(3, len(excluded)):]]\n\n            for item in best_excluded:\n                if potential_weight + weight_lst[item] <= capacity:\n                    new_solution[cluster_indices[0]] = 0\n                    new_solution[item] = 1\n                    break\n\n    # Step 4: If no improvement found, perform probabilistic cluster flip\n    if np.array_equal(new_solution, base_solution):\n        if len(included) > 0:\n            # Randomly select a cluster to flip\n            cluster_indices = np.random.choice(included, size=cluster_size, replace=False)\n            for idx in cluster_indices:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.4196592066165787,
            8.368073016405106
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a solution with high potential for multi-objective improvement\n    # Calculate the normalized dominance score for each solution\n    dominance_scores = []\n    for sol, (v1, v2) in archive:\n        dominated = 0\n        for _, (other_v1, other_v2) in archive:\n            if (other_v1 >= v1 and other_v2 >= v2) and (other_v1 > v1 or other_v2 > v2):\n                dominated += 1\n        dominance_scores.append(dominated)\n\n    # Select the solution with the lowest dominance (most non-dominated)\n    selected_idx = np.argmin(dominance_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Apply cluster-based local search with adaptive objective weighting\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n\n    # Calculate current weight and objective weights\n    current_weight = np.sum(weight_lst[included])\n    total_v1 = np.sum(value1_lst[included])\n    total_v2 = np.sum(value2_lst[included])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 3: Dynamic cluster flipping with feasibility checks\n    cluster_size = max(1, min(10, len(included) // 2))\n    if len(included) > 0:\n        # Select a cluster of items to flip\n        cluster_indices = np.random.choice(included, size=cluster_size, replace=False)\n        potential_weight = current_weight - np.sum(weight_lst[cluster_indices])\n\n        # Find best items to add from excluded\n        if len(excluded) > 0:\n            excluded_ratios = (weight_v1 * value1_lst[excluded] + weight_v2 * value2_lst[excluded]) / weight_lst[excluded]\n            best_excluded = excluded[np.argsort(excluded_ratios)[-min(3, len(excluded)):]]\n\n            for item in best_excluded:\n                if potential_weight + weight_lst[item] <= capacity:\n                    new_solution[cluster_indices[0]] = 0\n                    new_solution[item] = 1\n                    break\n\n    # Step 4: If no improvement found, perform probabilistic cluster flip\n    if np.array_equal(new_solution, base_solution):\n        if len(included) > 0:\n            # Randomly select a cluster to flip\n            cluster_indices = np.random.choice(included, size=cluster_size, replace=False)\n            for idx in cluster_indices:\n                if current_weight - weight_lst[idx] <= capacity:\n                    new_solution[idx] = 0\n                    current_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm first filters non-dominated solutions from the archive and selects a base solution using a weighted crowding-distance metric that balances Pareto dominance and diversity. It then applies a hybrid local search with dynamic neighborhood sizes (20-50% of items) based on weight utilization, using objective-weighted improvement criteria to prioritize flips that enhance both objectives proportionally. If no improvement is found, it performs a probabilistic flip based on potential contributions while ensuring feasibility. The method dynamically adjusts objective weights to balance exploration and exploitation, generating high-quality neighbors that dominate or complement existing solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with weighted crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic neighborhood size based on weight utilization\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 4: Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 5: Hybrid local search with cluster flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering both objectives\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design for the BI-KP, optimized for high-quality, feasible neighbors:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 82,
        "algorithm": "The algorithm combines **Pareto-frontier selection** with **adaptive cluster flipping**, prioritizing solutions with high crowding distance for exploration while dynamically adjusting neighborhood size and flip probabilities based on weight utilization and objective contributions. It ensures feasibility by precomputing weight constraints and employs a hybrid local search that balances greedy improvements with probabilistic flips to escape local optima.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter Pareto-optimal solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n        selected_idx = np.argmax(crowding_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic neighborhood size based on weight utilization\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.1 + 0.4 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 4: Calculate objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 5: Hybrid local search with cluster flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8723282347372585,
            0.6781617105007172
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter Pareto-optimal solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n        selected_idx = np.argmax(crowding_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic neighborhood size based on weight utilization\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.1 + 0.4 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 4: Calculate objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 5: Hybrid local search with cluster flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm first filters non-dominated solutions from the archive and selects a base solution using a weighted crowding-distance metric that balances Pareto dominance and diversity. It then applies a hybrid local search with dynamic neighborhood sizes (20-50% of items) based on weight utilization, using objective-weighted improvement criteria to prioritize flips that enhance both objectives proportionally. If no improvement is found, it performs a probabilistic flip based on potential contributions while ensuring feasibility. The method dynamically adjusts objective weights to balance exploration and exploitation, generating high-quality neighbors that dominate or complement existing solutions.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with weighted crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic neighborhood size based on weight utilization\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 4: Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 5: Hybrid local search with cluster flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering both objectives\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 83,
        "algorithm": "The algorithm combines Pareto dominance, solution quality, and diversity to select a promising base solution from the archive, then applies a dynamic neighborhood search that adaptively adjusts flip probabilities based on marginal contributions to both objectives while ensuring feasibility. It prioritizes high-quality solutions (40% weight) and diversity (20% weight), with Pareto rank contributing 40%. The neighborhood size and flip probabilities are weight-aware, favoring items that improve both objectives proportionally. If no improvement is found, it falls back to a random feasible flip.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hybrid solution selection\n    solutions = [sol for sol, _ in archive]\n    objectives = np.array([obj for _, obj in archive])\n    weights = np.array([np.sum(weight_lst[sol == 1]) for sol in solutions])\n\n    # Calculate Pareto ranks\n    pareto_ranks = np.zeros(len(solutions))\n    for i in range(len(solutions)):\n        for j in range(len(solutions)):\n            if (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and\n                (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1])):\n                pareto_ranks[i] += 1\n\n    # Calculate solution quality scores\n    quality_scores = (objectives[:, 0] + objectives[:, 1]) / (np.max(objectives[:, 0] + objectives[:, 1]) + 1e-6)\n\n    # Calculate diversity scores\n    diversity_scores = np.zeros(len(solutions))\n    for i in range(len(solutions)):\n        distances = np.sum(np.abs(solutions[i] - solutions), axis=1)\n        diversity_scores[i] = np.mean(distances)\n\n    # Combine metrics for selection\n    selection_scores = (1 - pareto_ranks/np.max(pareto_ranks)) * 0.4 + quality_scores * 0.4 + diversity_scores/np.max(diversity_scores) * 0.2\n    selected_idx = np.argmax(selection_scores)\n    base_solution = solutions[selected_idx].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = weights[selected_idx]\n\n    # Step 2: Dynamic neighborhood search\n    weight_ratio = current_weight / capacity\n    neighborhood_size = max(1, int(len(new_solution) * (0.3 + 0.7 * (1 - weight_ratio))))\n\n    # Calculate marginal contributions\n    marginal_v1 = np.where(new_solution == 1, -value1_lst, value1_lst)\n    marginal_v2 = np.where(new_solution == 1, -value2_lst, value2_lst)\n    marginal_weights = np.where(new_solution == 1, -weight_lst, weight_lst)\n\n    # Calculate potential objective improvements\n    potential_v1 = objectives[selected_idx, 0] + marginal_v1\n    potential_v2 = objectives[selected_idx, 1] + marginal_v2\n    potential_weights = current_weight + marginal_weights\n\n    # Calculate improvement scores\n    improvement_scores = np.zeros(len(new_solution))\n    for i in range(len(new_solution)):\n        if potential_weights[i] <= capacity:\n            improvement_scores[i] = (potential_v1[i] - objectives[selected_idx, 0]) * (potential_v2[i] - objectives[selected_idx, 1]) / (np.abs(value1_lst[i] * value2_lst[i]) + 1e-6)\n\n    # Select neighborhood candidates\n    candidate_indices = np.argsort(improvement_scores)[-neighborhood_size:]\n\n    # Step 3: Probabilistic flip selection\n    valid_indices = [i for i in candidate_indices if\n                    (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                    (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n\n    if valid_indices:\n        # Calculate flip probabilities\n        contributions = improvement_scores[valid_indices]\n        if np.sum(contributions) > 0:\n            probabilities = contributions / np.sum(contributions)\n        else:\n            probabilities = np.ones(len(valid_indices)) / len(valid_indices)\n\n        # Perform flip\n        idx = np.random.choice(valid_indices, p=probabilities)\n        new_solution[idx] = 1 - new_solution[idx]\n\n    # Step 4: Fallback to random flip if no improvement found\n    if np.array_equal(new_solution, base_solution):\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8648962251957176,
            3.907723367214203
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hybrid solution selection\n    solutions = [sol for sol, _ in archive]\n    objectives = np.array([obj for _, obj in archive])\n    weights = np.array([np.sum(weight_lst[sol == 1]) for sol in solutions])\n\n    # Calculate Pareto ranks\n    pareto_ranks = np.zeros(len(solutions))\n    for i in range(len(solutions)):\n        for j in range(len(solutions)):\n            if (objectives[j, 0] >= objectives[i, 0] and objectives[j, 1] >= objectives[i, 1] and\n                (objectives[j, 0] > objectives[i, 0] or objectives[j, 1] > objectives[i, 1])):\n                pareto_ranks[i] += 1\n\n    # Calculate solution quality scores\n    quality_scores = (objectives[:, 0] + objectives[:, 1]) / (np.max(objectives[:, 0] + objectives[:, 1]) + 1e-6)\n\n    # Calculate diversity scores\n    diversity_scores = np.zeros(len(solutions))\n    for i in range(len(solutions)):\n        distances = np.sum(np.abs(solutions[i] - solutions), axis=1)\n        diversity_scores[i] = np.mean(distances)\n\n    # Combine metrics for selection\n    selection_scores = (1 - pareto_ranks/np.max(pareto_ranks)) * 0.4 + quality_scores * 0.4 + diversity_scores/np.max(diversity_scores) * 0.2\n    selected_idx = np.argmax(selection_scores)\n    base_solution = solutions[selected_idx].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = weights[selected_idx]\n\n    # Step 2: Dynamic neighborhood search\n    weight_ratio = current_weight / capacity\n    neighborhood_size = max(1, int(len(new_solution) * (0.3 + 0.7 * (1 - weight_ratio))))\n\n    # Calculate marginal contributions\n    marginal_v1 = np.where(new_solution == 1, -value1_lst, value1_lst)\n    marginal_v2 = np.where(new_solution == 1, -value2_lst, value2_lst)\n    marginal_weights = np.where(new_solution == 1, -weight_lst, weight_lst)\n\n    # Calculate potential objective improvements\n    potential_v1 = objectives[selected_idx, 0] + marginal_v1\n    potential_v2 = objectives[selected_idx, 1] + marginal_v2\n    potential_weights = current_weight + marginal_weights\n\n    # Calculate improvement scores\n    improvement_scores = np.zeros(len(new_solution))\n    for i in range(len(new_solution)):\n        if potential_weights[i] <= capacity:\n            improvement_scores[i] = (potential_v1[i] - objectives[selected_idx, 0]) * (potential_v2[i] - objectives[selected_idx, 1]) / (np.abs(value1_lst[i] * value2_lst[i]) + 1e-6)\n\n    # Select neighborhood candidates\n    candidate_indices = np.argsort(improvement_scores)[-neighborhood_size:]\n\n    # Step 3: Probabilistic flip selection\n    valid_indices = [i for i in candidate_indices if\n                    (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                    (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n\n    if valid_indices:\n        # Calculate flip probabilities\n        contributions = improvement_scores[valid_indices]\n        if np.sum(contributions) > 0:\n            probabilities = contributions / np.sum(contributions)\n        else:\n            probabilities = np.ones(len(valid_indices)) / len(valid_indices)\n\n        # Perform flip\n        idx = np.random.choice(valid_indices, p=probabilities)\n        new_solution[idx] = 1 - new_solution[idx]\n\n    # Step 4: Fallback to random flip if no improvement found\n    if np.array_equal(new_solution, base_solution):\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm first filters non-dominated solutions from the archive and selects a base solution using a weighted crowding-distance metric that balances Pareto dominance and diversity. It then applies a hybrid local search with dynamic neighborhood sizes (20-50% of items) based on weight utilization, using objective-weighted improvement criteria to prioritize flips that enhance both objectives proportionally. If no improvement is found, it performs a probabilistic flip based on potential contributions while ensuring feasibility. The method dynamically adjusts objective weights to balance exploration and exploitation, generating high-quality neighbors that dominate or complement existing solutions.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with weighted crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic neighborhood size based on weight utilization\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 4: Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 5: Hybrid local search with cluster flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering both objectives\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 84,
        "algorithm": "The algorithm selects a base solution from the archive using a modified Pareto frontier approach that prioritizes solutions with higher combined value while maintaining diversity through normalized crowding distance. It then applies a hybrid local search that dynamically adjusts neighborhood size based on weight utilization and solution quality, using value-weighted improvement criteria to prioritize flips that enhance both objectives proportionally. If no improving flip is found, it performs a probabilistic flip based on normalized potential contributions while ensuring feasibility. The algorithm balances exploration and exploitation by dynamically adjusting neighborhood size and prioritizing value-weighted improvements.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter and select base solution using modified Pareto frontier\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Select solution with normalized crowding distance and combined value\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Normalize crowding distances and combine with solution quality\n        normalized_distances = (crowding_distances - np.min(crowding_distances)) / (np.max(crowding_distances) - np.min(crowding_distances) + 1e-6)\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        combined_scores = 0.7 * quality_scores + 0.3 * normalized_distances\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Dynamic neighborhood size based on both weight utilization and solution quality\n    weight_utilization = current_weight / capacity\n    solution_quality = (np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    subset_size = max(1, int(len(new_solution) * (0.1 + 0.4 * (1 - weight_utilization) + 0.3 * solution_quality)))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 3: Calculate value-weighted improvement criteria\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 4: Hybrid local search with value-weighted improvement\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Value-weighted improvement considering both objectives\n        improvement = (weight_v1 * delta_v1 + weight_v2 * delta_v2) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on normalized potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (weight_lst[i] + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8841568097735026,
            0.9990030229091644
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter and select base solution using modified Pareto frontier\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Select solution with normalized crowding distance and combined value\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Normalize crowding distances and combine with solution quality\n        normalized_distances = (crowding_distances - np.min(crowding_distances)) / (np.max(crowding_distances) - np.min(crowding_distances) + 1e-6)\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        combined_scores = 0.7 * quality_scores + 0.3 * normalized_distances\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Dynamic neighborhood size based on both weight utilization and solution quality\n    weight_utilization = current_weight / capacity\n    solution_quality = (np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    subset_size = max(1, int(len(new_solution) * (0.1 + 0.4 * (1 - weight_utilization) + 0.3 * solution_quality)))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 3: Calculate value-weighted improvement criteria\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 4: Hybrid local search with value-weighted improvement\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Value-weighted improvement considering both objectives\n        improvement = (weight_v1 * delta_v1 + weight_v2 * delta_v2) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on normalized potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (weight_lst[i] + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 5 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm first filters non-dominated solutions from the archive and selects a base solution using a weighted crowding-distance metric that balances Pareto dominance and diversity. It then applies a hybrid local search with dynamic neighborhood sizes (20-50% of items) based on weight utilization, using objective-weighted improvement criteria to prioritize flips that enhance both objectives proportionally. If no improvement is found, it performs a probabilistic flip based on potential contributions while ensuring feasibility. The method dynamically adjusts objective weights to balance exploration and exploitation, generating high-quality neighbors that dominate or complement existing solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with weighted crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic neighborhood size based on weight utilization\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 4: Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 5: Hybrid local search with cluster flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering both objectives\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThis algorithm combines Pareto dominance filtering with adaptive neighborhood diversification to generate high-quality neighbors. It selects solutions based on crowding distance from the Pareto front and uses dynamic cluster flips prioritized by weight-balanced item exchanges, adjusting flip sizes based on objective trade-offs and weight utilization, while falling back to greedy random flips if no improvement is found. The algorithm ensures feasibility by precomputing weight contributions and dynamically adjusting cluster sizes to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n        selected_idx = np.argmax(crowding_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Adaptive neighborhood diversification\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Dynamic cluster size based on weight utilization\n    cluster_size = max(1, int(len(new_solution) * 0.2 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Evaluate cluster flips\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy random flip if no improvement\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto ranking with crowding-distance selection to prioritize diverse, high-quality solutions from the archive, then applies an adaptive cluster-based local search that dynamically adjusts flip cluster sizes based on weight utilization and objective weights, ensuring feasibility through precomputed item contributions and move validation. It balances exploration/exploitation by flipping items within clusters while favoring moves that improve both objectives proportionally, falling back to random valid flips if no improvement is found.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max(obj[0] for _, obj in pareto_front) - min(obj[0] for _, obj in pareto_front) + 1e-6) + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max(obj[1] for _, obj in pareto_front) - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                crowding_distances.append(dist)\n\n            # Select solution with highest crowding distance\n            max_dist_idx = np.argmax(crowding_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive cluster-based item flips\n    cluster_size = max(1, int(len(new_solution) * 0.1 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            Here\u2019s the refined heuristic design for the BI-KP, optimized for high-quality, feasible neighbors:\n\n### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 85,
        "algorithm": "The heuristic algorithm selects a non-dominated solution from the archive using a weighted crowding-distance metric to balance Pareto dominance and diversity, then applies an adaptive hybrid local search with dynamic neighborhood sizes (10-40% of items) based on weight utilization, prioritizing flips that improve both objectives proportionally. If no improvement is found, it performs a probabilistic flip based on potential contributions while ensuring feasibility. The method dynamically adjusts objective weights and neighborhood sizes to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with weighted crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic neighborhood size based on weight utilization\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.1 + 0.3 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 4: Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 5: Hybrid local search with cluster flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering both objectives\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.852861631957075,
            0.8945692777633667
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with weighted crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic neighborhood size based on weight utilization\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.1 + 0.3 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 4: Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 5: Hybrid local search with cluster flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering both objectives\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 5 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThis heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm first identifies the Pareto front from the archive and selects a base solution using a weighted crowding-distance metric to prioritize high-diversity solutions. It then applies a hybrid local search that dynamically adjusts the neighborhood size based on the solution's weight utilization and objective weights, evaluating flips within the neighborhood while ensuring feasibility. If no improving move is found, it performs a probabilistic flip based on potential weighted contributions, dynamically adjusting objective weights to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Identify Pareto front\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            min_v1 = min(obj[0] for _, obj in pareto_front)\n            min_v2 = min(obj[1] for _, obj in pareto_front)\n\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min_v1 + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min_v2 + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto front clustering with dynamic objective-weighted cluster flips, prioritizing solutions based on crowding distance and hypervolume contributions while adaptively adjusting flip probabilities and cluster sizes to balance exploration and exploitation, using momentum to emphasize high-potential regions of the search space. Key design features include weighted improvement calculations, adaptive cluster sizing, and probabilistic flips with temperature-based selection to ensure feasible neighbor solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster solutions based on Pareto dominance and crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest combined crowding distance and hypervolume contribution\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster flip with momentum\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization and solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.3 + 0.2 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.9  # Higher momentum for more aggressive search\n\n    # Evaluate cluster flips with momentum\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm first filters non-dominated solutions from the archive and selects a base solution using a weighted crowding-distance metric that balances Pareto dominance and diversity. It then applies a hybrid local search with dynamic neighborhood sizes (20-50% of items) based on weight utilization, using objective-weighted improvement criteria to prioritize flips that enhance both objectives proportionally. If no improvement is found, it performs a probabilistic flip based on potential contributions while ensuring feasibility. The method dynamically adjusts objective weights to balance exploration and exploitation, generating high-quality neighbors that dominate or complement existing solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with weighted crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic neighborhood size based on weight utilization\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 4: Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 5: Hybrid local search with cluster flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering both objectives\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 86,
        "algorithm": "This algorithm selects a promising solution from the archive using a weighted combination of crowding distance and hypervolume contribution, then applies a hybrid local search with adaptive neighborhood size, dynamic objective weights, and probabilistic flips to generate high-quality neighbor solutions while ensuring feasibility. The selection prioritizes solutions with high crowding distances and hypervolume contributions, while the local search uses weighted improvements and adaptive temperature to balance exploration and exploitation. The neighborhood size dynamically adjusts based on solution quality and remaining capacity, and the search considers both objective values through weighted improvements.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a base solution using weighted crowding distance and hypervolume contribution\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Step 3: Calculate dynamic objective weights\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n        weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n        # Step 4: Hybrid local search with weighted improvements\n        best_improvement = -float('inf')\n        best_candidate = None\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Step 5: Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8114188899951833,
            0.8692453503608704
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a base solution using weighted crowding distance and hypervolume contribution\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Step 3: Calculate dynamic objective weights\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n        weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n        # Step 4: Hybrid local search with weighted improvements\n        best_improvement = -float('inf')\n        best_candidate = None\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Step 5: Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThis heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a solution from the archive using a hybrid of Pareto dominance and value-to-weight ratio analysis, then applies a dynamic cluster-based local search with adaptive flip probabilities that prioritize both objectives based on their alignment, ensuring feasibility through weighted random sampling and move validation. It dynamically adjusts cluster size and flip weights based on weight utilization and objective alignment, favoring moves that improve both objectives proportionally while maintaining solution feasibility.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: Pareto dominance + value-to-weight ratio\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate value-to-weight ratios\n            ratios = []\n            for sol, obj in pareto_front:\n                total_weight = np.sum(weight_lst[sol == 1])\n                ratio = (obj[0]/(total_weight + 1e-6) + obj[1]/(total_weight + 1e-6)) / 2\n                ratios.append(ratio)\n\n            # Select solution with highest ratio\n            max_ratio_idx = np.argmax(ratios)\n            base_solution = pareto_front[max_ratio_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic cluster size based on weight utilization and objective alignment\n    weight_util = current_weight / capacity\n    cluster_size = max(1, int(len(new_solution) * (0.2 - 0.1 * weight_util)))\n\n    # Objective alignment factor\n    if current_v1 > 0 and current_v2 > 0:\n        alignment = min(current_v1, current_v2) / max(current_v1, current_v2)\n    else:\n        alignment = 0.5\n\n    # Weighted random sampling of cluster indices\n    weights = np.ones(len(new_solution))\n    weights[new_solution == 1] *= (1 + alignment)  # Favor items already in solution\n    weights[new_solution == 0] *= (1 - weight_util)  # Favor adding items when underutilized\n\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False, p=weights/np.sum(weights))\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Adaptive improvement calculation\n        improvement = (delta_v1 * (1 - alignment) + delta_v2 * alignment) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Weighted random flip if no improvement found\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            weights = np.ones(len(valid_indices))\n            for i, idx in enumerate(valid_indices):\n                if new_solution[idx] == 1:\n                    weights[i] *= (1 + alignment)\n                else:\n                    weights[i] *= (1 - weight_util)\n            idx = np.random.choice(valid_indices, p=weights/np.sum(weights))\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 87,
        "algorithm": "The algorithm selects a promising solution from the archive using adaptive Pareto front selection and crowding distance, then applies a dynamic hybrid local search that adjusts neighborhood size based on solution quality and remaining capacity, prioritizing items with high value-to-weight ratios while balancing exploration and exploitation through temperature-based probabilistic flips and momentum-weighted improvement evaluation. The method ensures feasibility by precomputing item contributions and only considering valid flips, with objective weights dynamically adjusted based on current solution quality.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Adaptive selection using crowding distance\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distance\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                if i == 0 or i == len(pareto_front)-1:\n                    distance = float('inf')\n                else:\n                    left = pareto_front[i-1][1]\n                    right = pareto_front[i+1][1]\n                    distance = (right[0] - left[0]) / (max_v1 + 1e-6) + (right[1] - left[1]) / (max_v2 + 1e-6)\n                distances.append(distance)\n\n            # Select solution with highest crowding distance\n            max_distance_idx = np.argmax(distances)\n            base_solution = pareto_front[max_distance_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.3 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        # Select neighborhood with weighted probability based on item contributions\n        item_contributions = []\n        for i in range(total_items):\n            v1_contrib = value1_lst[i] / (weight_lst[i] + 1e-6)\n            v2_contrib = value2_lst[i] / (weight_lst[i] + 1e-6)\n            item_contributions.append(v1_contrib + v2_contrib)\n\n        total_contrib = sum(item_contributions)\n        if total_contrib > 0:\n            probabilities = [c / total_contrib for c in item_contributions]\n            neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False, p=probabilities)\n        else:\n            neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate adaptive objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.6 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.4 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.7  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with temperature-based selection\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.906463239357209,
            1.2843849062919617
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Adaptive selection using crowding distance\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distance\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                if i == 0 or i == len(pareto_front)-1:\n                    distance = float('inf')\n                else:\n                    left = pareto_front[i-1][1]\n                    right = pareto_front[i+1][1]\n                    distance = (right[0] - left[0]) / (max_v1 + 1e-6) + (right[1] - left[1]) / (max_v2 + 1e-6)\n                distances.append(distance)\n\n            # Select solution with highest crowding distance\n            max_distance_idx = np.argmax(distances)\n            base_solution = pareto_front[max_distance_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.3 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        # Select neighborhood with weighted probability based on item contributions\n        item_contributions = []\n        for i in range(total_items):\n            v1_contrib = value1_lst[i] / (weight_lst[i] + 1e-6)\n            v2_contrib = value2_lst[i] / (weight_lst[i] + 1e-6)\n            item_contributions.append(v1_contrib + v2_contrib)\n\n        total_contrib = sum(item_contributions)\n        if total_contrib > 0:\n            probabilities = [c / total_contrib for c in item_contributions]\n            neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False, p=probabilities)\n        else:\n            neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate adaptive objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.6 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.4 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.7  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with temperature-based selection\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThis heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a solution from the archive using a hybrid of Pareto dominance and value-to-weight ratio analysis, then applies a dynamic cluster-based local search with adaptive flip probabilities that prioritize both objectives based on their alignment, ensuring feasibility through weighted random sampling and move validation. It dynamically adjusts cluster size and flip weights based on weight utilization and objective alignment, favoring moves that improve both objectives proportionally while maintaining solution feasibility.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: Pareto dominance + value-to-weight ratio\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate value-to-weight ratios\n            ratios = []\n            for sol, obj in pareto_front:\n                total_weight = np.sum(weight_lst[sol == 1])\n                ratio = (obj[0]/(total_weight + 1e-6) + obj[1]/(total_weight + 1e-6)) / 2\n                ratios.append(ratio)\n\n            # Select solution with highest ratio\n            max_ratio_idx = np.argmax(ratios)\n            base_solution = pareto_front[max_ratio_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic cluster size based on weight utilization and objective alignment\n    weight_util = current_weight / capacity\n    cluster_size = max(1, int(len(new_solution) * (0.2 - 0.1 * weight_util)))\n\n    # Objective alignment factor\n    if current_v1 > 0 and current_v2 > 0:\n        alignment = min(current_v1, current_v2) / max(current_v1, current_v2)\n    else:\n        alignment = 0.5\n\n    # Weighted random sampling of cluster indices\n    weights = np.ones(len(new_solution))\n    weights[new_solution == 1] *= (1 + alignment)  # Favor items already in solution\n    weights[new_solution == 0] *= (1 - weight_util)  # Favor adding items when underutilized\n\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False, p=weights/np.sum(weights))\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Adaptive improvement calculation\n        improvement = (delta_v1 * (1 - alignment) + delta_v2 * alignment) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Weighted random flip if no improvement found\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            weights = np.ones(len(valid_indices))\n            for i, idx in enumerate(valid_indices):\n                if new_solution[idx] == 1:\n                    weights[i] *= (1 + alignment)\n                else:\n                    weights[i] *= (1 - weight_util)\n            idx = np.random.choice(valid_indices, p=weights/np.sum(weights))\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 88,
        "algorithm": "The algorithm selects a base solution from the archive using a weighted crowding-distance metric to prioritize high-diversity solutions, then applies a hybrid local search that adaptively flips 20-50% of items based on remaining capacity, with objective-weighted swaps and momentum-based improvement calculations to ensure feasibility and balance exploration/exploitation. The key variables are `neighborhood_size`, `weight_v1/v2`, and `momentum`, which dynamically adjust the search intensity and balance between objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution using weighted crowding-distance metric\n        crowding_distances = []\n        sorted_archive = sorted(archive, key=lambda x: (x[1][0], x[1][1]))\n\n        for i in range(len(sorted_archive)):\n            if i == 0 or i == len(sorted_archive) - 1:\n                crowding_distances.append(float('inf'))\n            else:\n                dist_v1 = sorted_archive[i+1][1][0] - sorted_archive[i-1][1][0]\n                dist_v2 = sorted_archive[i+1][1][1] - sorted_archive[i-1][1][1]\n                crowding_distances.append(dist_v1 + dist_v2)\n\n        max_dist_idx = np.argmax(crowding_distances)\n        base_solution = sorted_archive[max_dist_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Dynamic neighborhood size (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, min(total_items, int(total_items * (0.2 + 0.3 * (remaining_capacity / capacity)))))\n\n    if neighborhood_size > 0:\n        # Adaptive subset flips with objective-weighted swaps\n        subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate objective weights\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = total_v1 / (total_v1 + total_v2 + 1e-6)\n        weight_v2 = total_v2 / (total_v1 + total_v2 + 1e-6)\n\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.7  # Higher momentum for more aggressive search\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                temperature = 1 - (remaining_capacity / capacity)\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8753876025617844,
            0.396170973777771
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution using weighted crowding-distance metric\n        crowding_distances = []\n        sorted_archive = sorted(archive, key=lambda x: (x[1][0], x[1][1]))\n\n        for i in range(len(sorted_archive)):\n            if i == 0 or i == len(sorted_archive) - 1:\n                crowding_distances.append(float('inf'))\n            else:\n                dist_v1 = sorted_archive[i+1][1][0] - sorted_archive[i-1][1][0]\n                dist_v2 = sorted_archive[i+1][1][1] - sorted_archive[i-1][1][1]\n                crowding_distances.append(dist_v1 + dist_v2)\n\n        max_dist_idx = np.argmax(crowding_distances)\n        base_solution = sorted_archive[max_dist_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Dynamic neighborhood size (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, min(total_items, int(total_items * (0.2 + 0.3 * (remaining_capacity / capacity)))))\n\n    if neighborhood_size > 0:\n        # Adaptive subset flips with objective-weighted swaps\n        subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate objective weights\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = total_v1 / (total_v1 + total_v2 + 1e-6)\n        weight_v2 = total_v2 / (total_v1 + total_v2 + 1e-6)\n\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.7  # Higher momentum for more aggressive search\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                temperature = 1 - (remaining_capacity / capacity)\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a solution from the archive using a hybrid of Pareto dominance and value-to-weight ratio analysis, then applies a dynamic cluster-based local search with adaptive flip probabilities that prioritize both objectives based on their alignment, ensuring feasibility through weighted random sampling and move validation. It dynamically adjusts cluster size and flip weights based on weight utilization and objective alignment, favoring moves that improve both objectives proportionally while maintaining solution feasibility.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: Pareto dominance + value-to-weight ratio\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate value-to-weight ratios\n            ratios = []\n            for sol, obj in pareto_front:\n                total_weight = np.sum(weight_lst[sol == 1])\n                ratio = (obj[0]/(total_weight + 1e-6) + obj[1]/(total_weight + 1e-6)) / 2\n                ratios.append(ratio)\n\n            # Select solution with highest ratio\n            max_ratio_idx = np.argmax(ratios)\n            base_solution = pareto_front[max_ratio_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic cluster size based on weight utilization and objective alignment\n    weight_util = current_weight / capacity\n    cluster_size = max(1, int(len(new_solution) * (0.2 - 0.1 * weight_util)))\n\n    # Objective alignment factor\n    if current_v1 > 0 and current_v2 > 0:\n        alignment = min(current_v1, current_v2) / max(current_v1, current_v2)\n    else:\n        alignment = 0.5\n\n    # Weighted random sampling of cluster indices\n    weights = np.ones(len(new_solution))\n    weights[new_solution == 1] *= (1 + alignment)  # Favor items already in solution\n    weights[new_solution == 0] *= (1 - weight_util)  # Favor adding items when underutilized\n\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False, p=weights/np.sum(weights))\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Adaptive improvement calculation\n        improvement = (delta_v1 * (1 - alignment) + delta_v2 * alignment) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Weighted random flip if no improvement found\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            weights = np.ones(len(valid_indices))\n            for i, idx in enumerate(valid_indices):\n                if new_solution[idx] == 1:\n                    weights[i] *= (1 + alignment)\n                else:\n                    weights[i] *= (1 - weight_util)\n            idx = np.random.choice(valid_indices, p=weights/np.sum(weights))\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 89,
        "algorithm": "The algorithm selects a promising solution from the archive using a hybrid of Pareto dominance and objective diversity, then applies a dynamic segment-based local search with adaptive move probabilities that prioritize both objectives based on their correlation, ensuring feasibility through weighted segment selection and move validation. The segment size and move weights are dynamically adjusted based on weight utilization and objective correlation, favoring items already in the solution or adding new items when underutilized. The improvement calculation balances objective contributions using correlation factors, and a weighted random flip is used as a fallback when no improvement is found.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: Pareto dominance + objective diversity\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate objective correlation and diversity\n            correlations = []\n            for sol, obj in pareto_front:\n                total_weight = np.sum(weight_lst[sol == 1])\n                if total_weight > 0:\n                    corr = (obj[0] * obj[1]) / (total_weight * np.sqrt(obj[0]**2 + obj[1]**2 + 1e-6))\n                else:\n                    corr = 0\n                diversity = (obj[0] + obj[1]) / (total_weight + 1e-6)\n                score = corr * 0.6 + diversity * 0.4\n                correlations.append(score)\n\n            max_score_idx = np.argmax(correlations)\n            base_solution = pareto_front[max_score_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic segment size based on weight utilization and objective correlation\n    weight_util = current_weight / capacity\n    if current_v1 > 0 and current_v2 > 0:\n        corr = (current_v1 * current_v2) / (np.sqrt(current_v1**2 + current_v2**2) * np.sqrt(current_v1**2 + current_v2**2 + 1e-6))\n    else:\n        corr = 0\n    segment_size = max(1, int(len(new_solution) * (0.3 - 0.2 * weight_util + 0.1 * corr)))\n\n    # Weighted random selection of segments\n    weights = np.ones(len(new_solution))\n    weights[new_solution == 1] *= (1 + corr)  # Favor items already in solution\n    weights[new_solution == 0] *= (1 - weight_util)  # Favor adding items when underutilized\n\n    segments = []\n    for _ in range(segment_size):\n        segment = np.random.choice(len(new_solution), size=min(3, len(new_solution)), replace=False, p=weights/np.sum(weights))\n        segments.append(segment)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for segment in segments:\n        for idx in segment:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Adaptive improvement calculation with correlation factor\n            improvement = (delta_v1 * (1 - corr) + delta_v2 * corr) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Weighted random flip if no improvement found\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            weights = np.ones(len(valid_indices))\n            for i, idx in enumerate(valid_indices):\n                if new_solution[idx] == 1:\n                    weights[i] *= (1 + corr)\n                else:\n                    weights[i] *= (1 - weight_util)\n            idx = np.random.choice(valid_indices, p=weights/np.sum(weights))\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8328534279992987,
            4.821934789419174
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: Pareto dominance + objective diversity\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate objective correlation and diversity\n            correlations = []\n            for sol, obj in pareto_front:\n                total_weight = np.sum(weight_lst[sol == 1])\n                if total_weight > 0:\n                    corr = (obj[0] * obj[1]) / (total_weight * np.sqrt(obj[0]**2 + obj[1]**2 + 1e-6))\n                else:\n                    corr = 0\n                diversity = (obj[0] + obj[1]) / (total_weight + 1e-6)\n                score = corr * 0.6 + diversity * 0.4\n                correlations.append(score)\n\n            max_score_idx = np.argmax(correlations)\n            base_solution = pareto_front[max_score_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic segment size based on weight utilization and objective correlation\n    weight_util = current_weight / capacity\n    if current_v1 > 0 and current_v2 > 0:\n        corr = (current_v1 * current_v2) / (np.sqrt(current_v1**2 + current_v2**2) * np.sqrt(current_v1**2 + current_v2**2 + 1e-6))\n    else:\n        corr = 0\n    segment_size = max(1, int(len(new_solution) * (0.3 - 0.2 * weight_util + 0.1 * corr)))\n\n    # Weighted random selection of segments\n    weights = np.ones(len(new_solution))\n    weights[new_solution == 1] *= (1 + corr)  # Favor items already in solution\n    weights[new_solution == 0] *= (1 - weight_util)  # Favor adding items when underutilized\n\n    segments = []\n    for _ in range(segment_size):\n        segment = np.random.choice(len(new_solution), size=min(3, len(new_solution)), replace=False, p=weights/np.sum(weights))\n        segments.append(segment)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for segment in segments:\n        for idx in segment:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Adaptive improvement calculation with correlation factor\n            improvement = (delta_v1 * (1 - corr) + delta_v2 * corr) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Weighted random flip if no improvement found\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            weights = np.ones(len(valid_indices))\n            for i, idx in enumerate(valid_indices):\n                if new_solution[idx] == 1:\n                    weights[i] *= (1 + corr)\n                else:\n                    weights[i] *= (1 - weight_util)\n            idx = np.random.choice(valid_indices, p=weights/np.sum(weights))\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects a solution from the archive using a hybrid of Pareto dominance and value-to-weight ratio analysis, then applies a dynamic cluster-based local search with adaptive flip probabilities that prioritize both objectives based on their alignment, ensuring feasibility through weighted random sampling and move validation. It dynamically adjusts cluster size and flip weights based on weight utilization and objective alignment, favoring moves that improve both objectives proportionally while maintaining solution feasibility.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: Pareto dominance + value-to-weight ratio\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate value-to-weight ratios\n            ratios = []\n            for sol, obj in pareto_front:\n                total_weight = np.sum(weight_lst[sol == 1])\n                ratio = (obj[0]/(total_weight + 1e-6) + obj[1]/(total_weight + 1e-6)) / 2\n                ratios.append(ratio)\n\n            # Select solution with highest ratio\n            max_ratio_idx = np.argmax(ratios)\n            base_solution = pareto_front[max_ratio_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic cluster size based on weight utilization and objective alignment\n    weight_util = current_weight / capacity\n    cluster_size = max(1, int(len(new_solution) * (0.2 - 0.1 * weight_util)))\n\n    # Objective alignment factor\n    if current_v1 > 0 and current_v2 > 0:\n        alignment = min(current_v1, current_v2) / max(current_v1, current_v2)\n    else:\n        alignment = 0.5\n\n    # Weighted random sampling of cluster indices\n    weights = np.ones(len(new_solution))\n    weights[new_solution == 1] *= (1 + alignment)  # Favor items already in solution\n    weights[new_solution == 0] *= (1 - weight_util)  # Favor adding items when underutilized\n\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False, p=weights/np.sum(weights))\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Adaptive improvement calculation\n        improvement = (delta_v1 * (1 - alignment) + delta_v2 * alignment) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Weighted random flip if no improvement found\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            weights = np.ones(len(valid_indices))\n            for i, idx in enumerate(valid_indices):\n                if new_solution[idx] == 1:\n                    weights[i] *= (1 + alignment)\n                else:\n                    weights[i] *= (1 - weight_util)\n            idx = np.random.choice(valid_indices, p=weights/np.sum(weights))\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 90,
        "algorithm": "This algorithm selects a promising solution from the archive by combining Pareto dominance and objective-specific value-to-weight ratios, then applies a novel adaptive cluster-based local search with dynamic flip probabilities. It prioritizes both objectives based on their correlation, ensuring feasibility through weighted random sampling and move validation, with cluster size and flip weights adjusted based on weight utilization and objective correlation to favor proportionally balanced improvements. The algorithm intelligently balances exploration and exploitation by dynamically adjusting cluster sizes and flip probabilities while maintaining solution feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Enhanced selection: Pareto dominance + correlated value-to-weight ratio\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate correlated value-to-weight ratios\n            correlations = []\n            for sol, obj in pareto_front:\n                total_weight = np.sum(weight_lst[sol == 1])\n                v1_ratio = obj[0] / (total_weight + 1e-6)\n                v2_ratio = obj[1] / (total_weight + 1e-6)\n                correlation = (v1_ratio + v2_ratio) / (1 + abs(v1_ratio - v2_ratio))\n                correlations.append(correlation)\n\n            # Select solution with highest correlation\n            max_correlation_idx = np.argmax(correlations)\n            base_solution = pareto_front[max_correlation_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic cluster size based on weight utilization and objective correlation\n    weight_util = current_weight / capacity\n    if current_v1 > 0 and current_v2 > 0:\n        correlation = np.corrcoef(value1_lst[new_solution == 1], value2_lst[new_solution == 1])[0, 1]\n        cluster_size = max(1, int(len(new_solution) * (0.3 - 0.2 * weight_util) * (1 + 0.2 * correlation)))\n    else:\n        cluster_size = max(1, int(len(new_solution) * (0.3 - 0.2 * weight_util)))\n\n    # Weighted random sampling of cluster indices with objective-specific weights\n    weights = np.ones(len(new_solution))\n    weights[new_solution == 1] *= 0.7  # Lower weight for items already in solution\n    weights[new_solution == 0] *= 1.3  # Higher weight for potential additions\n\n    # Adjust weights based on objective correlation\n    if current_v1 > 0 and current_v2 > 0:\n        v1_weights = value1_lst / (value2_lst + 1e-6)\n        v2_weights = value2_lst / (value1_lst + 1e-6)\n        combined_weights = (v1_weights + v2_weights) / 2\n        weights *= combined_weights\n\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False, p=weights/np.sum(weights))\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Objective-specific improvement calculation\n        if current_v1 > 0 and current_v2 > 0:\n            improvement = (delta_v1 / (current_v1 + 1e-6) + delta_v2 / (current_v2 + 1e-6)) / (1 + abs(delta_v1/(current_v1+1e-6) - delta_v2/(current_v2+1e-6)))\n        else:\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Objective-specific weighted random flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            weights = np.ones(len(valid_indices))\n            for i, idx in enumerate(valid_indices):\n                if new_solution[idx] == 1:\n                    weights[i] *= 0.7\n                else:\n                    weights[i] *= 1.3\n                if current_v1 > 0 and current_v2 > 0:\n                    weights[i] *= (value1_lst[idx] + value2_lst[idx]) / (current_v1 + current_v2 + 1e-6)\n            idx = np.random.choice(valid_indices, p=weights/np.sum(weights))\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8909178154803127,
            0.7328149676322937
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Enhanced selection: Pareto dominance + correlated value-to-weight ratio\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate correlated value-to-weight ratios\n            correlations = []\n            for sol, obj in pareto_front:\n                total_weight = np.sum(weight_lst[sol == 1])\n                v1_ratio = obj[0] / (total_weight + 1e-6)\n                v2_ratio = obj[1] / (total_weight + 1e-6)\n                correlation = (v1_ratio + v2_ratio) / (1 + abs(v1_ratio - v2_ratio))\n                correlations.append(correlation)\n\n            # Select solution with highest correlation\n            max_correlation_idx = np.argmax(correlations)\n            base_solution = pareto_front[max_correlation_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic cluster size based on weight utilization and objective correlation\n    weight_util = current_weight / capacity\n    if current_v1 > 0 and current_v2 > 0:\n        correlation = np.corrcoef(value1_lst[new_solution == 1], value2_lst[new_solution == 1])[0, 1]\n        cluster_size = max(1, int(len(new_solution) * (0.3 - 0.2 * weight_util) * (1 + 0.2 * correlation)))\n    else:\n        cluster_size = max(1, int(len(new_solution) * (0.3 - 0.2 * weight_util)))\n\n    # Weighted random sampling of cluster indices with objective-specific weights\n    weights = np.ones(len(new_solution))\n    weights[new_solution == 1] *= 0.7  # Lower weight for items already in solution\n    weights[new_solution == 0] *= 1.3  # Higher weight for potential additions\n\n    # Adjust weights based on objective correlation\n    if current_v1 > 0 and current_v2 > 0:\n        v1_weights = value1_lst / (value2_lst + 1e-6)\n        v2_weights = value2_lst / (value1_lst + 1e-6)\n        combined_weights = (v1_weights + v2_weights) / 2\n        weights *= combined_weights\n\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False, p=weights/np.sum(weights))\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Objective-specific improvement calculation\n        if current_v1 > 0 and current_v2 > 0:\n            improvement = (delta_v1 / (current_v1 + 1e-6) + delta_v2 / (current_v2 + 1e-6)) / (1 + abs(delta_v1/(current_v1+1e-6) - delta_v2/(current_v2+1e-6)))\n        else:\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Objective-specific weighted random flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            weights = np.ones(len(valid_indices))\n            for i, idx in enumerate(valid_indices):\n                if new_solution[idx] == 1:\n                    weights[i] *= 0.7\n                else:\n                    weights[i] *= 1.3\n                if current_v1 > 0 and current_v2 > 0:\n                    weights[i] *= (value1_lst[idx] + value2_lst[idx]) / (current_v1 + current_v2 + 1e-6)\n            idx = np.random.choice(valid_indices, p=weights/np.sum(weights))\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 4 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a solution from the archive using a hybrid of Pareto dominance and value-to-weight ratio analysis, then applies a dynamic cluster-based local search with adaptive flip probabilities that prioritize both objectives based on their alignment, ensuring feasibility through weighted random sampling and move validation. It dynamically adjusts cluster size and flip weights based on weight utilization and objective alignment, favoring moves that improve both objectives proportionally while maintaining solution feasibility.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: Pareto dominance + value-to-weight ratio\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate value-to-weight ratios\n            ratios = []\n            for sol, obj in pareto_front:\n                total_weight = np.sum(weight_lst[sol == 1])\n                ratio = (obj[0]/(total_weight + 1e-6) + obj[1]/(total_weight + 1e-6)) / 2\n                ratios.append(ratio)\n\n            # Select solution with highest ratio\n            max_ratio_idx = np.argmax(ratios)\n            base_solution = pareto_front[max_ratio_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic cluster size based on weight utilization and objective alignment\n    weight_util = current_weight / capacity\n    cluster_size = max(1, int(len(new_solution) * (0.2 - 0.1 * weight_util)))\n\n    # Objective alignment factor\n    if current_v1 > 0 and current_v2 > 0:\n        alignment = min(current_v1, current_v2) / max(current_v1, current_v2)\n    else:\n        alignment = 0.5\n\n    # Weighted random sampling of cluster indices\n    weights = np.ones(len(new_solution))\n    weights[new_solution == 1] *= (1 + alignment)  # Favor items already in solution\n    weights[new_solution == 0] *= (1 - weight_util)  # Favor adding items when underutilized\n\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False, p=weights/np.sum(weights))\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Adaptive improvement calculation\n        improvement = (delta_v1 * (1 - alignment) + delta_v2 * alignment) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Weighted random flip if no improvement found\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            weights = np.ones(len(valid_indices))\n            for i, idx in enumerate(valid_indices):\n                if new_solution[idx] == 1:\n                    weights[i] *= (1 + alignment)\n                else:\n                    weights[i] *= (1 - weight_util)\n            idx = np.random.choice(valid_indices, p=weights/np.sum(weights))\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto ranking with crowding-distance selection to prioritize diverse, high-quality solutions from the archive, then applies an adaptive cluster-based local search that dynamically adjusts flip cluster sizes based on weight utilization and objective weights, ensuring feasibility through precomputed item contributions and move validation. It balances exploration/exploitation by flipping items within clusters while favoring moves that improve both objectives proportionally, falling back to random valid flips if no improvement is found.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max(obj[0] for _, obj in pareto_front) - min(obj[0] for _, obj in pareto_front) + 1e-6) + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max(obj[1] for _, obj in pareto_front) - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                crowding_distances.append(dist)\n\n            # Select solution with highest crowding distance\n            max_dist_idx = np.argmax(crowding_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive cluster-based item flips\n    cluster_size = max(1, int(len(new_solution) * 0.1 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            ### **Keywords**\n- **Adaptive selection**, **hybrid local search**, **Pareto-aware**, **dynamic feasibility**\n\n### **Advice**\n1. **Select solutions** using a **weighted crowding-distance** metric to prioritize high-diversity, high-potential solutions.\n2. **Hybridize local search** with **adaptive subset flips** (e.g., flip 20-50% of items) and **objective-weighted swaps** to balance exploration/exploitation.\n3. **Dynamically adjust** neighborhood size based on solution quality (e.g., larger neighborhoods for low-crowding solutions).\n4. **Ensure feasibility** by precomputing item contributions and using **greedy feasibility checks** during neighbor generation.\n\n### **Avoid**\n- **Static operators** (e.g., fixed-size swaps); instead, use **variable-length subset flips**.\n- **Random dominance-based selection**; prefer **Pareto-ranked candidates** for targeted improvement.\n- **Ignoring weight constraints**; always **validate feasibility** via precomputed item weights.\n\n### **Explanation**\nThis approach leverages **adaptive selection** to focus on promising regions while using **hybrid, dynamic operators** to escape local optima. By blending **crowding-distance** with **objective-weighted improvements**, it balances exploration and exploitation, ensuring high-quality, feasible neighbors.\n\n*(Word count: 100)*\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 91,
        "algorithm": "The algorithm selects a promising solution from the archive using a weighted crowding-distance metric to prioritize high-diversity, high-potential solutions, then applies a hybrid local search that dynamically adjusts neighborhood size and flip probabilities based on weight utilization and objective alignment, ensuring feasibility while balancing both objectives through weighted contributions. It evaluates flipping random subsets of items, where subset size and flip probabilities are adaptively determined by the solution's weight utilization and objective weights, and falls back to probabilistic flips when no improving moves are found. The key design priorities are diversity preservation, adaptive neighborhood exploration, and balanced objective optimization.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Weighted crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max(obj[0] for _, obj in pareto_front) - min(obj[0] for _, obj in pareto_front) + 1e-6) + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max(obj[1] for _, obj in pareto_front) - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                crowding_distances.append(dist)\n\n            # Calculate normalized objective scores\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            normalized_scores = []\n            for sol, obj in pareto_front:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_scores.append(norm_v1 + norm_v2)\n\n            # Weighted selection (crowding-distance + normalized objectives)\n            weights = []\n            for dist, score in zip(crowding_distances, normalized_scores):\n                weights.append(dist * score)\n\n            # Select solution with highest weight\n            max_weight_idx = np.argmax(weights)\n            base_solution = pareto_front[max_weight_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic cluster size based on weight utilization and objective alignment\n    weight_util = current_weight / capacity\n    cluster_size = max(1, int(len(new_solution) * (0.2 - 0.1 * weight_util)))\n\n    # Objective alignment factor\n    if current_v1 > 0 and current_v2 > 0:\n        alignment = min(current_v1, current_v2) / max(current_v1, current_v2)\n    else:\n        alignment = 0.5\n\n    # Weighted random sampling of cluster indices\n    weights = np.ones(len(new_solution))\n    weights[new_solution == 1] *= (1 + alignment)  # Favor items already in solution\n    weights[new_solution == 0] *= (1 - weight_util)  # Favor adding items when underutilized\n\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False, p=weights/np.sum(weights))\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Adaptive improvement calculation\n        improvement = (delta_v1 * (1 - alignment) + delta_v2 * alignment) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (potential_v1 * (1 - alignment) + potential_v2 * alignment) / (weight_lst[i] + 1e-6)\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.7982951457326366,
            0.6555376648902893
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Weighted crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max(obj[0] for _, obj in pareto_front) - min(obj[0] for _, obj in pareto_front) + 1e-6) + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max(obj[1] for _, obj in pareto_front) - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                crowding_distances.append(dist)\n\n            # Calculate normalized objective scores\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            normalized_scores = []\n            for sol, obj in pareto_front:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_scores.append(norm_v1 + norm_v2)\n\n            # Weighted selection (crowding-distance + normalized objectives)\n            weights = []\n            for dist, score in zip(crowding_distances, normalized_scores):\n                weights.append(dist * score)\n\n            # Select solution with highest weight\n            max_weight_idx = np.argmax(weights)\n            base_solution = pareto_front[max_weight_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    current_v1 = np.sum(value1_lst[new_solution == 1])\n    current_v2 = np.sum(value2_lst[new_solution == 1])\n\n    # Dynamic cluster size based on weight utilization and objective alignment\n    weight_util = current_weight / capacity\n    cluster_size = max(1, int(len(new_solution) * (0.2 - 0.1 * weight_util)))\n\n    # Objective alignment factor\n    if current_v1 > 0 and current_v2 > 0:\n        alignment = min(current_v1, current_v2) / max(current_v1, current_v2)\n    else:\n        alignment = 0.5\n\n    # Weighted random sampling of cluster indices\n    weights = np.ones(len(new_solution))\n    weights[new_solution == 1] *= (1 + alignment)  # Favor items already in solution\n    weights[new_solution == 0] *= (1 - weight_util)  # Favor adding items when underutilized\n\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False, p=weights/np.sum(weights))\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Adaptive improvement calculation\n        improvement = (delta_v1 * (1 - alignment) + delta_v2 * alignment) / (weight_lst[idx] + 1e-6)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (potential_v1 * (1 - alignment) + potential_v2 * alignment) / (weight_lst[i] + 1e-6)\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 5 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThis heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm first identifies the Pareto front from the archive and selects a base solution using a weighted crowding-distance metric to prioritize high-diversity solutions. It then applies a hybrid local search that dynamically adjusts the neighborhood size based on the solution's weight utilization and objective weights, evaluating flips within the neighborhood while ensuring feasibility. If no improving move is found, it performs a probabilistic flip based on potential weighted contributions, dynamically adjusting objective weights to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Identify Pareto front\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            min_v1 = min(obj[0] for _, obj in pareto_front)\n            min_v2 = min(obj[1] for _, obj in pareto_front)\n\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min_v1 + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min_v2 + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto front clustering with dynamic objective-weighted cluster flips, prioritizing solutions based on crowding distance and hypervolume contributions while adaptively adjusting flip probabilities and cluster sizes to balance exploration and exploitation, using momentum to emphasize high-potential regions of the search space. Key design features include weighted improvement calculations, adaptive cluster sizing, and probabilistic flips with temperature-based selection to ensure feasible neighbor solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster solutions based on Pareto dominance and crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest combined crowding distance and hypervolume contribution\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster flip with momentum\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization and solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.3 + 0.2 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.9  # Higher momentum for more aggressive search\n\n    # Evaluate cluster flips with momentum\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm first filters non-dominated solutions from the archive and selects a base solution using a weighted crowding-distance metric that balances Pareto dominance and diversity. It then applies a hybrid local search with dynamic neighborhood sizes (20-50% of items) based on weight utilization, using objective-weighted improvement criteria to prioritize flips that enhance both objectives proportionally. If no improvement is found, it performs a probabilistic flip based on potential contributions while ensuring feasibility. The method dynamically adjusts objective weights to balance exploration and exploitation, generating high-quality neighbors that dominate or complement existing solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with weighted crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic neighborhood size based on weight utilization\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 4: Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 5: Hybrid local search with cluster flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering both objectives\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: **Keywords**: Adaptive selection, hybrid local search, Pareto-aware, dynamic feasibility\n\n**Advice**:\n1. Use weighted crowding-distance to select high-diversity solutions.\n2. Hybridize local search with adaptive subset flips and objective-weighted swaps.\n3. Dynamically adjust neighborhood size based on solution quality.\n4. Precompute item contributions and validate feasibility greedily.\n\n**Avoid**:\n- Static operators (fixed-size swaps).\n- Random dominance-based selection; prefer Pareto-ranked candidates.\n- Ignoring weight constraints; always validate feasibility.\n\n**Explanation**: This approach balances exploration and exploitation by adaptively selecting solutions and using hybrid, dynamic operators to escape local optima while ensuring feasibility. *(Word count: 50)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 92,
        "algorithm": "The algorithm combines adaptive Pareto-ranked selection with a dynamic hybrid local search that prioritizes diverse solutions using weighted crowding-distance, performs objective-weighted flips within adaptive neighborhood subsets, and incorporates momentum-based improvement calculations to generate high-quality feasible neighbors while balancing exploration and exploitation. It dynamically adjusts neighborhood size based on current weight utilization and selects flips probabilistically based on weighted contribution scores, with temperature scaling to control randomness. The solution selection prioritizes high-quality Pareto front solutions while the local search focuses on improving both objectives with weighted trade-off considerations.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select base solution using weighted crowding-distance on Pareto front\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Dynamic hybrid local search\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive neighborhood size (20-50% of items)\n    neighborhood_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(len(new_solution), size=neighborhood_size, replace=False)\n\n    # Momentum-based improvement calculation\n    momentum = 0.8\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - (np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.761874729245626,
            0.5865095257759094
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Select base solution using weighted crowding-distance on Pareto front\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Dynamic hybrid local search\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive neighborhood size (20-50% of items)\n    neighborhood_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(len(new_solution), size=neighborhood_size, replace=False)\n\n    # Momentum-based improvement calculation\n    momentum = 0.8\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - (np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\nI have 5 existing algorithms with their codes as follows:\nNo. 1 algorithm and the corresponding code are:\nThe algorithm intelligently selects a solution from the archive by prioritizing those with high objective diversity (via crowding distance) and applies a hybrid local search combining item swaps and subset replacements to generate a feasible neighbor solution while ensuring weight constraints are not violated. It first evaluates solutions based on their crowding distances to identify promising candidates, then performs targeted swaps and subset operations to explore the neighborhood while maintaining feasibility. The algorithm balances exploration and exploitation by focusing on high-diversity regions of the Pareto front while dynamically adjusting the search based on the current solution's characteristics.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution (high objective diversity and low crowding distance)\n    # Calculate crowding distances for solutions in the archive\n    objectives = np.array([obj for _, obj in archive])\n    sorted_indices = np.argsort(objectives[:, 0])\n    objectives_sorted = objectives[sorted_indices]\n\n    # Compute crowding distances\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Select the solution with the highest crowding distance (most promising for improvement)\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Generate a neighbor solution using hybrid local search\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Strategy 1: Randomly swap items if feasible\n    for _ in range(min(5, n_items // 2)):\n        i, j = np.random.choice(n_items, 2, replace=False)\n        if new_solution[i] != new_solution[j]:\n            if new_solution[i] == 1 and new_solution[j] == 0:\n                new_weight = current_weight + weight_lst[j] - weight_lst[i]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n            elif new_solution[i] == 0 and new_solution[j] == 1:\n                new_weight = current_weight + weight_lst[i] - weight_lst[j]\n                if new_weight <= capacity:\n                    new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n                    current_weight = new_weight\n\n    # Strategy 2: Replace a subset of items with new ones if feasible\n    subset_size = min(3, n_items // 4)\n    if subset_size > 0:\n        # Remove a subset of items\n        remove_indices = np.where(new_solution == 1)[0]\n        if len(remove_indices) > 0:\n            remove_subset = np.random.choice(remove_indices, size=min(subset_size, len(remove_indices)), replace=False)\n            new_solution[remove_subset] = 0\n            current_weight -= np.sum(weight_lst[remove_subset])\n\n        # Add a subset of new items\n        add_indices = np.where(new_solution == 0)[0]\n        if len(add_indices) > 0:\n            add_subset = np.random.choice(add_indices, size=min(subset_size, len(add_indices)), replace=False)\n            potential_weight = current_weight + np.sum(weight_lst[add_subset])\n            if potential_weight <= capacity:\n                new_solution[add_subset] = 1\n                current_weight = potential_weight\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\nNo. 3 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 4 algorithm and the corresponding code are:\nThe algorithm selects the most promising solution from the archive (based on average value-to-weight density) and applies a hybrid local search strategy that prioritizes swapping low-value items for high-value ones while ensuring feasibility through adaptive perturbations. If no swaps are feasible, it randomly removes items to free up capacity. The approach balances exploration (via adaptive swaps) and exploitation (via value-to-weight ratios) to improve both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution from the archive\n    # Calculate the density (value/weight) for each objective\n    densities = []\n    for sol, (v1, v2) in archive:\n        included = sol == 1\n        total_weight = np.sum(weight_lst[included])\n        if total_weight == 0:\n            density1 = density2 = 0\n        else:\n            density1 = np.sum(value1_lst[included]) / total_weight\n            density2 = np.sum(value2_lst[included]) / total_weight\n        densities.append((density1 + density2) / 2)  # Average density\n\n    # Select the solution with the highest density (most promising for improvement)\n    best_idx = np.argmax(densities)\n    base_solution = archive[best_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Apply novel local search operator\n    # Hybrid strategy: adaptive item swaps and perturbations\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Adaptive swap: replace a low-value item with a high-value item\n        # Calculate value-to-weight ratio for included items\n        included_ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_ratios)]\n\n        # Calculate value-to-weight ratio for excluded items\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        # Check if swap is feasible\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n        else:\n            # If swap is not feasible, perform a perturbation\n            # Randomly flip a small number of items to find a feasible solution\n            max_perturbations = min(3, len(included_items))\n            for _ in range(max_perturbations):\n                candidate = random.choice(included_items)\n                if current_weight - weight_lst[candidate] <= capacity:\n                    new_solution[candidate] = 0\n                    current_weight -= weight_lst[candidate]\n                    break\n\n    # If no items are included, add the best item if possible\n    elif len(included_items) == 0 and len(excluded_items) > 0:\n        # Add the item with the highest value-to-weight ratio\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        if weight_lst[best_excluded] <= capacity:\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\nNo. 5 algorithm and the corresponding code are:\nThe algorithm selects a promising solution from the archive (top 30% by combined value) and applies a hybrid local search: 70% chance for a random item swap (ensuring feasibility) or 30% chance for flipping items based on high value-to-weight density, prioritizing items with the best combined value density. It ensures feasibility by checking weight constraints before applying changes.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) > 1:\n        # Sort archive by total value (sum of value1 and value2) in descending order\n        archive_sorted = sorted(archive, key=lambda x: x[1][0] + x[1][1], reverse=True)\n        # Select top 30% of solutions and pick one randomly\n        candidate_indices = min(3, len(archive_sorted))  # Ensure at least 1 candidate\n        selected_idx = np.random.randint(0, candidate_indices)\n        base_solution = archive_sorted[selected_idx][0].copy()\n    else:\n        base_solution = archive[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: randomly swap items or flip items based on value density\n    if np.random.rand() < 0.7:  # 70% chance for random swap\n        # Randomly select two items to swap\n        item_indices = np.where(new_solution == 1)[0]\n        if len(item_indices) >= 2:\n            i, j = np.random.choice(item_indices, 2, replace=False)\n            # Check if swapping is feasible (no weight violation)\n            if current_weight - weight_lst[i] + weight_lst[j] <= capacity:\n                new_solution[i], new_solution[j] = new_solution[j], new_solution[i]\n    else:  # 30% chance for value-based flip\n        # Calculate value density (value1 + value2) / weight for each item\n        value_density = (value1_lst + value2_lst) / weight_lst\n        # Sort items by value density in descending order\n        sorted_items = np.argsort(value_density)[::-1]\n        # Try to flip the most valuable items first\n        for item in sorted_items:\n            if new_solution[item] == 0:\n                if current_weight + weight_lst[item] <= capacity:\n                    new_solution[item] = 1\n                    current_weight += weight_lst[item]\n                    break\n            else:\n                if current_weight - weight_lst[item] >= 0:\n                    new_solution[item] = 0\n                    current_weight -= weight_lst[item]\n                    break\n\n    return new_solution\n\n\n\nAnalyze the logic of all the given code snippets carefully. Then identify the two code snippets whose logic is most different from each other\nand create a new algorithm that totally different in logic and form from both of them.\n1. First, describe your new algorithm and main steps in one long, detail sentence. The description must be inside within boxed {}.\n2. Next, implement the following Python function:\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\nCheck syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 93,
        "algorithm": "The algorithm selects the least crowded solution from the archive (based on crowding distance) and applies a hybrid local search combining value-weighted flips (prioritizing high-value items) and greedy swaps (optimizing combined objective density) while strictly maintaining feasibility through capacity checks. It balances exploration (randomized flips) and exploitation (greedy swaps) to efficiently navigate the solution space while prioritizing items with higher combined value-to-weight ratios.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the least crowded solution (minimum crowding distance)\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    selected_idx = np.argmin(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Value-weighted flip strategy\n    total_value1 = np.sum(value1_lst[new_solution == 1])\n    total_value2 = np.sum(value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    for idx in range(len(new_solution)):\n        if new_solution[idx] == 1:\n            # Probability of flipping based on normalized value contribution\n            flip_prob = (value1_lst[idx] / (total_value1 + 1e-6) + value2_lst[idx] / (total_value2 + 1e-6)) / 2\n            if np.random.rand() < flip_prob and current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            # Probability of adding based on normalized value\n            add_prob = (value1_lst[idx] + value2_lst[idx]) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n            if np.random.rand() < add_prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Step 3: Greedy swap to improve combined objectives\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate combined value density for included items\n        included_density = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_density)]\n\n        # Calculate combined value density for excluded items\n        excluded_density = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_density)]\n\n        # Check if swap is feasible\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.9081974355026639,
            0.9441126883029938
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the least crowded solution (minimum crowding distance)\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    selected_idx = np.argmin(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Value-weighted flip strategy\n    total_value1 = np.sum(value1_lst[new_solution == 1])\n    total_value2 = np.sum(value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    for idx in range(len(new_solution)):\n        if new_solution[idx] == 1:\n            # Probability of flipping based on normalized value contribution\n            flip_prob = (value1_lst[idx] / (total_value1 + 1e-6) + value2_lst[idx] / (total_value2 + 1e-6)) / 2\n            if np.random.rand() < flip_prob and current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            # Probability of adding based on normalized value\n            add_prob = (value1_lst[idx] + value2_lst[idx]) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n            if np.random.rand() < add_prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Step 3: Greedy swap to improve combined objectives\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate combined value density for included items\n        included_density = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_density)]\n\n        # Calculate combined value density for excluded items\n        excluded_density = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_density)]\n\n        # Check if swap is feasible\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n",
        "operation": "e1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects the most promising solution from the archive (based on average value-to-weight density) and applies a hybrid local search strategy that prioritizes swapping low-value items for high-value ones while ensuring feasibility through adaptive perturbations. If no swaps are feasible, it randomly removes items to free up capacity. The approach balances exploration (via adaptive swaps) and exploitation (via value-to-weight ratios) to improve both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution from the archive\n    # Calculate the density (value/weight) for each objective\n    densities = []\n    for sol, (v1, v2) in archive:\n        included = sol == 1\n        total_weight = np.sum(weight_lst[included])\n        if total_weight == 0:\n            density1 = density2 = 0\n        else:\n            density1 = np.sum(value1_lst[included]) / total_weight\n            density2 = np.sum(value2_lst[included]) / total_weight\n        densities.append((density1 + density2) / 2)  # Average density\n\n    # Select the solution with the highest density (most promising for improvement)\n    best_idx = np.argmax(densities)\n    base_solution = archive[best_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Apply novel local search operator\n    # Hybrid strategy: adaptive item swaps and perturbations\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Adaptive swap: replace a low-value item with a high-value item\n        # Calculate value-to-weight ratio for included items\n        included_ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_ratios)]\n\n        # Calculate value-to-weight ratio for excluded items\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        # Check if swap is feasible\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n        else:\n            # If swap is not feasible, perform a perturbation\n            # Randomly flip a small number of items to find a feasible solution\n            max_perturbations = min(3, len(included_items))\n            for _ in range(max_perturbations):\n                candidate = random.choice(included_items)\n                if current_weight - weight_lst[candidate] <= capacity:\n                    new_solution[candidate] = 0\n                    current_weight -= weight_lst[candidate]\n                    break\n\n    # If no items are included, add the best item if possible\n    elif len(included_items) == 0 and len(excluded_items) > 0:\n        # Add the item with the highest value-to-weight ratio\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        if weight_lst[best_excluded] <= capacity:\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThis heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            **Keywords**: Adaptive selection, hybrid local search, Pareto-aware, dynamic feasibility\n\n**Advice**:\n1. Use weighted crowding-distance to select high-diversity solutions.\n2. Hybridize local search with adaptive subset flips and objective-weighted swaps.\n3. Dynamically adjust neighborhood size based on solution quality.\n4. Precompute item contributions and validate feasibility greedily.\n\n**Avoid**:\n- Static operators (fixed-size swaps).\n- Random dominance-based selection; prefer Pareto-ranked candidates.\n- Ignoring weight constraints; always validate feasibility.\n\n**Explanation**: This approach balances exploration and exploitation by adaptively selecting solutions and using hybrid, dynamic operators to escape local optima while ensuring feasibility. *(Word count: 50)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 94,
        "algorithm": "The algorithm combines weighted crowding-distance selection with a dynamic subset flip operator, prioritizing solutions on the Pareto front with high crowding distance and adaptively flipping items based on marginal gains in the less-dominated objective while ensuring feasibility. It dynamically adjusts the flip size based on solution quality and remaining capacity, and greedily selects items with the highest marginal gains in the underperforming objective.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution using weighted crowding-distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        # Calculate crowding distance for Pareto front\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        crowding_distances = [0.0] * len(pareto_front)\n\n        # Sort by v1 and v2\n        sorted_v1 = sorted(range(len(pareto_front)), key=lambda i: pareto_front[i][1][0])\n        sorted_v2 = sorted(range(len(pareto_front)), key=lambda i: pareto_front[i][1][1])\n\n        for obj_idx in range(2):\n            sorted_idx = sorted_v1 if obj_idx == 0 else sorted_v2\n            crowding_distances[sorted_idx[0]] = float('inf')\n            crowding_distances[sorted_idx[-1]] = float('inf')\n            if max_v1 > 0 and max_v2 > 0:\n                for i in range(1, len(sorted_idx)-1):\n                    if obj_idx == 0:\n                        dist = (pareto_front[sorted_idx[i+1]][1][0] - pareto_front[sorted_idx[i-1]][1][0]) / max_v1\n                    else:\n                        dist = (pareto_front[sorted_idx[i+1]][1][1] - pareto_front[sorted_idx[i-1]][1][1]) / max_v2\n                    crowding_distances[sorted_idx[i]] += dist\n\n        # Select solution with highest crowding distance\n        best_idx = np.argmax(crowding_distances)\n        base_solution = pareto_front[best_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Dynamic subset flip operator\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate dynamic flip size\n    flip_size = max(1, int(total_items * (0.1 + 0.3 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    # Calculate marginal gains for each item\n    marginal_gains = []\n    for i in range(total_items):\n        if new_solution[i] == 1:\n            gain1 = -value1_lst[i]\n            gain2 = -value2_lst[i]\n        else:\n            gain1 = value1_lst[i]\n            gain2 = value2_lst[i]\n        marginal_gains.append((gain1, gain2))\n\n    # Identify less dominated objective\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    less_dominated = 1 if total_v1 < total_v2 else 2\n\n    # Sort items by marginal gain in less dominated objective\n    if less_dominated == 1:\n        sorted_items = sorted(range(total_items), key=lambda i: marginal_gains[i][0], reverse=True)\n    else:\n        sorted_items = sorted(range(total_items), key=lambda i: marginal_gains[i][1], reverse=True)\n\n    # Perform flips while maintaining feasibility\n    flipped = 0\n    for i in sorted_items:\n        if flipped >= flip_size:\n            break\n\n        temp_solution = new_solution.copy()\n        temp_solution[i] = 1 - temp_solution[i]\n\n        new_weight = current_weight\n        if temp_solution[i] == 1:\n            new_weight += weight_lst[i]\n        else:\n            new_weight -= weight_lst[i]\n\n        if new_weight <= capacity:\n            new_solution = temp_solution\n            current_weight = new_weight\n            flipped += 1\n\n    return new_solution\n\n",
        "score": [
            -0.8678806335802325,
            1.1344508826732635
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution using weighted crowding-distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        # Calculate crowding distance for Pareto front\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        crowding_distances = [0.0] * len(pareto_front)\n\n        # Sort by v1 and v2\n        sorted_v1 = sorted(range(len(pareto_front)), key=lambda i: pareto_front[i][1][0])\n        sorted_v2 = sorted(range(len(pareto_front)), key=lambda i: pareto_front[i][1][1])\n\n        for obj_idx in range(2):\n            sorted_idx = sorted_v1 if obj_idx == 0 else sorted_v2\n            crowding_distances[sorted_idx[0]] = float('inf')\n            crowding_distances[sorted_idx[-1]] = float('inf')\n            if max_v1 > 0 and max_v2 > 0:\n                for i in range(1, len(sorted_idx)-1):\n                    if obj_idx == 0:\n                        dist = (pareto_front[sorted_idx[i+1]][1][0] - pareto_front[sorted_idx[i-1]][1][0]) / max_v1\n                    else:\n                        dist = (pareto_front[sorted_idx[i+1]][1][1] - pareto_front[sorted_idx[i-1]][1][1]) / max_v2\n                    crowding_distances[sorted_idx[i]] += dist\n\n        # Select solution with highest crowding distance\n        best_idx = np.argmax(crowding_distances)\n        base_solution = pareto_front[best_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Dynamic subset flip operator\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate dynamic flip size\n    flip_size = max(1, int(total_items * (0.1 + 0.3 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    # Calculate marginal gains for each item\n    marginal_gains = []\n    for i in range(total_items):\n        if new_solution[i] == 1:\n            gain1 = -value1_lst[i]\n            gain2 = -value2_lst[i]\n        else:\n            gain1 = value1_lst[i]\n            gain2 = value2_lst[i]\n        marginal_gains.append((gain1, gain2))\n\n    # Identify less dominated objective\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    less_dominated = 1 if total_v1 < total_v2 else 2\n\n    # Sort items by marginal gain in less dominated objective\n    if less_dominated == 1:\n        sorted_items = sorted(range(total_items), key=lambda i: marginal_gains[i][0], reverse=True)\n    else:\n        sorted_items = sorted(range(total_items), key=lambda i: marginal_gains[i][1], reverse=True)\n\n    # Perform flips while maintaining feasibility\n    flipped = 0\n    for i in sorted_items:\n        if flipped >= flip_size:\n            break\n\n        temp_solution = new_solution.copy()\n        temp_solution[i] = 1 - temp_solution[i]\n\n        new_weight = current_weight\n        if temp_solution[i] == 1:\n            new_weight += weight_lst[i]\n        else:\n            new_weight -= weight_lst[i]\n\n        if new_weight <= capacity:\n            new_solution = temp_solution\n            current_weight = new_weight\n            flipped += 1\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a base solution from the archive using a weighted crowding-distance metric to prioritize high-diversity solutions, then applies a hybrid local search that adaptively flips 20-50% of items based on remaining capacity, with objective-weighted swaps and momentum-based improvement calculations to ensure feasibility and balance exploration/exploitation. The key variables are `neighborhood_size`, `weight_v1/v2`, and `momentum`, which dynamically adjust the search intensity and balance between objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution using weighted crowding-distance metric\n        crowding_distances = []\n        sorted_archive = sorted(archive, key=lambda x: (x[1][0], x[1][1]))\n\n        for i in range(len(sorted_archive)):\n            if i == 0 or i == len(sorted_archive) - 1:\n                crowding_distances.append(float('inf'))\n            else:\n                dist_v1 = sorted_archive[i+1][1][0] - sorted_archive[i-1][1][0]\n                dist_v2 = sorted_archive[i+1][1][1] - sorted_archive[i-1][1][1]\n                crowding_distances.append(dist_v1 + dist_v2)\n\n        max_dist_idx = np.argmax(crowding_distances)\n        base_solution = sorted_archive[max_dist_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Dynamic neighborhood size (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, min(total_items, int(total_items * (0.2 + 0.3 * (remaining_capacity / capacity)))))\n\n    if neighborhood_size > 0:\n        # Adaptive subset flips with objective-weighted swaps\n        subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate objective weights\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = total_v1 / (total_v1 + total_v2 + 1e-6)\n        weight_v2 = total_v2 / (total_v1 + total_v2 + 1e-6)\n\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.7  # Higher momentum for more aggressive search\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                temperature = 1 - (remaining_capacity / capacity)\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThis heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            **Keywords**: Adaptive selection, hybrid local search, Pareto-aware, dynamic feasibility\n\n**Advice**:\n1. Use weighted crowding-distance to select high-diversity solutions.\n2. Hybridize local search with adaptive subset flips and objective-weighted swaps.\n3. Dynamically adjust neighborhood size based on solution quality.\n4. Precompute item contributions and validate feasibility greedily.\n\n**Avoid**:\n- Static operators (fixed-size swaps).\n- Random dominance-based selection; prefer Pareto-ranked candidates.\n- Ignoring weight constraints; always validate feasibility.\n\n**Explanation**: This approach balances exploration and exploitation by adaptively selecting solutions and using hybrid, dynamic operators to escape local optima while ensuring feasibility. *(Word count: 50)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 95,
        "algorithm": "The algorithm combines hypervolume-based solution selection with a dynamic \"value-weighted subset swapping\" local search. It prioritizes solutions with high hypervolume contributions and adaptively adjusts neighborhood size based on solution quality and remaining capacity. The local search validates swaps in two phases (weight then value) and applies the top-k swaps weighted by their contributions to both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution using hypervolume-based fitness\n        hypervolumes = []\n        sorted_archive = sorted(archive, key=lambda x: (x[1][0], x[1][1]))\n\n        for i in range(len(sorted_archive)):\n            if i == 0 or i == len(sorted_archive) - 1:\n                hypervolumes.append(float('inf'))\n            else:\n                left = sorted_archive[i-1][1]\n                right = sorted_archive[i+1][1]\n                hypervolumes.append((right[0] - left[0]) * (right[1] - left[1]))\n\n        max_hv_idx = np.argmax(hypervolumes)\n        base_solution = sorted_archive[max_hv_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Dynamic neighborhood size based on solution quality and capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    max_possible_quality = np.sum(value1_lst) + np.sum(value2_lst)\n    neighborhood_size = max(2, min(10, int(0.1 * total_items * (1 + 0.5 * (solution_quality / max_possible_quality) *\n                    (remaining_capacity / capacity)))))\n\n    if neighborhood_size > 1:\n        # Value-weighted subset swapping\n        included = np.where(new_solution == 1)[0]\n        excluded = np.where(new_solution == 0)[0]\n\n        # Calculate objective weights\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = total_v1 / (total_v1 + total_v2 + 1e-6)\n        weight_v2 = total_v2 / (total_v1 + total_v2 + 1e-6)\n\n        # Generate candidate swaps (remove k items, add k items)\n        if len(included) >= neighborhood_size and len(excluded) >= neighborhood_size:\n            # Two-phase validation: first filter by weight, then by value\n            valid_in = []\n            for i in included:\n                if current_weight - weight_lst[i] <= capacity:\n                    valid_in.append(i)\n\n            valid_out = []\n            for j in excluded:\n                if current_weight + weight_lst[j] <= capacity:\n                    valid_out.append(j)\n\n            if len(valid_in) >= neighborhood_size and len(valid_out) >= neighborhood_size:\n                # Select top-k swaps by weighted contribution\n                contributions = []\n                for i in valid_in:\n                    for j in valid_out:\n                        delta_v1 = value1_lst[j] - value1_lst[i]\n                        delta_v2 = value2_lst[j] - value2_lst[i]\n                        contribution = weight_v1 * delta_v1 + weight_v2 * delta_v2\n                        contributions.append((contribution, i, j))\n\n                if contributions:\n                    contributions.sort(reverse=True, key=lambda x: x[0])\n                    best_swaps = contributions[:neighborhood_size]\n\n                    # Apply the best swaps\n                    for _, i, j in best_swaps:\n                        new_solution[i] = 0\n                        new_solution[j] = 1\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n    return new_solution\n\n",
        "score": [
            -0.9462587425764245,
            0.6900871098041534
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution using hypervolume-based fitness\n        hypervolumes = []\n        sorted_archive = sorted(archive, key=lambda x: (x[1][0], x[1][1]))\n\n        for i in range(len(sorted_archive)):\n            if i == 0 or i == len(sorted_archive) - 1:\n                hypervolumes.append(float('inf'))\n            else:\n                left = sorted_archive[i-1][1]\n                right = sorted_archive[i+1][1]\n                hypervolumes.append((right[0] - left[0]) * (right[1] - left[1]))\n\n        max_hv_idx = np.argmax(hypervolumes)\n        base_solution = sorted_archive[max_hv_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Dynamic neighborhood size based on solution quality and capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    max_possible_quality = np.sum(value1_lst) + np.sum(value2_lst)\n    neighborhood_size = max(2, min(10, int(0.1 * total_items * (1 + 0.5 * (solution_quality / max_possible_quality) *\n                    (remaining_capacity / capacity)))))\n\n    if neighborhood_size > 1:\n        # Value-weighted subset swapping\n        included = np.where(new_solution == 1)[0]\n        excluded = np.where(new_solution == 0)[0]\n\n        # Calculate objective weights\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = total_v1 / (total_v1 + total_v2 + 1e-6)\n        weight_v2 = total_v2 / (total_v1 + total_v2 + 1e-6)\n\n        # Generate candidate swaps (remove k items, add k items)\n        if len(included) >= neighborhood_size and len(excluded) >= neighborhood_size:\n            # Two-phase validation: first filter by weight, then by value\n            valid_in = []\n            for i in included:\n                if current_weight - weight_lst[i] <= capacity:\n                    valid_in.append(i)\n\n            valid_out = []\n            for j in excluded:\n                if current_weight + weight_lst[j] <= capacity:\n                    valid_out.append(j)\n\n            if len(valid_in) >= neighborhood_size and len(valid_out) >= neighborhood_size:\n                # Select top-k swaps by weighted contribution\n                contributions = []\n                for i in valid_in:\n                    for j in valid_out:\n                        delta_v1 = value1_lst[j] - value1_lst[i]\n                        delta_v2 = value2_lst[j] - value2_lst[i]\n                        contribution = weight_v1 * delta_v1 + weight_v2 * delta_v2\n                        contributions.append((contribution, i, j))\n\n                if contributions:\n                    contributions.sort(reverse=True, key=lambda x: x[0])\n                    best_swaps = contributions[:neighborhood_size]\n\n                    # Apply the best swaps\n                    for _, i, j in best_swaps:\n                        new_solution[i] = 0\n                        new_solution[j] = 1\n                        current_weight = current_weight - weight_lst[i] + weight_lst[j]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        This heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 96,
        "algorithm": "The algorithm selects a base solution from the archive using a hybrid approach (60% crowding distance, 40% adaptive weighting), then applies a dynamic neighborhood search where flip probabilities for items are determined by their utility scores and remaining capacity. It ensures feasibility through probabilistic flips and a capacity-aware repair mechanism, prioritizing items with higher utility while balancing weight constraints. The method dynamically adjusts exploration/exploitation by weighting utility and capacity factors, favoring high-impact flips while maintaining solution feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 60% crowding distance, 40% adaptive weighting\n        if np.random.random() < 0.6:\n            # Crowding distance selection\n            sorted_archive = sorted(archive, key=lambda x: (x[1][0], x[1][1]))\n            distances = []\n            for i in range(len(sorted_archive)):\n                if i == 0 or i == len(sorted_archive)-1:\n                    distances.append(float('inf'))\n                else:\n                    left = sorted_archive[i-1][1]\n                    right = sorted_archive[i+1][1]\n                    distances.append((right[0] - left[0]) + (right[1] - left[1]))\n\n            max_distance_idx = np.argmax(distances)\n            base_solution = sorted_archive[max_distance_idx][0].copy()\n        else:\n            # Adaptive weighting selection\n            total_v1 = sum(obj[0] for _, obj in archive)\n            total_v2 = sum(obj[1] for _, obj in archive)\n            avg_v1 = total_v1 / len(archive)\n            avg_v2 = total_v2 / len(archive)\n\n            weighted_scores = []\n            for sol, obj in archive:\n                weight_v1 = 0.6 * (1 - obj[0]/(avg_v1 + 1e-6)) + 0.4 * (obj[1]/(avg_v2 + 1e-6))\n                weight_v2 = 1 - weight_v1\n                weighted_scores.append(weight_v1 * obj[0] + weight_v2 * obj[1])\n\n            max_score_idx = np.argmax(weighted_scores)\n            base_solution = archive[max_score_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate item utility scores\n    utility_scores = []\n    for i in range(len(weight_lst)):\n        if new_solution[i] == 1:\n            utility_scores.append(-weight_lst[i] / (value1_lst[i] + value2_lst[i] + 1e-6))\n        else:\n            utility_scores.append((value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-6))\n\n    # Dynamic flip probability calculation\n    remaining_capacity = capacity - current_weight\n    flip_probabilities = []\n    for i in range(len(weight_lst)):\n        if new_solution[i] == 1:\n            prob = 0.5 * (1 - utility_scores[i] / (max(utility_scores) + 1e-6)) * (1 - weight_lst[i]/(remaining_capacity + capacity + 1e-6))\n        else:\n            prob = 0.5 * (utility_scores[i] / (max(utility_scores) + 1e-6)) * (weight_lst[i]/(remaining_capacity + capacity + 1e-6))\n\n        flip_probabilities.append(min(0.9, max(0.1, prob)))\n\n    # Apply probabilistic flips\n    for i in range(len(weight_lst)):\n        if np.random.random() < flip_probabilities[i]:\n            if new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n            elif new_solution[i] == 1:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n\n    # Capacity-aware repair\n    if current_weight > capacity:\n        excess = current_weight - capacity\n        items_in_solution = np.where(new_solution == 1)[0]\n        if len(items_in_solution) > 0:\n            # Remove items with lowest utility until feasible\n            sorted_items = sorted(items_in_solution, key=lambda x: utility_scores[x])\n            for i in sorted_items:\n                if excess <= 0:\n                    break\n                new_solution[i] = 0\n                excess -= weight_lst[i]\n\n    return new_solution\n\n",
        "score": [
            -0.7251377046776672,
            5.771623075008392
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 60% crowding distance, 40% adaptive weighting\n        if np.random.random() < 0.6:\n            # Crowding distance selection\n            sorted_archive = sorted(archive, key=lambda x: (x[1][0], x[1][1]))\n            distances = []\n            for i in range(len(sorted_archive)):\n                if i == 0 or i == len(sorted_archive)-1:\n                    distances.append(float('inf'))\n                else:\n                    left = sorted_archive[i-1][1]\n                    right = sorted_archive[i+1][1]\n                    distances.append((right[0] - left[0]) + (right[1] - left[1]))\n\n            max_distance_idx = np.argmax(distances)\n            base_solution = sorted_archive[max_distance_idx][0].copy()\n        else:\n            # Adaptive weighting selection\n            total_v1 = sum(obj[0] for _, obj in archive)\n            total_v2 = sum(obj[1] for _, obj in archive)\n            avg_v1 = total_v1 / len(archive)\n            avg_v2 = total_v2 / len(archive)\n\n            weighted_scores = []\n            for sol, obj in archive:\n                weight_v1 = 0.6 * (1 - obj[0]/(avg_v1 + 1e-6)) + 0.4 * (obj[1]/(avg_v2 + 1e-6))\n                weight_v2 = 1 - weight_v1\n                weighted_scores.append(weight_v1 * obj[0] + weight_v2 * obj[1])\n\n            max_score_idx = np.argmax(weighted_scores)\n            base_solution = archive[max_score_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate item utility scores\n    utility_scores = []\n    for i in range(len(weight_lst)):\n        if new_solution[i] == 1:\n            utility_scores.append(-weight_lst[i] / (value1_lst[i] + value2_lst[i] + 1e-6))\n        else:\n            utility_scores.append((value1_lst[i] + value2_lst[i]) / (weight_lst[i] + 1e-6))\n\n    # Dynamic flip probability calculation\n    remaining_capacity = capacity - current_weight\n    flip_probabilities = []\n    for i in range(len(weight_lst)):\n        if new_solution[i] == 1:\n            prob = 0.5 * (1 - utility_scores[i] / (max(utility_scores) + 1e-6)) * (1 - weight_lst[i]/(remaining_capacity + capacity + 1e-6))\n        else:\n            prob = 0.5 * (utility_scores[i] / (max(utility_scores) + 1e-6)) * (weight_lst[i]/(remaining_capacity + capacity + 1e-6))\n\n        flip_probabilities.append(min(0.9, max(0.1, prob)))\n\n    # Apply probabilistic flips\n    for i in range(len(weight_lst)):\n        if np.random.random() < flip_probabilities[i]:\n            if new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                new_solution[i] = 1\n                current_weight += weight_lst[i]\n            elif new_solution[i] == 1:\n                new_solution[i] = 0\n                current_weight -= weight_lst[i]\n\n    # Capacity-aware repair\n    if current_weight > capacity:\n        excess = current_weight - capacity\n        items_in_solution = np.where(new_solution == 1)[0]\n        if len(items_in_solution) > 0:\n            # Remove items with lowest utility until feasible\n            sorted_items = sorted(items_in_solution, key=lambda x: utility_scores[x])\n            for i in sorted_items:\n                if excess <= 0:\n                    break\n                new_solution[i] = 0\n                excess -= weight_lst[i]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        This heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 97,
        "algorithm": "This heuristic selects a base solution from the archive using a hybrid crowding-distance and value-based approach (60/40 split), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor while ensuring feasibility. It prioritizes solutions with higher crowding distances on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity, using momentum in improvement calculations and adaptive temperature for probabilistic flips. The algorithm balances exploration and exploitation by favoring high-contribution flips while maintaining feasibility through careful weight checks.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 60% crowding distance, 40% value-based selection\n        if np.random.random() < 0.6:\n            # Crowding distance selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate crowding distances\n                sorted_front = sorted(pareto_front, key=lambda x: x[1][0])\n                crowding_distances = [0.0] * len(sorted_front)\n                for m in range(len(sorted_front[0][1])):\n                    sorted_front.sort(key=lambda x: x[1][m])\n                    crowding_distances[0] = float('inf')\n                    crowding_distances[-1] = float('inf')\n                    for i in range(1, len(sorted_front)-1):\n                        crowding_distances[i] += (sorted_front[i+1][1][m] - sorted_front[i-1][1][m]) / (sorted_front[-1][1][m] - sorted_front[0][1][m] + 1e-6)\n\n                max_distance_idx = np.argmax(crowding_distances)\n                base_solution = sorted_front[max_distance_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.6 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.4 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.7  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8836812082636099,
            0.5478502213954926
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 60% crowding distance, 40% value-based selection\n        if np.random.random() < 0.6:\n            # Crowding distance selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate crowding distances\n                sorted_front = sorted(pareto_front, key=lambda x: x[1][0])\n                crowding_distances = [0.0] * len(sorted_front)\n                for m in range(len(sorted_front[0][1])):\n                    sorted_front.sort(key=lambda x: x[1][m])\n                    crowding_distances[0] = float('inf')\n                    crowding_distances[-1] = float('inf')\n                    for i in range(1, len(sorted_front)-1):\n                        crowding_distances[i] += (sorted_front[i+1][1][m] - sorted_front[i-1][1][m]) / (sorted_front[-1][1][m] - sorted_front[0][1][m] + 1e-6)\n\n                max_distance_idx = np.argmax(crowding_distances)\n                base_solution = sorted_front[max_distance_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.6 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.4 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.7  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 5 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThis heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and weighted crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min(obj[0] for _, obj in pareto_front) + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto ranking with crowding-distance selection to prioritize diverse, high-quality solutions from the archive, then applies an adaptive cluster-based local search that dynamically adjusts flip cluster sizes based on weight utilization and objective weights, ensuring feasibility through precomputed item contributions and move validation. It balances exploration/exploitation by flipping items within clusters while favoring moves that improve both objectives proportionally, falling back to random valid flips if no improvement is found.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max(obj[0] for _, obj in pareto_front) - min(obj[0] for _, obj in pareto_front) + 1e-6) + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max(obj[1] for _, obj in pareto_front) - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                crowding_distances.append(dist)\n\n            # Select solution with highest crowding distance\n            max_dist_idx = np.argmax(crowding_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive cluster-based item flips\n    cluster_size = max(1, int(len(new_solution) * 0.1 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            **Keywords**: Adaptive selection, hybrid local search, Pareto-aware, dynamic feasibility\n\n**Advice**:\n1. Use weighted crowding-distance to select high-diversity solutions.\n2. Hybridize local search with adaptive subset flips and objective-weighted swaps.\n3. Dynamically adjust neighborhood size based on solution quality.\n4. Precompute item contributions and validate feasibility greedily.\n\n**Avoid**:\n- Static operators (fixed-size swaps).\n- Random dominance-based selection; prefer Pareto-ranked candidates.\n- Ignoring weight constraints; always validate feasibility.\n\n**Explanation**: This approach balances exploration and exploitation by adaptively selecting solutions and using hybrid, dynamic operators to escape local optima while ensuring feasibility. *(Word count: 50)*\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 98,
        "algorithm": "The algorithm selects a base solution from the archive using a hybrid Pareto front approach with weighted crowding distance to prioritize diverse, high-contribution solutions. It then applies an adaptive hybrid local search that dynamically adjusts neighborhood size based on remaining capacity and uses momentum-based weighted objective improvements to evaluate potential flips, considering both individual item flips and cluster-based swaps while ensuring feasibility through precomputed item contributions. The search includes probabilistic exploration of non-improving moves with adaptive temperature control.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: Pareto front with weighted crowding distance\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            min_v1 = min(obj[0] for _, obj in pareto_front)\n            min_v2 = min(obj[1] for _, obj in pareto_front)\n\n            weighted_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min_v1 + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min_v2 + 1e-6)\n\n                # Calculate hypervolume contribution\n                left_area = left[0] * left[1]\n                right_area = right[0] * right[1]\n                current_area = obj[0] * obj[1]\n                contribution = (current_area - left_area) + (right_area - current_area)\n\n                # Combine crowding distance and hypervolume contribution\n                weighted_dist = 0.6 * (0.5 * dist_v1 + 0.5 * dist_v2) + 0.4 * contribution\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate dynamic objective weights with momentum\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    norm_v1 = total_v1 / (np.sum(value1_lst) + 1e-6)\n    norm_v2 = total_v2 / (np.sum(value2_lst) + 1e-6)\n    weight_v1 = 0.7 * norm_v2 + 0.3 * norm_v1\n    weight_v2 = 1 - weight_v1\n\n    # Dynamic neighborhood size adjustment\n    total_items = len(new_solution)\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (remaining_capacity / capacity) *\n                        (1 - (current_weight / capacity)))))\n\n    # Precompute potential contributions for feasibility validation\n    valid_indices = []\n    contributions = []\n    for i in range(total_items):\n        if (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or \\\n           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity):\n            valid_indices.append(i)\n            potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n            potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n            contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n            contributions.append(contribution)\n\n    if not valid_indices:\n        return new_solution\n\n    # Hybrid local search with momentum-based improvement\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.8\n\n    # Consider both single flips and cluster-based swaps\n    neighborhood_indices = np.random.choice(valid_indices, size=min(neighborhood_size, len(valid_indices)), replace=False)\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Momentum-based weighted improvement\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + \\\n                      (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Adaptive temperature probabilistic flip\n        temperature = 1 - (best_improvement / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n        if sum(contributions) > 0 and temperature > 0:\n            probabilities = [c / (sum(contributions) * temperature) for c in contributions]\n            probabilities = [p / sum(probabilities) for p in probabilities]\n            idx = np.random.choice(valid_indices, p=probabilities)\n            new_solution[idx] = 1 - new_solution[idx]\n        elif valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.91141436838077,
            1.251850038766861
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: Pareto front with weighted crowding distance\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            min_v1 = min(obj[0] for _, obj in pareto_front)\n            min_v2 = min(obj[1] for _, obj in pareto_front)\n\n            weighted_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min_v1 + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min_v2 + 1e-6)\n\n                # Calculate hypervolume contribution\n                left_area = left[0] * left[1]\n                right_area = right[0] * right[1]\n                current_area = obj[0] * obj[1]\n                contribution = (current_area - left_area) + (right_area - current_area)\n\n                # Combine crowding distance and hypervolume contribution\n                weighted_dist = 0.6 * (0.5 * dist_v1 + 0.5 * dist_v2) + 0.4 * contribution\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate dynamic objective weights with momentum\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    norm_v1 = total_v1 / (np.sum(value1_lst) + 1e-6)\n    norm_v2 = total_v2 / (np.sum(value2_lst) + 1e-6)\n    weight_v1 = 0.7 * norm_v2 + 0.3 * norm_v1\n    weight_v2 = 1 - weight_v1\n\n    # Dynamic neighborhood size adjustment\n    total_items = len(new_solution)\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (remaining_capacity / capacity) *\n                        (1 - (current_weight / capacity)))))\n\n    # Precompute potential contributions for feasibility validation\n    valid_indices = []\n    contributions = []\n    for i in range(total_items):\n        if (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or \\\n           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity):\n            valid_indices.append(i)\n            potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n            potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n            contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n            contributions.append(contribution)\n\n    if not valid_indices:\n        return new_solution\n\n    # Hybrid local search with momentum-based improvement\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.8\n\n    # Consider both single flips and cluster-based swaps\n    neighborhood_indices = np.random.choice(valid_indices, size=min(neighborhood_size, len(valid_indices)), replace=False)\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Momentum-based weighted improvement\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + \\\n                      (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Adaptive temperature probabilistic flip\n        temperature = 1 - (best_improvement / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n        if sum(contributions) > 0 and temperature > 0:\n            probabilities = [c / (sum(contributions) * temperature) for c in contributions]\n            probabilities = [p / sum(probabilities) for p in probabilities]\n            idx = np.random.choice(valid_indices, p=probabilities)\n            new_solution[idx] = 1 - new_solution[idx]\n        elif valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 6 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThis heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm first identifies the Pareto front from the archive and selects a base solution using a weighted crowding-distance metric to prioritize high-diversity solutions. It then applies a hybrid local search that dynamically adjusts the neighborhood size based on the solution's weight utilization and objective weights, evaluating flips within the neighborhood while ensuring feasibility. If no improving move is found, it performs a probabilistic flip based on potential weighted contributions, dynamically adjusting objective weights to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Identify Pareto front\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            min_v1 = min(obj[0] for _, obj in pareto_front)\n            min_v2 = min(obj[1] for _, obj in pareto_front)\n\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min_v1 + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min_v2 + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto front clustering with dynamic objective-weighted cluster flips, prioritizing solutions based on crowding distance and hypervolume contributions while adaptively adjusting flip probabilities and cluster sizes to balance exploration and exploitation, using momentum to emphasize high-potential regions of the search space. Key design features include weighted improvement calculations, adaptive cluster sizing, and probabilistic flips with temperature-based selection to ensure feasible neighbor solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster solutions based on Pareto dominance and crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest combined crowding distance and hypervolume contribution\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster flip with momentum\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization and solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.3 + 0.2 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.9  # Higher momentum for more aggressive search\n\n    # Evaluate cluster flips with momentum\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm first filters non-dominated solutions from the archive and selects a base solution using a weighted crowding-distance metric that balances Pareto dominance and diversity. It then applies a hybrid local search with dynamic neighborhood sizes (20-50% of items) based on weight utilization, using objective-weighted improvement criteria to prioritize flips that enhance both objectives proportionally. If no improvement is found, it performs a probabilistic flip based on potential contributions while ensuring feasibility. The method dynamically adjusts objective weights to balance exploration and exploitation, generating high-quality neighbors that dominate or complement existing solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with weighted crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic neighborhood size based on weight utilization\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 4: Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 5: Hybrid local search with cluster flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering both objectives\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm selects a base solution from the archive using a weighted crowding-distance metric to prioritize high-diversity solutions, then applies a hybrid local search that adaptively flips 20-50% of items based on remaining capacity, with objective-weighted swaps and momentum-based improvement calculations to ensure feasibility and balance exploration/exploitation. The key variables are `neighborhood_size`, `weight_v1/v2`, and `momentum`, which dynamically adjust the search intensity and balance between objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution using weighted crowding-distance metric\n        crowding_distances = []\n        sorted_archive = sorted(archive, key=lambda x: (x[1][0], x[1][1]))\n\n        for i in range(len(sorted_archive)):\n            if i == 0 or i == len(sorted_archive) - 1:\n                crowding_distances.append(float('inf'))\n            else:\n                dist_v1 = sorted_archive[i+1][1][0] - sorted_archive[i-1][1][0]\n                dist_v2 = sorted_archive[i+1][1][1] - sorted_archive[i-1][1][1]\n                crowding_distances.append(dist_v1 + dist_v2)\n\n        max_dist_idx = np.argmax(crowding_distances)\n        base_solution = sorted_archive[max_dist_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Dynamic neighborhood size (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, min(total_items, int(total_items * (0.2 + 0.3 * (remaining_capacity / capacity)))))\n\n    if neighborhood_size > 0:\n        # Adaptive subset flips with objective-weighted swaps\n        subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate objective weights\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = total_v1 / (total_v1 + total_v2 + 1e-6)\n        weight_v2 = total_v2 / (total_v1 + total_v2 + 1e-6)\n\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.7  # Higher momentum for more aggressive search\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                temperature = 1 - (remaining_capacity / capacity)\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: **Keywords**: Adaptive selection, hybrid local search, Pareto-aware, dynamic feasibility\n\n**Advice**:\n1. Use weighted crowding-distance to select high-diversity solutions.\n2. Hybridize local search with adaptive subset flips and objective-weighted swaps.\n3. Dynamically adjust neighborhood size based on solution quality.\n4. Precompute item contributions and validate feasibility greedily.\n\n**Avoid**:\n- Static operators (fixed-size swaps).\n- Random dominance-based selection; prefer Pareto-ranked candidates.\n- Ignoring weight constraints; always validate feasibility.\n\n**Explanation**: This approach balances exploration and exploitation by adaptively selecting solutions and using hybrid, dynamic operators to escape local optima while ensuring feasibility. *(Word count: 50)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 99,
        "algorithm": "The algorithm selects a base solution from the archive using Pareto dominance and hypervolume contribution, then applies an adaptive local search with dynamic neighborhood sizes and objective-weighted item flips, balancing exploration and exploitation while ensuring feasibility through precomputed item contributions and greedy feasibility validation. The solution prioritizes high-contribution items, dynamically adjusts neighborhood size based on remaining capacity, and uses weighted flips/swaps to improve both objectives, with fallback probabilistic selection when no immediate improvements are found.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Identify Pareto front and select base solution\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        # Calculate hypervolume contributions\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        # Select solution with highest hypervolume contribution\n        max_contrib_idx = np.argmax(contributions)\n        base_solution = pareto_front[max_contrib_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Adaptive local search with dynamic neighborhood\n    total_items = len(new_solution)\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (remaining_capacity / capacity))))\n\n    # Calculate objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Precompute item contributions\n    item_contributions = np.zeros(total_items)\n    for i in range(total_items):\n        if new_solution[i] == 1:\n            item_contributions[i] = -(weight_v1 * value1_lst[i] + weight_v2 * value2_lst[i])\n        else:\n            item_contributions[i] = weight_v1 * value1_lst[i] + weight_v2 * value2_lst[i]\n\n    # Step 3: Greedy feasibility validation and selection\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Consider both flipping and swapping\n    for _ in range(neighborhood_size):\n        # Flip operation\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = np.random.choice(flip_candidates)\n            if current_weight - weight_lst[flip_idx] <= capacity:\n                improvement = item_contributions[flip_idx]\n                if improvement > best_improvement:\n                    best_improvement = improvement\n                    temp_solution = new_solution.copy()\n                    temp_solution[flip_idx] = 0\n                    best_candidate = temp_solution\n\n        # Swap operation\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = np.random.choice(out_items)\n            in_idx = np.random.choice(in_items)\n            if current_weight - weight_lst[out_idx] + weight_lst[in_idx] <= capacity:\n                improvement = item_contributions[out_idx] + item_contributions[in_idx]\n                if improvement > best_improvement:\n                    best_improvement = improvement\n                    temp_solution = new_solution.copy()\n                    temp_solution[out_idx] = 0\n                    temp_solution[in_idx] = 1\n                    best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Fallback: probabilistic flip based on contributions\n        valid_indices = [i for i in range(total_items) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = [item_contributions[i] for i in valid_indices]\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = np.random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.6403950929270407,
            8.254030913114548
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Identify Pareto front and select base solution\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        # Calculate hypervolume contributions\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        # Select solution with highest hypervolume contribution\n        max_contrib_idx = np.argmax(contributions)\n        base_solution = pareto_front[max_contrib_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Adaptive local search with dynamic neighborhood\n    total_items = len(new_solution)\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (remaining_capacity / capacity))))\n\n    # Calculate objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Precompute item contributions\n    item_contributions = np.zeros(total_items)\n    for i in range(total_items):\n        if new_solution[i] == 1:\n            item_contributions[i] = -(weight_v1 * value1_lst[i] + weight_v2 * value2_lst[i])\n        else:\n            item_contributions[i] = weight_v1 * value1_lst[i] + weight_v2 * value2_lst[i]\n\n    # Step 3: Greedy feasibility validation and selection\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Consider both flipping and swapping\n    for _ in range(neighborhood_size):\n        # Flip operation\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = np.random.choice(flip_candidates)\n            if current_weight - weight_lst[flip_idx] <= capacity:\n                improvement = item_contributions[flip_idx]\n                if improvement > best_improvement:\n                    best_improvement = improvement\n                    temp_solution = new_solution.copy()\n                    temp_solution[flip_idx] = 0\n                    best_candidate = temp_solution\n\n        # Swap operation\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = np.random.choice(out_items)\n            in_idx = np.random.choice(in_items)\n            if current_weight - weight_lst[out_idx] + weight_lst[in_idx] <= capacity:\n                improvement = item_contributions[out_idx] + item_contributions[in_idx]\n                if improvement > best_improvement:\n                    best_improvement = improvement\n                    temp_solution = new_solution.copy()\n                    temp_solution[out_idx] = 0\n                    temp_solution[in_idx] = 1\n                    best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Fallback: probabilistic flip based on contributions\n        valid_indices = [i for i in range(total_items) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = [item_contributions[i] for i in valid_indices]\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = np.random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThis heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects the most promising solution from the archive (based on average value-to-weight density) and applies a hybrid local search strategy that prioritizes swapping low-value items for high-value ones while ensuring feasibility through adaptive perturbations. If no swaps are feasible, it randomly removes items to free up capacity. The approach balances exploration (via adaptive swaps) and exploitation (via value-to-weight ratios) to improve both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution from the archive\n    # Calculate the density (value/weight) for each objective\n    densities = []\n    for sol, (v1, v2) in archive:\n        included = sol == 1\n        total_weight = np.sum(weight_lst[included])\n        if total_weight == 0:\n            density1 = density2 = 0\n        else:\n            density1 = np.sum(value1_lst[included]) / total_weight\n            density2 = np.sum(value2_lst[included]) / total_weight\n        densities.append((density1 + density2) / 2)  # Average density\n\n    # Select the solution with the highest density (most promising for improvement)\n    best_idx = np.argmax(densities)\n    base_solution = archive[best_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Apply novel local search operator\n    # Hybrid strategy: adaptive item swaps and perturbations\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Adaptive swap: replace a low-value item with a high-value item\n        # Calculate value-to-weight ratio for included items\n        included_ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_ratios)]\n\n        # Calculate value-to-weight ratio for excluded items\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        # Check if swap is feasible\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n        else:\n            # If swap is not feasible, perform a perturbation\n            # Randomly flip a small number of items to find a feasible solution\n            max_perturbations = min(3, len(included_items))\n            for _ in range(max_perturbations):\n                candidate = random.choice(included_items)\n                if current_weight - weight_lst[candidate] <= capacity:\n                    new_solution[candidate] = 0\n                    current_weight -= weight_lst[candidate]\n                    break\n\n    # If no items are included, add the best item if possible\n    elif len(included_items) == 0 and len(excluded_items) > 0:\n        # Add the item with the highest value-to-weight ratio\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        if weight_lst[best_excluded] <= capacity:\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            **Keywords**: Adaptive selection, hybrid local search, Pareto-aware, dynamic feasibility\n\n**Advice**:\n1. Use weighted crowding-distance to select high-diversity solutions.\n2. Hybridize local search with adaptive subset flips and objective-weighted swaps.\n3. Dynamically adjust neighborhood size based on solution quality.\n4. Precompute item contributions and validate feasibility greedily.\n\n**Avoid**:\n- Static operators (fixed-size swaps).\n- Random dominance-based selection; prefer Pareto-ranked candidates.\n- Ignoring weight constraints; always validate feasibility.\n\n**Explanation**: This approach balances exploration and exploitation by adaptively selecting solutions and using hybrid, dynamic operators to escape local optima while ensuring feasibility. *(Word count: 50)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 100,
        "algorithm": "The algorithm selects a diverse solution from the archive using weighted crowding-distance, then applies an objective-aware local search that dynamically flips item subsets based on their weighted contributions (prioritizing high-contribution items while ensuring feasibility). The subset size adapts to remaining capacity, and flips are biased toward adding excluded high-value items and removing low-value included items, balancing both objectives dynamically.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate weighted crowding distance for selection\n    solutions = [sol for sol, _ in archive]\n    objectives = np.array([obj for _, obj in archive])\n\n    # Normalize objectives\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    if max_v1 == 0:\n        max_v1 = 1\n    if max_v2 == 0:\n        max_v2 = 1\n    normalized = objectives / [max_v1, max_v2]\n\n    # Calculate crowding distances\n    crowding = np.zeros(len(archive))\n    for i in range(2):  # For each objective\n        sorted_idx = np.argsort(normalized[:, i])\n        crowding[sorted_idx[0]] = crowding[sorted_idx[-1]] = float('inf')\n        for j in range(1, len(archive)-1):\n            crowding[sorted_idx[j]] += (normalized[sorted_idx[j+1], i] - normalized[sorted_idx[j-1], i])\n\n    # Select solution with highest crowding distance (most diverse)\n    base_idx = np.argmax(crowding)\n    base_solution = solutions[base_idx].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current weight and remaining capacity\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Precompute item contributions\n    included = new_solution == 1\n    excluded = new_solution == 0\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[included])\n    total_v2 = np.sum(value2_lst[included])\n    if total_v1 + total_v2 > 0:\n        weight_v1 = total_v1 / (total_v1 + total_v2)\n        weight_v2 = total_v2 / (total_v1 + total_v2)\n    else:\n        weight_v1 = weight_v2 = 0.5\n\n    # Dynamic subset size based on remaining capacity\n    subset_size = max(1, min(5, int(remaining_capacity / (np.mean(weight_lst) + 1e-6))))\n\n    # Objective-aware subset flipping\n    if subset_size > 0:\n        # Calculate weighted contributions for each item\n        contributions = weight_v1 * value1_lst + weight_v2 * value2_lst\n        contributions /= (weight_lst + 1e-6)  # Normalize by weight\n\n        # Priority for flipping: excluded items with high contribution, included items with low contribution\n        flip_priority = np.zeros(len(new_solution))\n        flip_priority[included] = -contributions[included]  # Lower priority to keep good items\n        flip_priority[excluded] = contributions[excluded]  # Higher priority to add good items\n\n        # Select top subset_size items to flip\n        flip_indices = np.argsort(flip_priority)[-subset_size:]\n\n        # Apply flips while maintaining feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 0:  # Adding item\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n            else:  # Removing item\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.6623106462270645,
            5.4831918478012085
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Calculate weighted crowding distance for selection\n    solutions = [sol for sol, _ in archive]\n    objectives = np.array([obj for _, obj in archive])\n\n    # Normalize objectives\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    if max_v1 == 0:\n        max_v1 = 1\n    if max_v2 == 0:\n        max_v2 = 1\n    normalized = objectives / [max_v1, max_v2]\n\n    # Calculate crowding distances\n    crowding = np.zeros(len(archive))\n    for i in range(2):  # For each objective\n        sorted_idx = np.argsort(normalized[:, i])\n        crowding[sorted_idx[0]] = crowding[sorted_idx[-1]] = float('inf')\n        for j in range(1, len(archive)-1):\n            crowding[sorted_idx[j]] += (normalized[sorted_idx[j+1], i] - normalized[sorted_idx[j-1], i])\n\n    # Select solution with highest crowding distance (most diverse)\n    base_idx = np.argmax(crowding)\n    base_solution = solutions[base_idx].copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current weight and remaining capacity\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Precompute item contributions\n    included = new_solution == 1\n    excluded = new_solution == 0\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[included])\n    total_v2 = np.sum(value2_lst[included])\n    if total_v1 + total_v2 > 0:\n        weight_v1 = total_v1 / (total_v1 + total_v2)\n        weight_v2 = total_v2 / (total_v1 + total_v2)\n    else:\n        weight_v1 = weight_v2 = 0.5\n\n    # Dynamic subset size based on remaining capacity\n    subset_size = max(1, min(5, int(remaining_capacity / (np.mean(weight_lst) + 1e-6))))\n\n    # Objective-aware subset flipping\n    if subset_size > 0:\n        # Calculate weighted contributions for each item\n        contributions = weight_v1 * value1_lst + weight_v2 * value2_lst\n        contributions /= (weight_lst + 1e-6)  # Normalize by weight\n\n        # Priority for flipping: excluded items with high contribution, included items with low contribution\n        flip_priority = np.zeros(len(new_solution))\n        flip_priority[included] = -contributions[included]  # Lower priority to keep good items\n        flip_priority[excluded] = contributions[excluded]  # Higher priority to add good items\n\n        # Select top subset_size items to flip\n        flip_indices = np.argsort(flip_priority)[-subset_size:]\n\n        # Apply flips while maintaining feasibility\n        for idx in flip_indices:\n            if new_solution[idx] == 0:  # Adding item\n                if current_weight + weight_lst[idx] <= capacity:\n                    new_solution[idx] = 1\n                    current_weight += weight_lst[idx]\n            else:  # Removing item\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm combines Pareto front clustering with dynamic objective-weighted cluster flips, prioritizing solutions based on crowding distance and hypervolume contributions while adaptively adjusting flip probabilities and cluster sizes to balance exploration and exploitation, using momentum to emphasize high-potential regions of the search space. Key design features include weighted improvement calculations, adaptive cluster sizing, and probabilistic flips with temperature-based selection to ensure feasible neighbor solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster solutions based on Pareto dominance and crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest combined crowding distance and hypervolume contribution\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster flip with momentum\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization and solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.3 + 0.2 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.9  # Higher momentum for more aggressive search\n\n    # Evaluate cluster flips with momentum\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects the most promising solution from the archive (based on average value-to-weight density) and applies a hybrid local search strategy that prioritizes swapping low-value items for high-value ones while ensuring feasibility through adaptive perturbations. If no swaps are feasible, it randomly removes items to free up capacity. The approach balances exploration (via adaptive swaps) and exploitation (via value-to-weight ratios) to improve both objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution from the archive\n    # Calculate the density (value/weight) for each objective\n    densities = []\n    for sol, (v1, v2) in archive:\n        included = sol == 1\n        total_weight = np.sum(weight_lst[included])\n        if total_weight == 0:\n            density1 = density2 = 0\n        else:\n            density1 = np.sum(value1_lst[included]) / total_weight\n            density2 = np.sum(value2_lst[included]) / total_weight\n        densities.append((density1 + density2) / 2)  # Average density\n\n    # Select the solution with the highest density (most promising for improvement)\n    best_idx = np.argmax(densities)\n    base_solution = archive[best_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Apply novel local search operator\n    # Hybrid strategy: adaptive item swaps and perturbations\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Adaptive swap: replace a low-value item with a high-value item\n        # Calculate value-to-weight ratio for included items\n        included_ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_ratios)]\n\n        # Calculate value-to-weight ratio for excluded items\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        # Check if swap is feasible\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n        else:\n            # If swap is not feasible, perform a perturbation\n            # Randomly flip a small number of items to find a feasible solution\n            max_perturbations = min(3, len(included_items))\n            for _ in range(max_perturbations):\n                candidate = random.choice(included_items)\n                if current_weight - weight_lst[candidate] <= capacity:\n                    new_solution[candidate] = 0\n                    current_weight -= weight_lst[candidate]\n                    break\n\n    # If no items are included, add the best item if possible\n    elif len(included_items) == 0 and len(excluded_items) > 0:\n        # Add the item with the highest value-to-weight ratio\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        if weight_lst[best_excluded] <= capacity:\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            **Keywords**: Adaptive selection, hybrid local search, Pareto-aware, dynamic feasibility\n\n**Advice**:\n1. Use weighted crowding-distance to select high-diversity solutions.\n2. Hybridize local search with adaptive subset flips and objective-weighted swaps.\n3. Dynamically adjust neighborhood size based on solution quality.\n4. Precompute item contributions and validate feasibility greedily.\n\n**Avoid**:\n- Static operators (fixed-size swaps).\n- Random dominance-based selection; prefer Pareto-ranked candidates.\n- Ignoring weight constraints; always validate feasibility.\n\n**Explanation**: This approach balances exploration and exploitation by adaptively selecting solutions and using hybrid, dynamic operators to escape local optima while ensuring feasibility. *(Word count: 50)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 101,
        "algorithm": "The algorithm combines adaptive Pareto-based selection with a hybrid local search strategy: it first selects a promising solution from the archive using weighted crowding-distance to balance exploration and exploitation, then applies an objective-aware knapsack perturbation that dynamically prioritizes items based on their marginal contributions to both objectives while ensuring feasibility through greedy validation. The method balances exploration through dynamic neighborhood sizes and exploitation through marginal contribution maximization.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution using weighted crowding-distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        solution_qualities = []\n        for sol, _ in pareto_front:\n            included = sol == 1\n            total_v1 = np.sum(value1_lst[included])\n            total_v2 = np.sum(value2_lst[included])\n            solution_qualities.append((total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n\n        weighted_scores = crowding_distances * np.array(solution_qualities)\n        selected_idx = np.argmax(weighted_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Apply objective-aware knapsack perturbation\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate marginal contributions for each item\n    marginal_v1 = np.zeros(len(new_solution))\n    marginal_v2 = np.zeros(len(new_solution))\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            marginal_v1[i] = -value1_lst[i]\n            marginal_v2[i] = -value2_lst[i]\n        else:\n            marginal_v1[i] = value1_lst[i]\n            marginal_v2[i] = value2_lst[i]\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    obj_weight1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    obj_weight2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Calculate combined marginal contributions\n    combined_contributions = obj_weight1 * marginal_v1 + obj_weight2 * marginal_v2\n\n    # Select top-k items with highest marginal contributions\n    k = max(1, int(len(new_solution) * 0.1))\n    candidate_indices = np.argsort(combined_contributions)[-k:]\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Evaluate all possible flips of the selected candidates\n    for idx in candidate_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        improvement = combined_contributions[idx]\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a greedy perturbation\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = np.argmax(combined_contributions[valid_indices])\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8642370399850805,
            1.0498494505882263
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution using weighted crowding-distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        solution_qualities = []\n        for sol, _ in pareto_front:\n            included = sol == 1\n            total_v1 = np.sum(value1_lst[included])\n            total_v2 = np.sum(value2_lst[included])\n            solution_qualities.append((total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n\n        weighted_scores = crowding_distances * np.array(solution_qualities)\n        selected_idx = np.argmax(weighted_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Apply objective-aware knapsack perturbation\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    # Calculate marginal contributions for each item\n    marginal_v1 = np.zeros(len(new_solution))\n    marginal_v2 = np.zeros(len(new_solution))\n    for i in range(len(new_solution)):\n        if new_solution[i] == 1:\n            marginal_v1[i] = -value1_lst[i]\n            marginal_v2[i] = -value2_lst[i]\n        else:\n            marginal_v1[i] = value1_lst[i]\n            marginal_v2[i] = value2_lst[i]\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    obj_weight1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    obj_weight2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Calculate combined marginal contributions\n    combined_contributions = obj_weight1 * marginal_v1 + obj_weight2 * marginal_v2\n\n    # Select top-k items with highest marginal contributions\n    k = max(1, int(len(new_solution) * 0.1))\n    candidate_indices = np.argsort(combined_contributions)[-k:]\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Evaluate all possible flips of the selected candidates\n    for idx in candidate_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        improvement = combined_contributions[idx]\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a greedy perturbation\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = np.argmax(combined_contributions[valid_indices])\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects the most promising solution from the archive (based on average value-to-weight density) and applies a hybrid local search strategy that prioritizes swapping low-value items for high-value ones while ensuring feasibility through adaptive perturbations. If no swaps are feasible, it randomly removes items to free up capacity. The approach balances exploration (via adaptive swaps) and exploitation (via value-to-weight ratios) to improve both objectives.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution from the archive\n    # Calculate the density (value/weight) for each objective\n    densities = []\n    for sol, (v1, v2) in archive:\n        included = sol == 1\n        total_weight = np.sum(weight_lst[included])\n        if total_weight == 0:\n            density1 = density2 = 0\n        else:\n            density1 = np.sum(value1_lst[included]) / total_weight\n            density2 = np.sum(value2_lst[included]) / total_weight\n        densities.append((density1 + density2) / 2)  # Average density\n\n    # Select the solution with the highest density (most promising for improvement)\n    best_idx = np.argmax(densities)\n    base_solution = archive[best_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Apply novel local search operator\n    # Hybrid strategy: adaptive item swaps and perturbations\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Adaptive swap: replace a low-value item with a high-value item\n        # Calculate value-to-weight ratio for included items\n        included_ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_ratios)]\n\n        # Calculate value-to-weight ratio for excluded items\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        # Check if swap is feasible\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n        else:\n            # If swap is not feasible, perform a perturbation\n            # Randomly flip a small number of items to find a feasible solution\n            max_perturbations = min(3, len(included_items))\n            for _ in range(max_perturbations):\n                candidate = random.choice(included_items)\n                if current_weight - weight_lst[candidate] <= capacity:\n                    new_solution[candidate] = 0\n                    current_weight -= weight_lst[candidate]\n                    break\n\n    # If no items are included, add the best item if possible\n    elif len(included_items) == 0 and len(excluded_items) > 0:\n        # Add the item with the highest value-to-weight ratio\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        if weight_lst[best_excluded] <= capacity:\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 102,
        "algorithm": "The algorithm selects the solution with the highest hypervolume contribution from the archive, then applies a multi-objective local search that prioritizes high-value items (using a score combining both objectives and weight) while ensuring feasibility through dynamic capacity adjustments and probabilistic flips to escape local optima. It iteratively improves the solution by flipping excluded items with the best potential and removing worst-performing included items, with a final feasibility check to enforce strict capacity constraints.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the solution with highest hypervolume contribution\n    def hypervolume(v1, v2):\n        return v1 * v2  # Simple hypervolume approximation\n\n    volumes = [hypervolume(v1, v2) for _, (v1, v2) in archive]\n    best_idx = np.argmax(volumes)\n    base_solution = archive[best_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Multi-objective local search with dynamic perturbations\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n    current_weight = np.sum(weight_lst[included])\n\n    if len(included) > 0:\n        # Calculate multi-objective dominance scores\n        included_scores = (value1_lst[included] * value2_lst[included]) / (weight_lst[included] ** 0.5)\n        worst_item = included[np.argmin(included_scores)]\n\n        # Probabilistic flip to escape local optima\n        if random.random() < 0.3:\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    if len(excluded) > 0:\n        # Calculate potential improvements\n        excluded_scores = (value1_lst[excluded] * value2_lst[excluded]) / (weight_lst[excluded] ** 0.5)\n        best_candidate = excluded[np.argmax(excluded_scores)]\n\n        # Dynamic capacity adjustment\n        if current_weight + weight_lst[best_candidate] <= capacity * 1.1:  # Allow slight overcapacity\n            new_solution[best_candidate] = 1\n\n    # Final feasibility check\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    while current_weight > capacity:\n        # Remove items with lowest marginal contribution\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        marginal_contrib = (value1_lst[included] + value2_lst[included]) / weight_lst[included]\n        worst_item = included[np.argmin(marginal_contrib)]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "score": [
            -0.22728182771860844,
            6.331079989671707
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the solution with highest hypervolume contribution\n    def hypervolume(v1, v2):\n        return v1 * v2  # Simple hypervolume approximation\n\n    volumes = [hypervolume(v1, v2) for _, (v1, v2) in archive]\n    best_idx = np.argmax(volumes)\n    base_solution = archive[best_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Multi-objective local search with dynamic perturbations\n    included = np.where(new_solution == 1)[0]\n    excluded = np.where(new_solution == 0)[0]\n    current_weight = np.sum(weight_lst[included])\n\n    if len(included) > 0:\n        # Calculate multi-objective dominance scores\n        included_scores = (value1_lst[included] * value2_lst[included]) / (weight_lst[included] ** 0.5)\n        worst_item = included[np.argmin(included_scores)]\n\n        # Probabilistic flip to escape local optima\n        if random.random() < 0.3:\n            new_solution[worst_item] = 0\n            current_weight -= weight_lst[worst_item]\n\n    if len(excluded) > 0:\n        # Calculate potential improvements\n        excluded_scores = (value1_lst[excluded] * value2_lst[excluded]) / (weight_lst[excluded] ** 0.5)\n        best_candidate = excluded[np.argmax(excluded_scores)]\n\n        # Dynamic capacity adjustment\n        if current_weight + weight_lst[best_candidate] <= capacity * 1.1:  # Allow slight overcapacity\n            new_solution[best_candidate] = 1\n\n    # Final feasibility check\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    while current_weight > capacity:\n        # Remove items with lowest marginal contribution\n        included = np.where(new_solution == 1)[0]\n        if len(included) == 0:\n            break\n        marginal_contrib = (value1_lst[included] + value2_lst[included]) / weight_lst[included]\n        worst_item = included[np.argmin(marginal_contrib)]\n        new_solution[worst_item] = 0\n        current_weight -= weight_lst[worst_item]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects the most promising solution from the archive (based on average value-to-weight density) and applies a hybrid local search strategy that prioritizes swapping low-value items for high-value ones while ensuring feasibility through adaptive perturbations. If no swaps are feasible, it randomly removes items to free up capacity. The approach balances exploration (via adaptive swaps) and exploitation (via value-to-weight ratios) to improve both objectives.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution from the archive\n    # Calculate the density (value/weight) for each objective\n    densities = []\n    for sol, (v1, v2) in archive:\n        included = sol == 1\n        total_weight = np.sum(weight_lst[included])\n        if total_weight == 0:\n            density1 = density2 = 0\n        else:\n            density1 = np.sum(value1_lst[included]) / total_weight\n            density2 = np.sum(value2_lst[included]) / total_weight\n        densities.append((density1 + density2) / 2)  # Average density\n\n    # Select the solution with the highest density (most promising for improvement)\n    best_idx = np.argmax(densities)\n    base_solution = archive[best_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Apply novel local search operator\n    # Hybrid strategy: adaptive item swaps and perturbations\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Adaptive swap: replace a low-value item with a high-value item\n        # Calculate value-to-weight ratio for included items\n        included_ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_ratios)]\n\n        # Calculate value-to-weight ratio for excluded items\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        # Check if swap is feasible\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n        else:\n            # If swap is not feasible, perform a perturbation\n            # Randomly flip a small number of items to find a feasible solution\n            max_perturbations = min(3, len(included_items))\n            for _ in range(max_perturbations):\n                candidate = random.choice(included_items)\n                if current_weight - weight_lst[candidate] <= capacity:\n                    new_solution[candidate] = 0\n                    current_weight -= weight_lst[candidate]\n                    break\n\n    # If no items are included, add the best item if possible\n    elif len(included_items) == 0 and len(excluded_items) > 0:\n        # Add the item with the highest value-to-weight ratio\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        if weight_lst[best_excluded] <= capacity:\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 103,
        "algorithm": "The algorithm selects the solution with the highest combined value-to-weight ratio for both objectives, then applies a hybrid local search that prioritizes adding high-value items (based on combined value-to-weight ratio) while strategically removing low-contribution items to balance capacity, using adaptive perturbations (random removal) when no improvements are possible. The method ensures feasibility by checking weight constraints before additions and balances exploration with exploitation by focusing on high-value items.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the solution with highest combined value-to-weight ratio\n    ratios = []\n    for sol, (v1, v2) in archive:\n        included = sol == 1\n        if np.sum(included) == 0:\n            ratios.append(0)\n        else:\n            total_weight = np.sum(weight_lst[included])\n            ratio1 = np.sum(value1_lst[included]) / total_weight\n            ratio2 = np.sum(value2_lst[included]) / total_weight\n            ratios.append((ratio1 + ratio2) / 2)\n\n    best_idx = np.argmax(ratios)\n    base_solution = archive[best_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Hybrid local search with strategic additions and removals\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    current_weight = np.sum(weight_lst[included_items])\n\n    # Strategy: Add highest-value items first, then remove lowest-contribution items\n    if len(excluded_items) > 0:\n        # Calculate combined value-to-weight ratio for excluded items\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        # Try to add the best excluded item\n        if current_weight + weight_lst[best_excluded] <= capacity:\n            new_solution[best_excluded] = 1\n            current_weight += weight_lst[best_excluded]\n        else:\n            # If no addition possible, remove worst included item\n            if len(included_items) > 0:\n                included_ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n                worst_included = included_items[np.argmin(included_ratios)]\n                new_solution[worst_included] = 0\n                current_weight -= weight_lst[worst_included]\n\n    # If still no changes, perform adaptive perturbation\n    if np.array_equal(new_solution, base_solution):\n        if len(included_items) > 0:\n            # Randomly remove one item to free capacity\n            candidate = random.choice(included_items)\n            new_solution[candidate] = 0\n\n    return new_solution\n\n",
        "score": [
            -0.7649568962036761,
            2.064754992723465
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the solution with highest combined value-to-weight ratio\n    ratios = []\n    for sol, (v1, v2) in archive:\n        included = sol == 1\n        if np.sum(included) == 0:\n            ratios.append(0)\n        else:\n            total_weight = np.sum(weight_lst[included])\n            ratio1 = np.sum(value1_lst[included]) / total_weight\n            ratio2 = np.sum(value2_lst[included]) / total_weight\n            ratios.append((ratio1 + ratio2) / 2)\n\n    best_idx = np.argmax(ratios)\n    base_solution = archive[best_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Hybrid local search with strategic additions and removals\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n    current_weight = np.sum(weight_lst[included_items])\n\n    # Strategy: Add highest-value items first, then remove lowest-contribution items\n    if len(excluded_items) > 0:\n        # Calculate combined value-to-weight ratio for excluded items\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        # Try to add the best excluded item\n        if current_weight + weight_lst[best_excluded] <= capacity:\n            new_solution[best_excluded] = 1\n            current_weight += weight_lst[best_excluded]\n        else:\n            # If no addition possible, remove worst included item\n            if len(included_items) > 0:\n                included_ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n                worst_included = included_items[np.argmin(included_ratios)]\n                new_solution[worst_included] = 0\n                current_weight -= weight_lst[worst_included]\n\n    # If still no changes, perform adaptive perturbation\n    if np.array_equal(new_solution, base_solution):\n        if len(included_items) > 0:\n            # Randomly remove one item to free capacity\n            candidate = random.choice(included_items)\n            new_solution[candidate] = 0\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 6 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThis heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm first identifies the Pareto front from the archive and selects a base solution using a weighted crowding-distance metric to prioritize high-diversity solutions. It then applies a hybrid local search that dynamically adjusts the neighborhood size based on the solution's weight utilization and objective weights, evaluating flips within the neighborhood while ensuring feasibility. If no improving move is found, it performs a probabilistic flip based on potential weighted contributions, dynamically adjusting objective weights to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Identify Pareto front\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            min_v1 = min(obj[0] for _, obj in pareto_front)\n            min_v2 = min(obj[1] for _, obj in pareto_front)\n\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min_v1 + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min_v2 + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto front clustering with dynamic objective-weighted cluster flips, prioritizing solutions based on crowding distance and hypervolume contributions while adaptively adjusting flip probabilities and cluster sizes to balance exploration and exploitation, using momentum to emphasize high-potential regions of the search space. Key design features include weighted improvement calculations, adaptive cluster sizing, and probabilistic flips with temperature-based selection to ensure feasible neighbor solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster solutions based on Pareto dominance and crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest combined crowding distance and hypervolume contribution\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster flip with momentum\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization and solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.3 + 0.2 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.9  # Higher momentum for more aggressive search\n\n    # Evaluate cluster flips with momentum\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm first filters non-dominated solutions from the archive and selects a base solution using a weighted crowding-distance metric that balances Pareto dominance and diversity. It then applies a hybrid local search with dynamic neighborhood sizes (20-50% of items) based on weight utilization, using objective-weighted improvement criteria to prioritize flips that enhance both objectives proportionally. If no improvement is found, it performs a probabilistic flip based on potential contributions while ensuring feasibility. The method dynamically adjusts objective weights to balance exploration and exploitation, generating high-quality neighbors that dominate or complement existing solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with weighted crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic neighborhood size based on weight utilization\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 4: Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 5: Hybrid local search with cluster flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering both objectives\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm selects a base solution from the archive using a weighted crowding-distance metric to prioritize high-diversity solutions, then applies a hybrid local search that adaptively flips 20-50% of items based on remaining capacity, with objective-weighted swaps and momentum-based improvement calculations to ensure feasibility and balance exploration/exploitation. The key variables are `neighborhood_size`, `weight_v1/v2`, and `momentum`, which dynamically adjust the search intensity and balance between objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution using weighted crowding-distance metric\n        crowding_distances = []\n        sorted_archive = sorted(archive, key=lambda x: (x[1][0], x[1][1]))\n\n        for i in range(len(sorted_archive)):\n            if i == 0 or i == len(sorted_archive) - 1:\n                crowding_distances.append(float('inf'))\n            else:\n                dist_v1 = sorted_archive[i+1][1][0] - sorted_archive[i-1][1][0]\n                dist_v2 = sorted_archive[i+1][1][1] - sorted_archive[i-1][1][1]\n                crowding_distances.append(dist_v1 + dist_v2)\n\n        max_dist_idx = np.argmax(crowding_distances)\n        base_solution = sorted_archive[max_dist_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Dynamic neighborhood size (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, min(total_items, int(total_items * (0.2 + 0.3 * (remaining_capacity / capacity)))))\n\n    if neighborhood_size > 0:\n        # Adaptive subset flips with objective-weighted swaps\n        subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate objective weights\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = total_v1 / (total_v1 + total_v2 + 1e-6)\n        weight_v2 = total_v2 / (total_v1 + total_v2 + 1e-6)\n\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.7  # Higher momentum for more aggressive search\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                temperature = 1 - (remaining_capacity / capacity)\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: **Keywords**: Adaptive selection, hybrid local search, Pareto-aware, dynamic feasibility\n\n**Advice**:\n1. Use weighted crowding-distance to prioritize high-diversity solutions.\n2. Hybridize local search with adaptive subset flips and objective-weighted swaps.\n3. Dynamically adjust neighborhood size based on solution quality.\n4. Ensure feasibility via precomputed item contributions and greedy checks.\n\n**Avoid**:\n- Static operators (fixed-size swaps).\n- Random dominance-based selection; prefer Pareto-ranked candidates.\n- Ignoring weight constraints; always validate feasibility.\n\n**Explanation**: This approach balances exploration and exploitation by adaptively selecting solutions and using hybrid, dynamic operators to escape local optima while ensuring feasibility. *(Word count: 50)*\n\n*(Total word count: 50)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 104,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Identify Pareto front and select solution with highest weighted crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 2: Dynamic neighborhood size based on weight utilization and solution quality\n    total_items = len(new_solution)\n    weight_utilization = current_weight / capacity\n    solution_quality = (np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - weight_utilization) * solution_quality)))\n\n    # Step 3: Hybrid local search with weighted swaps and subset flips\n    subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.8  # Higher momentum for more aggressive search\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement with momentum\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8966897838182759,
            0.5121578574180603
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Identify Pareto front and select solution with highest weighted crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 2: Dynamic neighborhood size based on weight utilization and solution quality\n    total_items = len(new_solution)\n    weight_utilization = current_weight / capacity\n    solution_quality = (np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - weight_utilization) * solution_quality)))\n\n    # Step 3: Hybrid local search with weighted swaps and subset flips\n    subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.8  # Higher momentum for more aggressive search\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement with momentum\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThis heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects the least crowded solution from the archive (based on crowding distance) and applies a hybrid local search combining value-weighted flips (prioritizing high-value items) and greedy swaps (optimizing combined objective density) while strictly maintaining feasibility through capacity checks. It balances exploration (randomized flips) and exploitation (greedy swaps) to efficiently navigate the solution space while prioritizing items with higher combined value-to-weight ratios.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the least crowded solution (minimum crowding distance)\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    selected_idx = np.argmin(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Value-weighted flip strategy\n    total_value1 = np.sum(value1_lst[new_solution == 1])\n    total_value2 = np.sum(value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    for idx in range(len(new_solution)):\n        if new_solution[idx] == 1:\n            # Probability of flipping based on normalized value contribution\n            flip_prob = (value1_lst[idx] / (total_value1 + 1e-6) + value2_lst[idx] / (total_value2 + 1e-6)) / 2\n            if np.random.rand() < flip_prob and current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            # Probability of adding based on normalized value\n            add_prob = (value1_lst[idx] + value2_lst[idx]) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n            if np.random.rand() < add_prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Step 3: Greedy swap to improve combined objectives\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate combined value density for included items\n        included_density = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_density)]\n\n        # Calculate combined value density for excluded items\n        excluded_density = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_density)]\n\n        # Check if swap is feasible\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            **Keywords**: Adaptive selection, hybrid local search, Pareto-aware, dynamic feasibility\n\n**Advice**:\n1. Use weighted crowding-distance to prioritize high-diversity solutions.\n2. Hybridize local search with adaptive subset flips and objective-weighted swaps.\n3. Dynamically adjust neighborhood size based on solution quality.\n4. Ensure feasibility via precomputed item contributions and greedy checks.\n\n**Avoid**:\n- Static operators (fixed-size swaps).\n- Random dominance-based selection; prefer Pareto-ranked candidates.\n- Ignoring weight constraints; always validate feasibility.\n\n**Explanation**: This approach balances exploration and exploitation by adaptively selecting solutions and using hybrid, dynamic operators to escape local optima while ensuring feasibility. *(Word count: 50)*\n\n*(Total word count: 50)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 105,
        "algorithm": "The algorithm dynamically selects a solution from the archive using a hybrid of weighted crowding distance and Pareto dominance, then applies an adaptive subset flip operator that prioritizes high-value-to-weight items while adjusting neighborhood size and flip probabilities based on solution quality and remaining capacity. It ensures feasibility through precomputed contributions and feasibility checks, balancing exploitation of high-value items with exploration of less obvious candidates through momentum-based improvements and adaptive temperature.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hybrid selection (weighted crowding + Pareto dominance)\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance and Pareto dominance\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        pareto_rank[i] = 1 if dominated else 0\n\n    # Normalize and combine metrics\n    norm_crowding = (crowding_distances - np.min(crowding_distances)) / (np.max(crowding_distances) - np.min(crowding_distances) + 1e-6)\n    selection_scores = 0.6 * norm_crowding + 0.4 * (1 - pareto_rank)\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Adaptive subset flip operator\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Dynamic neighborhood size\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.3 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        # Calculate value-to-weight ratios\n        vw_ratios = (value1_lst + value2_lst) / weight_lst\n\n        # Prioritize items with high v/w ratio\n        sorted_indices = np.argsort(vw_ratios)[::-1]\n        neighborhood_indices = sorted_indices[:neighborhood_size]\n\n        # Calculate dynamic weights\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.6 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.4 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Momentum-based improvement\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.7\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Adaptive temperature probabilistic flip\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8685109928684205,
            0.9019868969917297
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Hybrid selection (weighted crowding + Pareto dominance)\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance and Pareto dominance\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        pareto_rank[i] = 1 if dominated else 0\n\n    # Normalize and combine metrics\n    norm_crowding = (crowding_distances - np.min(crowding_distances)) / (np.max(crowding_distances) - np.min(crowding_distances) + 1e-6)\n    selection_scores = 0.6 * norm_crowding + 0.4 * (1 - pareto_rank)\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Adaptive subset flip operator\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Dynamic neighborhood size\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.3 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        # Calculate value-to-weight ratios\n        vw_ratios = (value1_lst + value2_lst) / weight_lst\n\n        # Prioritize items with high v/w ratio\n        sorted_indices = np.argsort(vw_ratios)[::-1]\n        neighborhood_indices = sorted_indices[:neighborhood_size]\n\n        # Calculate dynamic weights\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.6 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.4 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Momentum-based improvement\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.7\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Adaptive temperature probabilistic flip\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm first filters non-dominated solutions from the archive and selects a base solution using a weighted crowding-distance metric that balances Pareto dominance and diversity. It then applies a hybrid local search with dynamic neighborhood sizes (20-50% of items) based on weight utilization, using objective-weighted improvement criteria to prioritize flips that enhance both objectives proportionally. If no improvement is found, it performs a probabilistic flip based on potential contributions while ensuring feasibility. The method dynamically adjusts objective weights to balance exploration and exploitation, generating high-quality neighbors that dominate or complement existing solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with weighted crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic neighborhood size based on weight utilization\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 4: Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 5: Hybrid local search with cluster flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering both objectives\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm selects the least crowded solution from the archive (based on crowding distance) and applies a hybrid local search combining value-weighted flips (prioritizing high-value items) and greedy swaps (optimizing combined objective density) while strictly maintaining feasibility through capacity checks. It balances exploration (randomized flips) and exploitation (greedy swaps) to efficiently navigate the solution space while prioritizing items with higher combined value-to-weight ratios.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the least crowded solution (minimum crowding distance)\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    selected_idx = np.argmin(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Value-weighted flip strategy\n    total_value1 = np.sum(value1_lst[new_solution == 1])\n    total_value2 = np.sum(value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    for idx in range(len(new_solution)):\n        if new_solution[idx] == 1:\n            # Probability of flipping based on normalized value contribution\n            flip_prob = (value1_lst[idx] / (total_value1 + 1e-6) + value2_lst[idx] / (total_value2 + 1e-6)) / 2\n            if np.random.rand() < flip_prob and current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            # Probability of adding based on normalized value\n            add_prob = (value1_lst[idx] + value2_lst[idx]) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n            if np.random.rand() < add_prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Step 3: Greedy swap to improve combined objectives\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate combined value density for included items\n        included_density = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_density)]\n\n        # Calculate combined value density for excluded items\n        excluded_density = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_density)]\n\n        # Check if swap is feasible\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            **Keywords**: Adaptive selection, hybrid local search, Pareto-aware, dynamic feasibility\n\n**Advice**:\n1. Use weighted crowding-distance to prioritize high-diversity solutions.\n2. Hybridize local search with adaptive subset flips and objective-weighted swaps.\n3. Dynamically adjust neighborhood size based on solution quality.\n4. Ensure feasibility via precomputed item contributions and greedy checks.\n\n**Avoid**:\n- Static operators (fixed-size swaps).\n- Random dominance-based selection; prefer Pareto-ranked candidates.\n- Ignoring weight constraints; always validate feasibility.\n\n**Explanation**: This approach balances exploration and exploitation by adaptively selecting solutions and using hybrid, dynamic operators to escape local optima while ensuring feasibility. *(Word count: 50)*\n\n*(Total word count: 50)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 106,
        "algorithm": "The algorithm combines adaptive Pareto selection with weighted crowding-distance prioritization, followed by dynamic cluster-based flips and greedy value-density swaps. It intelligently balances exploration of high-value regions while ensuring feasibility through weighted objective scoring and precomputed weight thresholds. The approach prioritizes items with high combined objective contributions, dynamically adjusting cluster selection and swap operations based on current solution quality.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Adaptive Pareto-ranked selection\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Weighted crowding-distance selection\n    objectives = np.array([obj for _, obj in pareto_front])\n    crowding_distances = np.zeros(len(pareto_front))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(pareto_front) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster-based flips\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Cluster items by weight and value\n    item_scores = (weight_v1 * value1_lst + weight_v2 * value2_lst) / (weight_lst + 1e-6)\n    clusters = np.argsort(item_scores)[::-1]\n    cluster_size = max(1, len(clusters) // 4)\n    selected_clusters = np.random.choice(len(clusters), size=cluster_size, replace=False)\n\n    for idx in clusters[selected_clusters]:\n        if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Step 4: Greedy value-density swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        included_density = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_density)]\n\n        excluded_density = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_density)]\n\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.3748428143840935,
            0.6599393486976624
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Adaptive Pareto-ranked selection\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Weighted crowding-distance selection\n    objectives = np.array([obj for _, obj in pareto_front])\n    crowding_distances = np.zeros(len(pareto_front))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(pareto_front) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = pareto_front[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster-based flips\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Cluster items by weight and value\n    item_scores = (weight_v1 * value1_lst + weight_v2 * value2_lst) / (weight_lst + 1e-6)\n    clusters = np.argsort(item_scores)[::-1]\n    cluster_size = max(1, len(clusters) // 4)\n    selected_clusters = np.random.choice(len(clusters), size=cluster_size, replace=False)\n\n    for idx in clusters[selected_clusters]:\n        if new_solution[idx] == 1 and current_weight - weight_lst[idx] <= capacity:\n            new_solution[idx] = 0\n            current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and current_weight + weight_lst[idx] <= capacity:\n            new_solution[idx] = 1\n            current_weight += weight_lst[idx]\n\n    # Step 4: Greedy value-density swaps\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        included_density = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_density)]\n\n        excluded_density = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_density)]\n\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects the least crowded solution from the archive (based on crowding distance) and applies a hybrid local search combining value-weighted flips (prioritizing high-value items) and greedy swaps (optimizing combined objective density) while strictly maintaining feasibility through capacity checks. It balances exploration (randomized flips) and exploitation (greedy swaps) to efficiently navigate the solution space while prioritizing items with higher combined value-to-weight ratios.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the least crowded solution (minimum crowding distance)\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    selected_idx = np.argmin(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Value-weighted flip strategy\n    total_value1 = np.sum(value1_lst[new_solution == 1])\n    total_value2 = np.sum(value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    for idx in range(len(new_solution)):\n        if new_solution[idx] == 1:\n            # Probability of flipping based on normalized value contribution\n            flip_prob = (value1_lst[idx] / (total_value1 + 1e-6) + value2_lst[idx] / (total_value2 + 1e-6)) / 2\n            if np.random.rand() < flip_prob and current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            # Probability of adding based on normalized value\n            add_prob = (value1_lst[idx] + value2_lst[idx]) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n            if np.random.rand() < add_prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Step 3: Greedy swap to improve combined objectives\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate combined value density for included items\n        included_density = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_density)]\n\n        # Calculate combined value density for excluded items\n        excluded_density = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_density)]\n\n        # Check if swap is feasible\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 107,
        "algorithm": "The algorithm selects the most under-explored solution from the Pareto front based on objective trade-off diversity, then applies a hybrid local search combining probabilistic value-weighted flips and an objective-balanced swap strategy, dynamically adjusting based on the current solution's trade-off while strictly maintaining feasibility through adaptive capacity checks. The method prioritizes items with higher weighted value densities and balances exploration of both objectives by adjusting flip probabilities and swap selections based on the current solution's objective trade-off.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the most under-explored solution based on objective trade-off diversity\n    objectives = np.array([obj for _, obj in archive])\n    total_value1 = np.sum(objectives[:, 0])\n    total_value2 = np.sum(objectives[:, 1])\n\n    # Calculate normalized objective trade-offs\n    norm_obj1 = objectives[:, 0] / (total_value1 + 1e-6)\n    norm_obj2 = objectives[:, 1] / (total_value2 + 1e-6)\n    trade_off = norm_obj1 / (norm_obj2 + 1e-6)\n\n    # Select solution with median trade-off (most under-explored)\n    selected_idx = np.argsort(trade_off)[len(trade_off)//2]\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Probabilistic value-weighted flip with objective balancing\n    total_value1 = np.sum(value1_lst[new_solution == 1])\n    total_value2 = np.sum(value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate objective weights based on current solution's trade-off\n    obj_weight1 = 1.0 if total_value1 < total_value2 else 0.7\n    obj_weight2 = 1.0 if total_value2 < total_value1 else 0.7\n\n    for idx in range(len(new_solution)):\n        if new_solution[idx] == 1:\n            # Probability of flipping based on weighted value contribution\n            flip_prob = (obj_weight1 * value1_lst[idx] / (total_value1 + 1e-6) +\n                         obj_weight2 * value2_lst[idx] / (total_value2 + 1e-6))\n            if np.random.rand() < flip_prob and current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            # Probability of adding based on weighted value\n            add_prob = (obj_weight1 * value1_lst[idx] + obj_weight2 * value2_lst[idx]) / \\\n                       (obj_weight1 * np.sum(value1_lst) + obj_weight2 * np.sum(value2_lst) + 1e-6)\n            if np.random.rand() < add_prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Step 3: Objective-balanced swap strategy\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate weighted value density for included items\n        included_density = (obj_weight1 * value1_lst[included_items] +\n                           obj_weight2 * value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_density)]\n\n        # Calculate weighted value density for excluded items\n        excluded_density = (obj_weight1 * value1_lst[excluded_items] +\n                           obj_weight2 * value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_density)]\n\n        # Check if swap is feasible\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.7716861601054754,
            0.6088873445987701
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the most under-explored solution based on objective trade-off diversity\n    objectives = np.array([obj for _, obj in archive])\n    total_value1 = np.sum(objectives[:, 0])\n    total_value2 = np.sum(objectives[:, 1])\n\n    # Calculate normalized objective trade-offs\n    norm_obj1 = objectives[:, 0] / (total_value1 + 1e-6)\n    norm_obj2 = objectives[:, 1] / (total_value2 + 1e-6)\n    trade_off = norm_obj1 / (norm_obj2 + 1e-6)\n\n    # Select solution with median trade-off (most under-explored)\n    selected_idx = np.argsort(trade_off)[len(trade_off)//2]\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Probabilistic value-weighted flip with objective balancing\n    total_value1 = np.sum(value1_lst[new_solution == 1])\n    total_value2 = np.sum(value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Calculate objective weights based on current solution's trade-off\n    obj_weight1 = 1.0 if total_value1 < total_value2 else 0.7\n    obj_weight2 = 1.0 if total_value2 < total_value1 else 0.7\n\n    for idx in range(len(new_solution)):\n        if new_solution[idx] == 1:\n            # Probability of flipping based on weighted value contribution\n            flip_prob = (obj_weight1 * value1_lst[idx] / (total_value1 + 1e-6) +\n                         obj_weight2 * value2_lst[idx] / (total_value2 + 1e-6))\n            if np.random.rand() < flip_prob and current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            # Probability of adding based on weighted value\n            add_prob = (obj_weight1 * value1_lst[idx] + obj_weight2 * value2_lst[idx]) / \\\n                       (obj_weight1 * np.sum(value1_lst) + obj_weight2 * np.sum(value2_lst) + 1e-6)\n            if np.random.rand() < add_prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Step 3: Objective-balanced swap strategy\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate weighted value density for included items\n        included_density = (obj_weight1 * value1_lst[included_items] +\n                           obj_weight2 * value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_density)]\n\n        # Calculate weighted value density for excluded items\n        excluded_density = (obj_weight1 * value1_lst[excluded_items] +\n                           obj_weight2 * value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_density)]\n\n        # Check if swap is feasible\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm selects the least crowded solution from the archive (based on crowding distance) and applies a hybrid local search combining value-weighted flips (prioritizing high-value items) and greedy swaps (optimizing combined objective density) while strictly maintaining feasibility through capacity checks. It balances exploration (randomized flips) and exploitation (greedy swaps) to efficiently navigate the solution space while prioritizing items with higher combined value-to-weight ratios.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the least crowded solution (minimum crowding distance)\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    selected_idx = np.argmin(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Value-weighted flip strategy\n    total_value1 = np.sum(value1_lst[new_solution == 1])\n    total_value2 = np.sum(value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    for idx in range(len(new_solution)):\n        if new_solution[idx] == 1:\n            # Probability of flipping based on normalized value contribution\n            flip_prob = (value1_lst[idx] / (total_value1 + 1e-6) + value2_lst[idx] / (total_value2 + 1e-6)) / 2\n            if np.random.rand() < flip_prob and current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            # Probability of adding based on normalized value\n            add_prob = (value1_lst[idx] + value2_lst[idx]) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n            if np.random.rand() < add_prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Step 3: Greedy swap to improve combined objectives\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate combined value density for included items\n        included_density = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_density)]\n\n        # Calculate combined value density for excluded items\n        excluded_density = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_density)]\n\n        # Check if swap is feasible\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 108,
        "algorithm": "The algorithm selects the most diverse solution from the archive (based on crowding distance) and applies a hybrid local search combining weighted random flips (prioritizing high-value items) and adaptive greedy swaps (optimizing individual objective improvements) while strictly maintaining feasibility through capacity checks. It first selects a solution with maximum crowding distance to ensure diversity, then probabilistically flips items based on their normalized value contributions, and finally performs a greedy swap between the worst included item and the best excluded item to improve overall value efficiency. The method ensures feasibility by checking weight constraints at each step.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the most diverse solution (maximum crowding distance)\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Weighted random flip strategy\n    total_value1 = np.sum(value1_lst[new_solution == 1])\n    total_value2 = np.sum(value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    for idx in range(len(new_solution)):\n        if new_solution[idx] == 1:\n            # Probability of flipping based on normalized value contribution\n            flip_prob = (value1_lst[idx] / (np.max(value1_lst) + 1e-6) + value2_lst[idx] / (np.max(value2_lst) + 1e-6)) / 2\n            if np.random.rand() < flip_prob and current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            # Probability of adding based on normalized value\n            add_prob = (value1_lst[idx] / np.max(value1_lst) + value2_lst[idx] / np.max(value2_lst)) / 2\n            if np.random.rand() < add_prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Step 3: Adaptive greedy swap to improve individual objectives\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate value improvements for included items\n        value1_improvement = value1_lst[included_items] / weight_lst[included_items]\n        value2_improvement = value2_lst[included_items] / weight_lst[included_items]\n        worst_included = included_items[np.argmin(value1_improvement + value2_improvement)]\n\n        # Calculate value improvements for excluded items\n        value1_improvement = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value2_improvement = value2_lst[excluded_items] / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(value1_improvement + value2_improvement)]\n\n        # Check if swap is feasible\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.8028131417249917,
            1.751551628112793
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select the most diverse solution (maximum crowding distance)\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    selected_idx = np.argmax(crowding_distances)\n    base_solution = archive[selected_idx][0].copy()\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 2: Weighted random flip strategy\n    total_value1 = np.sum(value1_lst[new_solution == 1])\n    total_value2 = np.sum(value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    for idx in range(len(new_solution)):\n        if new_solution[idx] == 1:\n            # Probability of flipping based on normalized value contribution\n            flip_prob = (value1_lst[idx] / (np.max(value1_lst) + 1e-6) + value2_lst[idx] / (np.max(value2_lst) + 1e-6)) / 2\n            if np.random.rand() < flip_prob and current_weight - weight_lst[idx] <= capacity:\n                new_solution[idx] = 0\n                current_weight -= weight_lst[idx]\n        elif new_solution[idx] == 0 and weight_lst[idx] <= remaining_capacity:\n            # Probability of adding based on normalized value\n            add_prob = (value1_lst[idx] / np.max(value1_lst) + value2_lst[idx] / np.max(value2_lst)) / 2\n            if np.random.rand() < add_prob:\n                new_solution[idx] = 1\n                current_weight += weight_lst[idx]\n                remaining_capacity -= weight_lst[idx]\n\n    # Step 3: Adaptive greedy swap to improve individual objectives\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Calculate value improvements for included items\n        value1_improvement = value1_lst[included_items] / weight_lst[included_items]\n        value2_improvement = value2_lst[included_items] / weight_lst[included_items]\n        worst_included = included_items[np.argmin(value1_improvement + value2_improvement)]\n\n        # Calculate value improvements for excluded items\n        value1_improvement = value1_lst[excluded_items] / weight_lst[excluded_items]\n        value2_improvement = value2_lst[excluded_items] / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(value1_improvement + value2_improvement)]\n\n        # Check if swap is feasible\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 7 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThis heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm first identifies the Pareto front from the archive and selects a base solution using a weighted crowding-distance metric to prioritize high-diversity solutions. It then applies a hybrid local search that dynamically adjusts the neighborhood size based on the solution's weight utilization and objective weights, evaluating flips within the neighborhood while ensuring feasibility. If no improving move is found, it performs a probabilistic flip based on potential weighted contributions, dynamically adjusting objective weights to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Identify Pareto front\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            min_v1 = min(obj[0] for _, obj in pareto_front)\n            min_v2 = min(obj[1] for _, obj in pareto_front)\n\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min_v1 + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min_v2 + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto front clustering with dynamic objective-weighted cluster flips, prioritizing solutions based on crowding distance and hypervolume contributions while adaptively adjusting flip probabilities and cluster sizes to balance exploration and exploitation, using momentum to emphasize high-potential regions of the search space. Key design features include weighted improvement calculations, adaptive cluster sizing, and probabilistic flips with temperature-based selection to ensure feasible neighbor solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster solutions based on Pareto dominance and crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest combined crowding distance and hypervolume contribution\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster flip with momentum\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization and solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.3 + 0.2 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.9  # Higher momentum for more aggressive search\n\n    # Evaluate cluster flips with momentum\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm first filters non-dominated solutions from the archive and selects a base solution using a weighted crowding-distance metric that balances Pareto dominance and diversity. It then applies a hybrid local search with dynamic neighborhood sizes (20-50% of items) based on weight utilization, using objective-weighted improvement criteria to prioritize flips that enhance both objectives proportionally. If no improvement is found, it performs a probabilistic flip based on potential contributions while ensuring feasibility. The method dynamically adjusts objective weights to balance exploration and exploitation, generating high-quality neighbors that dominate or complement existing solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with weighted crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic neighborhood size based on weight utilization\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 4: Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 5: Hybrid local search with cluster flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering both objectives\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm selects a base solution from the archive using a weighted crowding-distance metric to prioritize high-diversity solutions, then applies a hybrid local search that adaptively flips 20-50% of items based on remaining capacity, with objective-weighted swaps and momentum-based improvement calculations to ensure feasibility and balance exploration/exploitation. The key variables are `neighborhood_size`, `weight_v1/v2`, and `momentum`, which dynamically adjust the search intensity and balance between objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution using weighted crowding-distance metric\n        crowding_distances = []\n        sorted_archive = sorted(archive, key=lambda x: (x[1][0], x[1][1]))\n\n        for i in range(len(sorted_archive)):\n            if i == 0 or i == len(sorted_archive) - 1:\n                crowding_distances.append(float('inf'))\n            else:\n                dist_v1 = sorted_archive[i+1][1][0] - sorted_archive[i-1][1][0]\n                dist_v2 = sorted_archive[i+1][1][1] - sorted_archive[i-1][1][1]\n                crowding_distances.append(dist_v1 + dist_v2)\n\n        max_dist_idx = np.argmax(crowding_distances)\n        base_solution = sorted_archive[max_dist_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Dynamic neighborhood size (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, min(total_items, int(total_items * (0.2 + 0.3 * (remaining_capacity / capacity)))))\n\n    if neighborhood_size > 0:\n        # Adaptive subset flips with objective-weighted swaps\n        subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate objective weights\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = total_v1 / (total_v1 + total_v2 + 1e-6)\n        weight_v2 = total_v2 / (total_v1 + total_v2 + 1e-6)\n\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.7  # Higher momentum for more aggressive search\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                temperature = 1 - (remaining_capacity / capacity)\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Identify Pareto front and select solution with highest weighted crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 2: Dynamic neighborhood size based on weight utilization and solution quality\n    total_items = len(new_solution)\n    weight_utilization = current_weight / capacity\n    solution_quality = (np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - weight_utilization) * solution_quality)))\n\n    # Step 3: Hybrid local search with weighted swaps and subset flips\n    subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.8  # Higher momentum for more aggressive search\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement with momentum\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: **Keywords**: Adaptive selection, hybrid local search, Pareto-aware, dynamic feasibility\n\n**Advice**:\n1. Use weighted crowding-distance to prioritize high-diversity solutions.\n2. Hybridize local search with adaptive subset flips and objective-weighted swaps.\n3. Dynamically adjust neighborhood size based on solution quality.\n4. Precompute item contributions and validate feasibility greedily.\n\n**Avoid**:\n- Static operators (fixed-size swaps).\n- Random dominance-based selection; prefer Pareto-ranked candidates.\n- Ignoring weight constraints; always validate feasibility.\n\n**Explanation**: This approach balances exploration and exploitation by adaptively selecting solutions and using hybrid, dynamic operators to escape local optima while ensuring feasibility. *(Word count: 50)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 109,
        "algorithm": "The algorithm selects a promising solution from the archive using a weighted combination of crowding distance and hypervolume contribution, then applies a hybrid local search with dynamic neighborhood size, weighted objective swaps, and probabilistic exploration to generate a feasible neighbor solution that balances both objectives while prioritizing higher-quality improvements. The method adapts to solution quality and weight utilization, ensuring feasible moves while favoring high-impact changes based on dynamic objective weights and momentum.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest weighted crowding-distance and hypervolume contribution\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 2: Dynamic neighborhood size based on weight utilization and solution quality\n    total_items = len(new_solution)\n    weight_utilization = current_weight / capacity\n    solution_quality = (np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - weight_utilization) * solution_quality)))\n\n    # Step 3: Hybrid local search with weighted swaps and subset flips\n    subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.8\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8847395620932276,
            0.4530027508735657
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest weighted crowding-distance and hypervolume contribution\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 2: Dynamic neighborhood size based on weight utilization and solution quality\n    total_items = len(new_solution)\n    weight_utilization = current_weight / capacity\n    solution_quality = (np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - weight_utilization) * solution_quality)))\n\n    # Step 3: Hybrid local search with weighted swaps and subset flips\n    subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.8\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThis heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution with highest weighted objective diversity\n        weighted_diversities = []\n        for sol, obj in archive:\n            v1, v2 = obj\n            diversity = (v1 / (v2 + 1e-6)) + (v2 / (v1 + 1e-6))\n            weighted_diversities.append(diversity)\n        base_solution = archive[np.argmax(weighted_diversities)][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive neighborhood selection based on solution balance\n    total_items = len(new_solution)\n    balance = np.sum(value1_lst[new_solution == 1]) / (np.sum(value2_lst[new_solution == 1]) + 1e-6)\n    neighborhood_size = max(1, int(total_items * (0.3 + 0.2 * (1 - min(balance, 1/balance)))))\n\n    # Dynamic objective weighting based on solution's current imbalance\n    if balance > 1:\n        weight_v1, weight_v2 = 0.7, 0.3\n    elif balance < 1:\n        weight_v1, weight_v2 = 0.3, 0.7\n    else:\n        weight_v1, weight_v2 = 0.5, 0.5\n\n    # Multi-flip local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for _ in range(min(3, neighborhood_size)):\n        temp_solution = new_solution.copy()\n        # Select multiple items to flip (1-3)\n        flip_count = np.random.randint(1, min(4, neighborhood_size))\n        flip_indices = np.random.choice(total_items, size=flip_count, replace=False)\n\n        new_weight = current_weight\n        for idx in flip_indices:\n            if temp_solution[idx] == 1:\n                new_weight -= weight_lst[idx]\n            else:\n                new_weight += weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        improvement = 0\n        for idx in flip_indices:\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 0 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 0 else -value2_lst[idx]\n            improvement += weight_v1 * delta_v1 + weight_v2 * delta_v2\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution.copy()\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Feasibility-preserving random flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            **Keywords**: Adaptive selection, hybrid local search, Pareto-aware, dynamic feasibility\n\n**Advice**:\n1. Use weighted crowding-distance to prioritize high-diversity solutions.\n2. Hybridize local search with adaptive subset flips and objective-weighted swaps.\n3. Dynamically adjust neighborhood size based on solution quality.\n4. Precompute item contributions and validate feasibility greedily.\n\n**Avoid**:\n- Static operators (fixed-size swaps).\n- Random dominance-based selection; prefer Pareto-ranked candidates.\n- Ignoring weight constraints; always validate feasibility.\n\n**Explanation**: This approach balances exploration and exploitation by adaptively selecting solutions and using hybrid, dynamic operators to escape local optima while ensuring feasibility. *(Word count: 50)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 110,
        "algorithm": "The algorithm combines Pareto-aware selection with a dynamic hybrid local search that prioritizes crowding-distance solutions, adaptively adjusts neighborhood size and objective weights based on solution balance, and uses multi-subset flips with feasibility checks to generate high-quality neighbors while ensuring diversity and convergence toward the Pareto front. It balances exploration and exploitation through weighted objective diversification and probabilistic flips, while maintaining feasibility through greedy validation. The critical design ideas are the dynamic neighborhood adaptation, weighted objective prioritization, and hybrid local search with multi-subset flips.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution with highest weighted crowding-distance\n        crowding_distances = []\n        sorted_v1 = sorted([obj[0] for _, obj in archive])\n        sorted_v2 = sorted([obj[1] for _, obj in archive])\n\n        for sol, obj in archive:\n            v1, v2 = obj\n            v1_idx = np.searchsorted(sorted_v1, v1)\n            v2_idx = np.searchsorted(sorted_v2, v2)\n\n            if v1_idx == 0 or v1_idx == len(sorted_v1) - 1:\n                crowding_v1 = float('inf')\n            else:\n                crowding_v1 = (sorted_v1[v1_idx + 1] - sorted_v1[v1_idx - 1]) / (sorted_v1[-1] - sorted_v1[0] + 1e-6)\n\n            if v2_idx == 0 or v2_idx == len(sorted_v2) - 1:\n                crowding_v2 = float('inf')\n            else:\n                crowding_v2 = (sorted_v2[v2_idx + 1] - sorted_v2[v2_idx - 1]) / (sorted_v2[-1] - sorted_v2[0] + 1e-6)\n\n            crowding_distances.append(crowding_v1 + crowding_v2)\n\n        base_solution = archive[np.argmax(crowding_distances)][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution balance and quality\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    balance = total_v1 / (total_v2 + 1e-6)\n    quality = total_v1 + total_v2\n    total_quality = np.sum(value1_lst) + np.sum(value2_lst)\n    neighborhood_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - min(balance, 1/balance)) * (quality / (total_quality + 1e-6)))))\n\n    # Dynamic objective weights based on balance\n    if balance > 1:\n        weight_v1, weight_v2 = 0.6, 0.4\n    elif balance < 1:\n        weight_v1, weight_v2 = 0.4, 0.6\n    else:\n        weight_v1, weight_v2 = 0.5, 0.5\n\n    # Hybrid local search with multi-subset flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for _ in range(min(2, neighborhood_size)):\n        temp_solution = new_solution.copy()\n        # Select multiple subsets to flip (1-2)\n        flip_count = np.random.randint(1, min(3, neighborhood_size))\n        flip_indices = np.random.choice(len(new_solution), size=flip_count, replace=False)\n\n        new_weight = current_weight\n        for idx in flip_indices:\n            if temp_solution[idx] == 1:\n                new_weight -= weight_lst[idx]\n            else:\n                new_weight += weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        improvement = 0\n        for idx in flip_indices:\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 0 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 0 else -value2_lst[idx]\n            improvement += weight_v1 * delta_v1 + weight_v2 * delta_v2\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution.copy()\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy feasibility-preserving flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Select item with highest weighted marginal contribution\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contributions.append(weight_v1 * potential_v1 + weight_v2 * potential_v2)\n\n            idx = valid_indices[np.argmax(contributions)]\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.7725535839751627,
            1.101166546344757
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution with highest weighted crowding-distance\n        crowding_distances = []\n        sorted_v1 = sorted([obj[0] for _, obj in archive])\n        sorted_v2 = sorted([obj[1] for _, obj in archive])\n\n        for sol, obj in archive:\n            v1, v2 = obj\n            v1_idx = np.searchsorted(sorted_v1, v1)\n            v2_idx = np.searchsorted(sorted_v2, v2)\n\n            if v1_idx == 0 or v1_idx == len(sorted_v1) - 1:\n                crowding_v1 = float('inf')\n            else:\n                crowding_v1 = (sorted_v1[v1_idx + 1] - sorted_v1[v1_idx - 1]) / (sorted_v1[-1] - sorted_v1[0] + 1e-6)\n\n            if v2_idx == 0 or v2_idx == len(sorted_v2) - 1:\n                crowding_v2 = float('inf')\n            else:\n                crowding_v2 = (sorted_v2[v2_idx + 1] - sorted_v2[v2_idx - 1]) / (sorted_v2[-1] - sorted_v2[0] + 1e-6)\n\n            crowding_distances.append(crowding_v1 + crowding_v2)\n\n        base_solution = archive[np.argmax(crowding_distances)][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution balance and quality\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    balance = total_v1 / (total_v2 + 1e-6)\n    quality = total_v1 + total_v2\n    total_quality = np.sum(value1_lst) + np.sum(value2_lst)\n    neighborhood_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - min(balance, 1/balance)) * (quality / (total_quality + 1e-6)))))\n\n    # Dynamic objective weights based on balance\n    if balance > 1:\n        weight_v1, weight_v2 = 0.6, 0.4\n    elif balance < 1:\n        weight_v1, weight_v2 = 0.4, 0.6\n    else:\n        weight_v1, weight_v2 = 0.5, 0.5\n\n    # Hybrid local search with multi-subset flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for _ in range(min(2, neighborhood_size)):\n        temp_solution = new_solution.copy()\n        # Select multiple subsets to flip (1-2)\n        flip_count = np.random.randint(1, min(3, neighborhood_size))\n        flip_indices = np.random.choice(len(new_solution), size=flip_count, replace=False)\n\n        new_weight = current_weight\n        for idx in flip_indices:\n            if temp_solution[idx] == 1:\n                new_weight -= weight_lst[idx]\n            else:\n                new_weight += weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        improvement = 0\n        for idx in flip_indices:\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 0 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 0 else -value2_lst[idx]\n            improvement += weight_v1 * delta_v1 + weight_v2 * delta_v2\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution.copy()\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy feasibility-preserving flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Select item with highest weighted marginal contribution\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contributions.append(weight_v1 * potential_v1 + weight_v2 * potential_v2)\n\n            idx = valid_indices[np.argmax(contributions)]\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive selection - prioritize solutions with high crowding distance and normalized objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance with normalized objective diversity\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_diversity = np.zeros(len(archive))\n    for i, (v1, v2) in enumerate(objectives):\n        normalized_diversity[i] = (v1 / max_v1 + v2 / max_v2) if max_v1 > 0 and max_v2 > 0 else 0\n\n    selection_scores = crowding_distances * normalized_diversity\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search - adaptive subset flips with objective-weighted improvements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic neighborhood size based on solution quality\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Strategy 1: Adaptive subset flip with objective-weighted selection\n    for _ in range(neighborhood_size):\n        # Select a random subset of items to consider\n        subset_size = min(3, max(1, int(n_items * 0.2)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            # Try flipping the item\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            # Calculate new weight\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            # Check feasibility\n            if new_weight > capacity:\n                continue\n\n            # Calculate objective improvements\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-weighted improvement\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Strategy 2: Objective-weighted item swaps if no improvement found\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    # Objective-weighted swap decision\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution with highest weighted objective diversity\n        weighted_diversities = []\n        for sol, obj in archive:\n            v1, v2 = obj\n            diversity = (v1 / (v2 + 1e-6)) + (v2 / (v1 + 1e-6))\n            weighted_diversities.append(diversity)\n        base_solution = archive[np.argmax(weighted_diversities)][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive neighborhood selection based on solution balance\n    total_items = len(new_solution)\n    balance = np.sum(value1_lst[new_solution == 1]) / (np.sum(value2_lst[new_solution == 1]) + 1e-6)\n    neighborhood_size = max(1, int(total_items * (0.3 + 0.2 * (1 - min(balance, 1/balance)))))\n\n    # Dynamic objective weighting based on solution's current imbalance\n    if balance > 1:\n        weight_v1, weight_v2 = 0.7, 0.3\n    elif balance < 1:\n        weight_v1, weight_v2 = 0.3, 0.7\n    else:\n        weight_v1, weight_v2 = 0.5, 0.5\n\n    # Multi-flip local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for _ in range(min(3, neighborhood_size)):\n        temp_solution = new_solution.copy()\n        # Select multiple items to flip (1-3)\n        flip_count = np.random.randint(1, min(4, neighborhood_size))\n        flip_indices = np.random.choice(total_items, size=flip_count, replace=False)\n\n        new_weight = current_weight\n        for idx in flip_indices:\n            if temp_solution[idx] == 1:\n                new_weight -= weight_lst[idx]\n            else:\n                new_weight += weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        improvement = 0\n        for idx in flip_indices:\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 0 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 0 else -value2_lst[idx]\n            improvement += weight_v1 * delta_v1 + weight_v2 * delta_v2\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution.copy()\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Feasibility-preserving random flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            **Keywords**: Adaptive selection, hybrid local search, Pareto-aware, dynamic feasibility\n\n**Advice**:\n1. Use weighted crowding-distance to prioritize high-diversity solutions.\n2. Hybridize local search with adaptive subset flips and objective-weighted swaps.\n3. Dynamically adjust neighborhood size based on solution quality.\n4. Precompute item contributions and validate feasibility greedily.\n\n**Avoid**:\n- Static operators (fixed-size swaps).\n- Random dominance-based selection; prefer Pareto-ranked candidates.\n- Ignoring weight constraints; always validate feasibility.\n\n**Explanation**: This approach balances exploration and exploitation by adaptively selecting solutions and using hybrid, dynamic operators to escape local optima while ensuring feasibility. *(Word count: 50)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 111,
        "algorithm": "The algorithm combines adaptive Pareto selection with a multi-phase local search that dynamically balances objective priorities, uses cluster-based flips and objective-driven flip chains, and ensures feasibility through weighted diversity and feasibility-preserving operations. It prioritizes value1 when its total is significantly higher than value2 and vice versa, while maintaining solution quality through correlated item flips and chain reactions, with fallback to random but feasible moves when no improvements are found. The overall structure alternates between selection, objective balancing, and improvement phases to efficiently explore the solution space while preserving feasibility.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive Pareto-ranked selection with weighted diversity\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i, (v1, v2) in enumerate(objectives):\n        dominated = False\n        for j in range(len(objectives)):\n            if i != j and objectives[j, 0] >= v1 and objectives[j, 1] >= v2 and (objectives[j, 0] > v1 or objectives[j, 1] > v2):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if not pareto_front:\n        pareto_front = list(range(len(archive)))\n\n    # Calculate weighted diversity scores\n    diversity_scores = []\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    for i in pareto_front:\n        v1, v2 = objectives[i]\n        diversity = (v1 / (max_v1 + 1e-6) + v2 / (max_v2 + 1e-6)) * (1 - np.abs(v1 - v2) / (v1 + v2 + 1e-6))\n        diversity_scores.append(diversity)\n\n    selected_idx = pareto_front[np.argmax(diversity_scores)]\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Dynamic objective balancing\n    total_v1 = np.sum(value1_lst[base_solution == 1])\n    total_v2 = np.sum(value2_lst[base_solution == 1])\n    balance = total_v1 / (total_v2 + 1e-6)\n\n    if balance > 1.5:\n        weight_v1, weight_v2 = 0.6, 0.4\n    elif balance < 0.66:\n        weight_v1, weight_v2 = 0.4, 0.6\n    else:\n        weight_v1, weight_v2 = 0.5, 0.5\n\n    # Step 3: Multi-phase local search\n    n_items = len(weight_lst)\n    max_flips = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Phase 1: Cluster-based flips\n    clusters = []\n    for i in range(n_items):\n        if new_solution[i] == 1:\n            clusters.append([i])\n    for _ in range(max_flips):\n        if not clusters:\n            break\n        cluster_idx = np.random.choice(len(clusters))\n        cluster = clusters[cluster_idx]\n        best_improvement = 0\n        best_candidate = None\n\n        for item in cluster:\n            temp_solution = new_solution.copy()\n            temp_solution[item] = 1 - temp_solution[item]\n\n            new_weight = current_weight\n            if temp_solution[item] == 1:\n                new_weight += weight_lst[item]\n            else:\n                new_weight -= weight_lst[item]\n\n            if new_weight > capacity:\n                continue\n\n            improvement = (weight_v1 * (value1_lst[item] if temp_solution[item] == 1 else -value1_lst[item]) +\n                          weight_v2 * (value2_lst[item] if temp_solution[item] == 1 else -value2_lst[item]))\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n            clusters[cluster_idx] = [i for i in cluster if new_solution[i] == 1]\n            if not clusters[cluster_idx]:\n                clusters.pop(cluster_idx)\n\n    # Phase 2: Objective-driven flip chain\n    if best_improvement > 0:\n        for _ in range(max_flips):\n            flip_candidates = []\n            for i in range(n_items):\n                if new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                    flip_candidates.append((i, weight_v1 * value1_lst[i] + weight_v2 * value2_lst[i]))\n                elif new_solution[i] == 1 and current_weight - weight_lst[i] >= 0:\n                    flip_candidates.append((i, -(weight_v1 * value1_lst[i] + weight_v2 * value2_lst[i])))\n\n            if not flip_candidates:\n                break\n\n            flip_candidates.sort(key=lambda x: -x[1])\n            best_item, best_val = flip_candidates[0]\n\n            new_solution[best_item] = 1 - new_solution[best_item]\n            current_weight += weight_lst[best_item] if new_solution[best_item] == 1 else -weight_lst[best_item]\n\n    # Phase 3: Feasibility-preserving random flip if no improvement\n    if best_improvement == 0:\n        valid_indices = [i for i in range(n_items) if\n                       (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                       (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = np.random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.818621060197146,
            0.9092504978179932
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive Pareto-ranked selection with weighted diversity\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i, (v1, v2) in enumerate(objectives):\n        dominated = False\n        for j in range(len(objectives)):\n            if i != j and objectives[j, 0] >= v1 and objectives[j, 1] >= v2 and (objectives[j, 0] > v1 or objectives[j, 1] > v2):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if not pareto_front:\n        pareto_front = list(range(len(archive)))\n\n    # Calculate weighted diversity scores\n    diversity_scores = []\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    for i in pareto_front:\n        v1, v2 = objectives[i]\n        diversity = (v1 / (max_v1 + 1e-6) + v2 / (max_v2 + 1e-6)) * (1 - np.abs(v1 - v2) / (v1 + v2 + 1e-6))\n        diversity_scores.append(diversity)\n\n    selected_idx = pareto_front[np.argmax(diversity_scores)]\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Step 2: Dynamic objective balancing\n    total_v1 = np.sum(value1_lst[base_solution == 1])\n    total_v2 = np.sum(value2_lst[base_solution == 1])\n    balance = total_v1 / (total_v2 + 1e-6)\n\n    if balance > 1.5:\n        weight_v1, weight_v2 = 0.6, 0.4\n    elif balance < 0.66:\n        weight_v1, weight_v2 = 0.4, 0.6\n    else:\n        weight_v1, weight_v2 = 0.5, 0.5\n\n    # Step 3: Multi-phase local search\n    n_items = len(weight_lst)\n    max_flips = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Phase 1: Cluster-based flips\n    clusters = []\n    for i in range(n_items):\n        if new_solution[i] == 1:\n            clusters.append([i])\n    for _ in range(max_flips):\n        if not clusters:\n            break\n        cluster_idx = np.random.choice(len(clusters))\n        cluster = clusters[cluster_idx]\n        best_improvement = 0\n        best_candidate = None\n\n        for item in cluster:\n            temp_solution = new_solution.copy()\n            temp_solution[item] = 1 - temp_solution[item]\n\n            new_weight = current_weight\n            if temp_solution[item] == 1:\n                new_weight += weight_lst[item]\n            else:\n                new_weight -= weight_lst[item]\n\n            if new_weight > capacity:\n                continue\n\n            improvement = (weight_v1 * (value1_lst[item] if temp_solution[item] == 1 else -value1_lst[item]) +\n                          weight_v2 * (value2_lst[item] if temp_solution[item] == 1 else -value2_lst[item]))\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n            clusters[cluster_idx] = [i for i in cluster if new_solution[i] == 1]\n            if not clusters[cluster_idx]:\n                clusters.pop(cluster_idx)\n\n    # Phase 2: Objective-driven flip chain\n    if best_improvement > 0:\n        for _ in range(max_flips):\n            flip_candidates = []\n            for i in range(n_items):\n                if new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity:\n                    flip_candidates.append((i, weight_v1 * value1_lst[i] + weight_v2 * value2_lst[i]))\n                elif new_solution[i] == 1 and current_weight - weight_lst[i] >= 0:\n                    flip_candidates.append((i, -(weight_v1 * value1_lst[i] + weight_v2 * value2_lst[i])))\n\n            if not flip_candidates:\n                break\n\n            flip_candidates.sort(key=lambda x: -x[1])\n            best_item, best_val = flip_candidates[0]\n\n            new_solution[best_item] = 1 - new_solution[best_item]\n            current_weight += weight_lst[best_item] if new_solution[best_item] == 1 else -weight_lst[best_item]\n\n    # Phase 3: Feasibility-preserving random flip if no improvement\n    if best_improvement == 0:\n        valid_indices = [i for i in range(n_items) if\n                       (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                       (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = np.random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        None\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution with highest weighted objective diversity\n        weighted_diversities = []\n        for sol, obj in archive:\n            v1, v2 = obj\n            diversity = (v1 / (v2 + 1e-6)) + (v2 / (v1 + 1e-6))\n            weighted_diversities.append(diversity)\n        base_solution = archive[np.argmax(weighted_diversities)][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive neighborhood selection based on solution balance\n    total_items = len(new_solution)\n    balance = np.sum(value1_lst[new_solution == 1]) / (np.sum(value2_lst[new_solution == 1]) + 1e-6)\n    neighborhood_size = max(1, int(total_items * (0.3 + 0.2 * (1 - min(balance, 1/balance)))))\n\n    # Dynamic objective weighting based on solution's current imbalance\n    if balance > 1:\n        weight_v1, weight_v2 = 0.7, 0.3\n    elif balance < 1:\n        weight_v1, weight_v2 = 0.3, 0.7\n    else:\n        weight_v1, weight_v2 = 0.5, 0.5\n\n    # Multi-flip local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for _ in range(min(3, neighborhood_size)):\n        temp_solution = new_solution.copy()\n        # Select multiple items to flip (1-3)\n        flip_count = np.random.randint(1, min(4, neighborhood_size))\n        flip_indices = np.random.choice(total_items, size=flip_count, replace=False)\n\n        new_weight = current_weight\n        for idx in flip_indices:\n            if temp_solution[idx] == 1:\n                new_weight -= weight_lst[idx]\n            else:\n                new_weight += weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        improvement = 0\n        for idx in flip_indices:\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 0 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 0 else -value2_lst[idx]\n            improvement += weight_v1 * delta_v1 + weight_v2 * delta_v2\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution.copy()\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Feasibility-preserving random flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 112,
        "algorithm": "The algorithm selects a promising solution from the archive by evaluating each solution's potential for improvement in both objectives, then applies a hybrid local search that combines weighted multi-flip operations with adaptive neighborhood exploration, dynamically adjusting the search based on the balance between the two objectives to generate high-quality feasible neighbor solutions. It prioritizes flipping items with higher combined value while respecting capacity constraints, using probabilistic selection to explore the search space effectively.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution with highest potential improvement\n        improvement_potentials = []\n        for sol, obj in archive:\n            v1, v2 = obj\n            current_weight = np.sum(weight_lst[sol == 1])\n            remaining_capacity = capacity - current_weight\n            potential_v1 = np.sum(value1_lst[(sol == 0) & (weight_lst <= remaining_capacity)])\n            potential_v2 = np.sum(value2_lst[(sol == 0) & (weight_lst <= remaining_capacity)])\n            improvement_potentials.append(potential_v1 + potential_v2)\n        base_solution = archive[np.argmax(improvement_potentials)][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search with adaptive objective weighting\n    total_items = len(new_solution)\n    balance = np.sum(value1_lst[new_solution == 1]) / (np.sum(value2_lst[new_solution == 1]) + 1e-6)\n\n    # Dynamic neighborhood selection\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - np.tanh(np.abs(np.log(balance + 1e-6)))))))\n\n    # Weighted objective function\n    if balance > 1.5:\n        weight_v1, weight_v2 = 0.8, 0.2\n    elif balance < 0.67:\n        weight_v1, weight_v2 = 0.2, 0.8\n    else:\n        weight_v1, weight_v2 = 0.5, 0.5\n\n    # Multi-flip with probabilistic selection\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for _ in range(min(4, neighborhood_size)):\n        temp_solution = new_solution.copy()\n        flip_count = np.random.randint(1, min(5, neighborhood_size))\n        flip_indices = np.random.choice(total_items, size=flip_count, replace=False, p=weight_lst/np.sum(weight_lst))\n\n        new_weight = current_weight\n        for idx in flip_indices:\n            if temp_solution[idx] == 1:\n                new_weight -= weight_lst[idx]\n            else:\n                new_weight += weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        improvement = 0\n        for idx in flip_indices:\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 0 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 0 else -value2_lst[idx]\n            improvement += weight_v1 * delta_v1 + weight_v2 * delta_v2\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution.copy()\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Weighted random flip\n        feasible_indices = [i for i in range(len(new_solution))\n                          if (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                             (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if feasible_indices:\n            weights = np.array([value1_lst[i] + value2_lst[i] for i in feasible_indices])\n            weights = weights / np.sum(weights)\n            idx = np.random.choice(feasible_indices, p=weights)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.6467510495732438,
            1.8358054459095001
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution with highest potential improvement\n        improvement_potentials = []\n        for sol, obj in archive:\n            v1, v2 = obj\n            current_weight = np.sum(weight_lst[sol == 1])\n            remaining_capacity = capacity - current_weight\n            potential_v1 = np.sum(value1_lst[(sol == 0) & (weight_lst <= remaining_capacity)])\n            potential_v2 = np.sum(value2_lst[(sol == 0) & (weight_lst <= remaining_capacity)])\n            improvement_potentials.append(potential_v1 + potential_v2)\n        base_solution = archive[np.argmax(improvement_potentials)][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search with adaptive objective weighting\n    total_items = len(new_solution)\n    balance = np.sum(value1_lst[new_solution == 1]) / (np.sum(value2_lst[new_solution == 1]) + 1e-6)\n\n    # Dynamic neighborhood selection\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - np.tanh(np.abs(np.log(balance + 1e-6)))))))\n\n    # Weighted objective function\n    if balance > 1.5:\n        weight_v1, weight_v2 = 0.8, 0.2\n    elif balance < 0.67:\n        weight_v1, weight_v2 = 0.2, 0.8\n    else:\n        weight_v1, weight_v2 = 0.5, 0.5\n\n    # Multi-flip with probabilistic selection\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for _ in range(min(4, neighborhood_size)):\n        temp_solution = new_solution.copy()\n        flip_count = np.random.randint(1, min(5, neighborhood_size))\n        flip_indices = np.random.choice(total_items, size=flip_count, replace=False, p=weight_lst/np.sum(weight_lst))\n\n        new_weight = current_weight\n        for idx in flip_indices:\n            if temp_solution[idx] == 1:\n                new_weight -= weight_lst[idx]\n            else:\n                new_weight += weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        improvement = 0\n        for idx in flip_indices:\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 0 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 0 else -value2_lst[idx]\n            improvement += weight_v1 * delta_v1 + weight_v2 * delta_v2\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution.copy()\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Weighted random flip\n        feasible_indices = [i for i in range(len(new_solution))\n                          if (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                             (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if feasible_indices:\n            weights = np.array([value1_lst[i] + value2_lst[i] for i in feasible_indices])\n            weights = weights / np.sum(weights)\n            idx = np.random.choice(feasible_indices, p=weights)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        None\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution with highest weighted objective diversity\n        weighted_diversities = []\n        for sol, obj in archive:\n            v1, v2 = obj\n            diversity = (v1 / (v2 + 1e-6)) + (v2 / (v1 + 1e-6))\n            weighted_diversities.append(diversity)\n        base_solution = archive[np.argmax(weighted_diversities)][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive neighborhood selection based on solution balance\n    total_items = len(new_solution)\n    balance = np.sum(value1_lst[new_solution == 1]) / (np.sum(value2_lst[new_solution == 1]) + 1e-6)\n    neighborhood_size = max(1, int(total_items * (0.3 + 0.2 * (1 - min(balance, 1/balance)))))\n\n    # Dynamic objective weighting based on solution's current imbalance\n    if balance > 1:\n        weight_v1, weight_v2 = 0.7, 0.3\n    elif balance < 1:\n        weight_v1, weight_v2 = 0.3, 0.7\n    else:\n        weight_v1, weight_v2 = 0.5, 0.5\n\n    # Multi-flip local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for _ in range(min(3, neighborhood_size)):\n        temp_solution = new_solution.copy()\n        # Select multiple items to flip (1-3)\n        flip_count = np.random.randint(1, min(4, neighborhood_size))\n        flip_indices = np.random.choice(total_items, size=flip_count, replace=False)\n\n        new_weight = current_weight\n        for idx in flip_indices:\n            if temp_solution[idx] == 1:\n                new_weight -= weight_lst[idx]\n            else:\n                new_weight += weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        improvement = 0\n        for idx in flip_indices:\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 0 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 0 else -value2_lst[idx]\n            improvement += weight_v1 * delta_v1 + weight_v2 * delta_v2\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution.copy()\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Feasibility-preserving random flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 113,
        "algorithm": "The algorithm selects a solution from the archive based on a combination of objective diversity and solution density, then applies a hybrid local search that combines randomized flips with a greedy improvement phase, while dynamically adjusting the neighborhood size and objective weights based on the solution's balance between objectives. It prioritizes solutions with higher diversity and density scores, and uses a weighted approach to balance improvements in both objectives, with dynamic adjustments to neighborhood size and objective weights to guide the search toward better solutions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution with highest diversity and solution density\n        diversities = []\n        for sol, obj in archive:\n            v1, v2 = obj\n            diversity = (v1 / (v2 + 1e-6)) + (v2 / (v1 + 1e-6))\n            density = np.sum(sol) / len(sol)\n            combined_score = diversity * (1 + 0.5 * density)\n            diversities.append(combined_score)\n        base_solution = archive[np.argmax(diversities)][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution's position in Pareto front\n    total_items = len(new_solution)\n    balance = np.sum(value1_lst[new_solution == 1]) / (np.sum(value2_lst[new_solution == 1]) + 1e-6)\n    neighborhood_size = max(2, int(total_items * (0.2 + 0.3 * (1 - min(balance, 1/balance)))))\n\n    # Objective weighting based on solution's position in Pareto front\n    if balance > 1.2:\n        weight_v1, weight_v2 = 0.8, 0.2\n    elif balance < 0.8:\n        weight_v1, weight_v2 = 0.2, 0.8\n    else:\n        weight_v1, weight_v2 = 0.5, 0.5\n\n    # Hybrid local search: randomized flips followed by greedy improvement\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Randomized flip phase\n    for _ in range(min(5, neighborhood_size)):\n        temp_solution = new_solution.copy()\n        flip_count = np.random.randint(1, min(5, neighborhood_size))\n        flip_indices = np.random.choice(total_items, size=flip_count, replace=False)\n\n        new_weight = current_weight\n        for idx in flip_indices:\n            if temp_solution[idx] == 1:\n                new_weight -= weight_lst[idx]\n            else:\n                new_weight += weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        improvement = 0\n        for idx in flip_indices:\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 0 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 0 else -value2_lst[idx]\n            improvement += weight_v1 * delta_v1 + weight_v2 * delta_v2\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution.copy()\n\n    # Greedy improvement phase\n    if best_candidate is not None:\n        new_solution = best_candidate\n        current_weight = np.sum(weight_lst[new_solution == 1])\n\n        # Try to add items that improve both objectives\n        for _ in range(3):\n            candidate_items = [i for i in range(total_items) if new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity]\n            if not candidate_items:\n                break\n\n            best_item = None\n            best_score = -float('inf')\n            for i in candidate_items:\n                score = weight_v1 * value1_lst[i] + weight_v2 * value2_lst[i]\n                if score > best_score:\n                    best_score = score\n                    best_item = i\n\n            if best_item is not None:\n                new_solution[best_item] = 1\n                current_weight += weight_lst[best_item]\n    else:\n        # Feasibility-preserving random flip if no improvement found\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.42098427386037496,
            5.208342015743256
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution with highest diversity and solution density\n        diversities = []\n        for sol, obj in archive:\n            v1, v2 = obj\n            diversity = (v1 / (v2 + 1e-6)) + (v2 / (v1 + 1e-6))\n            density = np.sum(sol) / len(sol)\n            combined_score = diversity * (1 + 0.5 * density)\n            diversities.append(combined_score)\n        base_solution = archive[np.argmax(diversities)][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution's position in Pareto front\n    total_items = len(new_solution)\n    balance = np.sum(value1_lst[new_solution == 1]) / (np.sum(value2_lst[new_solution == 1]) + 1e-6)\n    neighborhood_size = max(2, int(total_items * (0.2 + 0.3 * (1 - min(balance, 1/balance)))))\n\n    # Objective weighting based on solution's position in Pareto front\n    if balance > 1.2:\n        weight_v1, weight_v2 = 0.8, 0.2\n    elif balance < 0.8:\n        weight_v1, weight_v2 = 0.2, 0.8\n    else:\n        weight_v1, weight_v2 = 0.5, 0.5\n\n    # Hybrid local search: randomized flips followed by greedy improvement\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Randomized flip phase\n    for _ in range(min(5, neighborhood_size)):\n        temp_solution = new_solution.copy()\n        flip_count = np.random.randint(1, min(5, neighborhood_size))\n        flip_indices = np.random.choice(total_items, size=flip_count, replace=False)\n\n        new_weight = current_weight\n        for idx in flip_indices:\n            if temp_solution[idx] == 1:\n                new_weight -= weight_lst[idx]\n            else:\n                new_weight += weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        improvement = 0\n        for idx in flip_indices:\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 0 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 0 else -value2_lst[idx]\n            improvement += weight_v1 * delta_v1 + weight_v2 * delta_v2\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution.copy()\n\n    # Greedy improvement phase\n    if best_candidate is not None:\n        new_solution = best_candidate\n        current_weight = np.sum(weight_lst[new_solution == 1])\n\n        # Try to add items that improve both objectives\n        for _ in range(3):\n            candidate_items = [i for i in range(total_items) if new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity]\n            if not candidate_items:\n                break\n\n            best_item = None\n            best_score = -float('inf')\n            for i in candidate_items:\n                score = weight_v1 * value1_lst[i] + weight_v2 * value2_lst[i]\n                if score > best_score:\n                    best_score = score\n                    best_item = i\n\n            if best_item is not None:\n                new_solution[best_item] = 1\n                current_weight += weight_lst[best_item]\n    else:\n        # Feasibility-preserving random flip if no improvement found\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 5 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution with highest weighted objective diversity\n        weighted_diversities = []\n        for sol, obj in archive:\n            v1, v2 = obj\n            diversity = (v1 / (v2 + 1e-6)) + (v2 / (v1 + 1e-6))\n            weighted_diversities.append(diversity)\n        base_solution = archive[np.argmax(weighted_diversities)][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive neighborhood selection based on solution balance\n    total_items = len(new_solution)\n    balance = np.sum(value1_lst[new_solution == 1]) / (np.sum(value2_lst[new_solution == 1]) + 1e-6)\n    neighborhood_size = max(1, int(total_items * (0.3 + 0.2 * (1 - min(balance, 1/balance)))))\n\n    # Dynamic objective weighting based on solution's current imbalance\n    if balance > 1:\n        weight_v1, weight_v2 = 0.7, 0.3\n    elif balance < 1:\n        weight_v1, weight_v2 = 0.3, 0.7\n    else:\n        weight_v1, weight_v2 = 0.5, 0.5\n\n    # Multi-flip local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for _ in range(min(3, neighborhood_size)):\n        temp_solution = new_solution.copy()\n        # Select multiple items to flip (1-3)\n        flip_count = np.random.randint(1, min(4, neighborhood_size))\n        flip_indices = np.random.choice(total_items, size=flip_count, replace=False)\n\n        new_weight = current_weight\n        for idx in flip_indices:\n            if temp_solution[idx] == 1:\n                new_weight -= weight_lst[idx]\n            else:\n                new_weight += weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        improvement = 0\n        for idx in flip_indices:\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 0 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 0 else -value2_lst[idx]\n            improvement += weight_v1 * delta_v1 + weight_v2 * delta_v2\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution.copy()\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Feasibility-preserving random flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm combines adaptive Pareto-front clustering with weighted path relinking to generate neighbors, prioritizing solutions from the largest cluster and dynamically adjusting flip probabilities based on item contributions to both objectives while ensuring feasibility through precomputed weight validation and greedy fallback moves.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Cluster solutions by Pareto dominance\n        clusters = []\n        for sol, obj in archive:\n            assigned = False\n            for cluster in clusters:\n                if all(any(obj[i] >= c_obj[i] for i in range(2)) and not all(obj[i] >= c_obj[i] for i in range(2)) for _, c_obj in cluster):\n                    cluster.append((sol, obj))\n                    assigned = True\n                    break\n            if not assigned:\n                clusters.append([(sol, obj)])\n\n        # Select base solution from largest cluster\n        largest_cluster = max(clusters, key=len)\n        base_solution = random.choice(largest_cluster)[0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Calculate objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Path relinking with weighted contributions\n    target_solution = random.choice([sol for sol, _ in largest_cluster if not np.array_equal(sol, base_solution)])\n    diff_indices = np.where(base_solution != target_solution)[0]\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in diff_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Greedy diversity-aware flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Prefer items with high contribution to underrepresented objective\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto ranking with crowding-distance selection to prioritize diverse, high-quality solutions from the archive, then applies an adaptive cluster-based local search that dynamically adjusts flip cluster sizes based on weight utilization and objective weights, ensuring feasibility through precomputed item contributions and move validation. It balances exploration/exploitation by flipping items within clusters while favoring moves that improve both objectives proportionally, falling back to random valid flips if no improvement is found.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max(obj[0] for _, obj in pareto_front) - min(obj[0] for _, obj in pareto_front) + 1e-6) + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max(obj[1] for _, obj in pareto_front) - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                crowding_distances.append(dist)\n\n            # Select solution with highest crowding distance\n            max_dist_idx = np.argmax(crowding_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive cluster-based item flips\n    cluster_size = max(1, int(len(new_solution) * 0.1 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            **Keywords**: Adaptive selection, hybrid local search, Pareto-aware, dynamic feasibility\n\n**Advice**:\n1. Use weighted crowding-distance to prioritize high-diversity solutions.\n2. Hybridize local search with adaptive subset flips and objective-weighted swaps.\n3. Dynamically adjust neighborhood size based on solution quality.\n4. Precompute item contributions and validate feasibility greedily.\n\n**Avoid**:\n- Static operators (fixed-size swaps).\n- Random dominance-based selection; prefer Pareto-ranked candidates.\n- Ignoring weight constraints; always validate feasibility.\n\n**Explanation**: This approach balances exploration and exploitation by adaptively selecting solutions and using hybrid, dynamic operators to escape local optima while ensuring feasibility. *(Word count: 50)*\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 114,
        "algorithm": "This algorithm combines Pareto-aware solution selection with weighted crowding-distance metrics to prioritize diverse solutions, followed by a dynamic hybrid local search that adaptively adjusts neighborhood size based on weight utilization and solution quality, ensuring feasibility through rigorous constraint validation and probabilistic flips when no improving moves are found. The method dynamically balances exploration and exploitation by selecting solutions with high crowding distances and using weighted objective contributions to guide local search, with neighborhood size and flip probabilities adjusted based on current weight utilization and solution value distributions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and weighted crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                # Weighted crowding distance with dynamic weights\n                total_v1 = np.sum(value1_lst[sol == 1])\n                total_v2 = np.sum(value2_lst[sol == 1])\n                weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n                weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) * weight_v1 + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) * weight_v2\n                crowding_distances.append(dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(crowding_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and weight utilization\n    total_items = len(new_solution)\n    weight_utilization = current_weight / capacity\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - weight_utilization))))\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with subset flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Randomly select a subset of items to consider\n    subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution.copy()\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential weighted contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.7406991881339582,
            0.835734635591507
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and weighted crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                # Weighted crowding distance with dynamic weights\n                total_v1 = np.sum(value1_lst[sol == 1])\n                total_v2 = np.sum(value2_lst[sol == 1])\n                weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n                weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) * weight_v1 + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) * weight_v2\n                crowding_distances.append(dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(crowding_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and weight utilization\n    total_items = len(new_solution)\n    weight_utilization = current_weight / capacity\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - weight_utilization))))\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with subset flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    # Randomly select a subset of items to consider\n    subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution.copy()\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential weighted contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have 8 existing algorithms with their codes as follows:\n        No. 1 algorithm's description and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThis heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm first identifies the Pareto front from the archive and selects a base solution using a weighted crowding-distance metric to prioritize high-diversity solutions. It then applies a hybrid local search that dynamically adjusts the neighborhood size based on the solution's weight utilization and objective weights, evaluating flips within the neighborhood while ensuring feasibility. If no improving move is found, it performs a probabilistic flip based on potential weighted contributions, dynamically adjusting objective weights to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Identify Pareto front\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            min_v1 = min(obj[0] for _, obj in pareto_front)\n            min_v2 = min(obj[1] for _, obj in pareto_front)\n\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min_v1 + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min_v2 + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto front clustering with dynamic objective-weighted cluster flips, prioritizing solutions based on crowding distance and hypervolume contributions while adaptively adjusting flip probabilities and cluster sizes to balance exploration and exploitation, using momentum to emphasize high-potential regions of the search space. Key design features include weighted improvement calculations, adaptive cluster sizing, and probabilistic flips with temperature-based selection to ensure feasible neighbor solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster solutions based on Pareto dominance and crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest combined crowding distance and hypervolume contribution\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster flip with momentum\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization and solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.3 + 0.2 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.9  # Higher momentum for more aggressive search\n\n    # Evaluate cluster flips with momentum\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm first filters non-dominated solutions from the archive and selects a base solution using a weighted crowding-distance metric that balances Pareto dominance and diversity. It then applies a hybrid local search with dynamic neighborhood sizes (20-50% of items) based on weight utilization, using objective-weighted improvement criteria to prioritize flips that enhance both objectives proportionally. If no improvement is found, it performs a probabilistic flip based on potential contributions while ensuring feasibility. The method dynamically adjusts objective weights to balance exploration and exploitation, generating high-quality neighbors that dominate or complement existing solutions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with weighted crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic neighborhood size based on weight utilization\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 4: Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 5: Hybrid local search with cluster flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering both objectives\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 6 algorithm's description and the corresponding code are:\nThe algorithm selects a base solution from the archive using a weighted crowding-distance metric to prioritize high-diversity solutions, then applies a hybrid local search that adaptively flips 20-50% of items based on remaining capacity, with objective-weighted swaps and momentum-based improvement calculations to ensure feasibility and balance exploration/exploitation. The key variables are `neighborhood_size`, `weight_v1/v2`, and `momentum`, which dynamically adjust the search intensity and balance between objectives.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution using weighted crowding-distance metric\n        crowding_distances = []\n        sorted_archive = sorted(archive, key=lambda x: (x[1][0], x[1][1]))\n\n        for i in range(len(sorted_archive)):\n            if i == 0 or i == len(sorted_archive) - 1:\n                crowding_distances.append(float('inf'))\n            else:\n                dist_v1 = sorted_archive[i+1][1][0] - sorted_archive[i-1][1][0]\n                dist_v2 = sorted_archive[i+1][1][1] - sorted_archive[i-1][1][1]\n                crowding_distances.append(dist_v1 + dist_v2)\n\n        max_dist_idx = np.argmax(crowding_distances)\n        base_solution = sorted_archive[max_dist_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Dynamic neighborhood size (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, min(total_items, int(total_items * (0.2 + 0.3 * (remaining_capacity / capacity)))))\n\n    if neighborhood_size > 0:\n        # Adaptive subset flips with objective-weighted swaps\n        subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate objective weights\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = total_v1 / (total_v1 + total_v2 + 1e-6)\n        weight_v2 = total_v2 / (total_v1 + total_v2 + 1e-6)\n\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.7  # Higher momentum for more aggressive search\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                temperature = 1 - (remaining_capacity / capacity)\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 7 algorithm's description and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Identify Pareto front and select solution with highest weighted crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 2: Dynamic neighborhood size based on weight utilization and solution quality\n    total_items = len(new_solution)\n    weight_utilization = current_weight / capacity\n    solution_quality = (np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - weight_utilization) * solution_quality)))\n\n    # Step 3: Hybrid local search with weighted swaps and subset flips\n    subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.8  # Higher momentum for more aggressive search\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement with momentum\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 8 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using a weighted combination of crowding distance and hypervolume contribution, then applies a hybrid local search with dynamic neighborhood size, weighted objective swaps, and probabilistic exploration to generate a feasible neighbor solution that balances both objectives while prioritizing higher-quality improvements. The method adapts to solution quality and weight utilization, ensuring feasible moves while favoring high-impact changes based on dynamic objective weights and momentum.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest weighted crowding-distance and hypervolume contribution\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 2: Dynamic neighborhood size based on weight utilization and solution quality\n    total_items = len(new_solution)\n    weight_utilization = current_weight / capacity\n    solution_quality = (np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - weight_utilization) * solution_quality)))\n\n    # Step 3: Hybrid local search with weighted swaps and subset flips\n    subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.8\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n        Adjusted long-term guide: **Keywords**: Adaptive selection, hybrid local search, Pareto-aware, dynamic feasibility\n\n**Advice**:\n1. Use **weighted crowding-distance** to select high-diversity solutions.\n2. Hybridize local search with **adaptive subset flips** and **objective-weighted swaps**.\n3. Dynamically adjust neighborhood size based on solution quality.\n4. Precompute item contributions and validate feasibility greedily.\n\n**Avoid**:\n- Static operators (fixed-size swaps).\n- Random dominance-based selection; prefer Pareto-ranked candidates.\n- Ignoring weight constraints; always validate feasibility.\n\n**Explanation**: This approach balances exploration and exploitation by adaptively selecting solutions and using hybrid, dynamic operators to escape local optima while ensuring feasibility. *(Word count: 50)*\n\n*(Total word count: 50)*\n        Please create a new algorithm inspired by above with better objectives, using the long-term guide.\n        1. Describe new algorithm in one sentence. Boxed {}.\n        2. Implement:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax. No extra explanations.",
        "sample_order": 115,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest weighted crowding-distance and hypervolume contribution\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 2: Dynamic neighborhood size based on weight utilization and solution quality\n    total_items = len(new_solution)\n    weight_utilization = current_weight / capacity\n    solution_quality = (np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - weight_utilization) * solution_quality)))\n\n    # Step 3: Hybrid local search with weighted swaps and subset flips\n    subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.8\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8562915179936009,
            0.6794827282428741
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest weighted crowding-distance and hypervolume contribution\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 2: Dynamic neighborhood size based on weight utilization and solution quality\n    total_items = len(new_solution)\n    weight_utilization = current_weight / capacity\n    solution_quality = (np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - weight_utilization) * solution_quality)))\n\n    # Step 3: Hybrid local search with weighted swaps and subset flips\n    subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.8\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "elitist"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nNone\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Identify Pareto front and select solution with highest weighted crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 2: Dynamic neighborhood size based on weight utilization and solution quality\n    total_items = len(new_solution)\n    weight_utilization = current_weight / capacity\n    solution_quality = (np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - weight_utilization) * solution_quality)))\n\n    # Step 3: Hybrid local search with weighted swaps and subset flips\n    subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.8  # Higher momentum for more aggressive search\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement with momentum\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines adaptive Pareto-aware selection (prioritizing solutions with high crowding distance and Pareto rank) with a hybrid local search that alternates between probabilistic subset flips (weighted by marginal gains in both objectives) and feasibility-preserving swaps, dynamically adjusting neighborhood size based on remaining capacity to ensure high-quality, diverse, and feasible neighbors. It emphasizes marginal improvement over both objectives while maintaining feasibility through careful weight tracking and constrained random swaps. The selection process prioritizes solutions that are both non-dominated and well-distributed in the objective space, while the local search intelligently explores the neighborhood to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive Pareto-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Value-weighted diversification\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Feasibility-preserving swaps\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            **Keywords**: Adaptive selection, hybrid local search, Pareto-aware, dynamic feasibility\n\n**Advice**:\n1. Use **weighted crowding-distance** to select high-diversity solutions.\n2. Hybridize local search with **adaptive subset flips** and **objective-weighted swaps**.\n3. Dynamically adjust neighborhood size based on solution quality.\n4. Precompute item contributions and validate feasibility greedily.\n\n**Avoid**:\n- Static operators (fixed-size swaps).\n- Random dominance-based selection; prefer Pareto-ranked candidates.\n- Ignoring weight constraints; always validate feasibility.\n\n**Explanation**: This approach balances exploration and exploitation by adaptively selecting solutions and using hybrid, dynamic operators to escape local optima while ensuring feasibility. *(Word count: 50)*\n\n*(Total word count: 50)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 116,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive solution selection based on Pareto rank and crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Dynamic neighborhood exploration with value-weighted subset flips\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    remaining_capacity = capacity - current_weight\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Determine neighborhood size based on remaining capacity and solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    neighborhood_size = max(1, int(n_items * (0.1 + 0.4 * (remaining_capacity / capacity) * (1 - solution_quality))))\n\n    # Generate candidate subsets for exploration\n    candidate_indices = np.random.choice(n_items, size=min(neighborhood_size, n_items), replace=False)\n\n    # Evaluate candidate flips with weighted objective improvement\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in candidate_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement calculation\n        weighted_improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if weighted_improvement > best_improvement:\n            best_improvement = weighted_improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    # Step 3: Feasibility-preserving refinement with value-weighted swaps\n    if best_improvement > 0:\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            # Calculate value ratios for weighted selection\n            out_values = (weight_v1 * value1_lst[out_items] + weight_v2 * value2_lst[out_items]) / (weight_lst[out_items] + 1e-6)\n            in_values = (weight_v1 * value1_lst[in_items] + weight_v2 * value2_lst[in_items]) / (weight_lst[in_items] + 1e-6)\n\n            # Select items for swap based on value ratios\n            out_idx = np.random.choice(out_items, p=out_values/np.sum(out_values))\n            in_idx = np.random.choice(in_items, p=in_values/np.sum(in_values))\n\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.8627386138891171,
            0.751680999994278
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive solution selection based on Pareto rank and crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Dynamic neighborhood exploration with value-weighted subset flips\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    remaining_capacity = capacity - current_weight\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Determine neighborhood size based on remaining capacity and solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    neighborhood_size = max(1, int(n_items * (0.1 + 0.4 * (remaining_capacity / capacity) * (1 - solution_quality))))\n\n    # Generate candidate subsets for exploration\n    candidate_indices = np.random.choice(n_items, size=min(neighborhood_size, n_items), replace=False)\n\n    # Evaluate candidate flips with weighted objective improvement\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in candidate_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement calculation\n        weighted_improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if weighted_improvement > best_improvement:\n            best_improvement = weighted_improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    # Step 3: Feasibility-preserving refinement with value-weighted swaps\n    if best_improvement > 0:\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            # Calculate value ratios for weighted selection\n            out_values = (weight_v1 * value1_lst[out_items] + weight_v2 * value2_lst[out_items]) / (weight_lst[out_items] + 1e-6)\n            in_values = (weight_v1 * value1_lst[in_items] + weight_v2 * value2_lst[in_items]) / (weight_lst[in_items] + 1e-6)\n\n            # Select items for swap based on value ratios\n            out_idx = np.random.choice(out_items, p=out_values/np.sum(out_values))\n            in_idx = np.random.choice(in_items, p=in_values/np.sum(in_values))\n\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 2 existing algorithms with their codes as follows:\n            No. 1 algorithm and the corresponding code are:\nThe algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\nNo. 2 algorithm and the corresponding code are:\nThe algorithm combines adaptive Pareto-aware selection (prioritizing solutions with high crowding distance and Pareto rank) with a hybrid local search that alternates between probabilistic subset flips (weighted by marginal gains in both objectives) and feasibility-preserving swaps, dynamically adjusting neighborhood size based on remaining capacity to ensure high-quality, diverse, and feasible neighbors. It emphasizes marginal improvement over both objectives while maintaining feasibility through careful weight tracking and constrained random swaps. The selection process prioritizes solutions that are both non-dominated and well-distributed in the objective space, while the local search intelligently explores the neighborhood to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive Pareto-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Value-weighted diversification\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Feasibility-preserving swaps\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            **Keywords**: Adaptive selection, hybrid local search, Pareto-aware, dynamic feasibility\n\n**Advice**:\n1. Use **weighted crowding-distance** to select high-diversity solutions.\n2. Hybridize local search with **adaptive subset flips** and **objective-weighted swaps**.\n3. Dynamically adjust neighborhood size based on solution quality.\n4. Precompute item contributions and validate feasibility greedily.\n\n**Avoid**:\n- Static operators (fixed-size swaps).\n- Random dominance-based selection; prefer Pareto-ranked candidates.\n- Ignoring weight constraints; always validate feasibility.\n\n**Explanation**: This approach balances exploration and exploitation by adaptively selecting solutions and using hybrid, dynamic operators to escape local optima while ensuring feasibility. *(Word count: 50)*\n\n*(Total word count: 50)*\n\n            Please help me create a new algorithm that has a totally different form from the given ones but can be motivated from them and the above long-term reflection.\n            1. Firstly, identify the common backbone idea in the provided algorithms.\n            2. Secondly, based on both the backbone idea and the long-term reflection, describe your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax and code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 117,
        "algorithm": "This algorithm combines adaptive Pareto-frontier selection with a hybrid local search that dynamically explores neighborhoods via weighted item clustering and probabilistic subset flips, adjusting neighborhood size based on solution proximity to the Pareto frontier while ensuring feasibility through constrained swaps that prioritize marginal improvements in both objectives. It alternates between cluster-based flips and feasibility-preserving swaps to balance exploration and exploitation, with higher priority given to items with better combined value-to-weight ratios.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive Pareto-frontier selection\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if not pareto_front:\n        pareto_front = list(range(len(archive)))\n\n    selected_idx = np.random.choice(pareto_front)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Dynamic neighborhood exploration via weighted item clustering\n    n_items = len(weight_lst)\n    cluster_size = min(10, max(3, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Compute item contributions and cluster items\n    item_scores = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_items = np.argsort(-item_scores)\n    clusters = [sorted_items[i:i+cluster_size] for i in range(0, n_items, cluster_size)]\n\n    # Hybrid local search: probabilistic subset flips and feasibility-preserving swaps\n    for cluster in clusters:\n        if len(cluster) == 0:\n            continue\n\n        # Probabilistic flip within cluster\n        if random.random() < 0.7:  # Higher probability for clusters\n            flip_idx = random.choice(cluster)\n            if new_solution[flip_idx] == 1:\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n            else:\n                new_weight = current_weight + weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 1\n                    current_weight = new_weight\n\n        # Feasibility-preserving swap\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                if (delta_v1 + delta_v2) > 0:\n                    new_solution[out_idx] = 0\n                    new_solution[in_idx] = 1\n                    current_weight = new_weight\n\n    return new_solution\n\n",
        "score": [
            -0.34305681311366865,
            1.572097897529602
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive Pareto-frontier selection\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if not pareto_front:\n        pareto_front = list(range(len(archive)))\n\n    selected_idx = np.random.choice(pareto_front)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n    new_solution = base_solution.copy()\n\n    # Dynamic neighborhood exploration via weighted item clustering\n    n_items = len(weight_lst)\n    cluster_size = min(10, max(3, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Compute item contributions and cluster items\n    item_scores = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    sorted_items = np.argsort(-item_scores)\n    clusters = [sorted_items[i:i+cluster_size] for i in range(0, n_items, cluster_size)]\n\n    # Hybrid local search: probabilistic subset flips and feasibility-preserving swaps\n    for cluster in clusters:\n        if len(cluster) == 0:\n            continue\n\n        # Probabilistic flip within cluster\n        if random.random() < 0.7:  # Higher probability for clusters\n            flip_idx = random.choice(cluster)\n            if new_solution[flip_idx] == 1:\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n            else:\n                new_weight = current_weight + weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 1\n                    current_weight = new_weight\n\n        # Feasibility-preserving swap\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                if (delta_v1 + delta_v2) > 0:\n                    new_solution[out_idx] = 0\n                    new_solution[in_idx] = 1\n                    current_weight = new_weight\n\n    return new_solution\n\n",
        "operation": "e2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines adaptive Pareto-aware selection (prioritizing solutions with high crowding distance and Pareto rank) with a hybrid local search that alternates between probabilistic subset flips (weighted by marginal gains in both objectives) and feasibility-preserving swaps, dynamically adjusting neighborhood size based on remaining capacity to ensure high-quality, diverse, and feasible neighbors. It emphasizes marginal improvement over both objectives while maintaining feasibility through careful weight tracking and constrained random swaps. The selection process prioritizes solutions that are both non-dominated and well-distributed in the objective space, while the local search intelligently explores the neighborhood to balance exploration and exploitation.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive Pareto-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Value-weighted diversification\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Feasibility-preserving swaps\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\n        Please create a new algorithm that has a different form but can be a modified version of the provided algorithm. Attempt to introduce more novel mechanisms and new equations or programme segments.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 118,
        "algorithm": "The algorithm combines adaptive Pareto frontier analysis with a balanced neighborhood exploration, prioritizing solutions near the frontier and items with high combined value-to-weight ratios. It uses a two-phase approach: first probabilistically flips items to improve both objectives, then performs constrained substitutions to trade off items while maintaining feasibility. The selection emphasizes balanced improvements in both objectives, with a fallback to strategic substitutions when no further improvements are found.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Dynamic Pareto frontier analysis\n    objectives = np.array([obj for _, obj in archive])\n    ideal = np.max(objectives, axis=0)\n    nadir = np.min(objectives, axis=0)\n    normalized = (objectives - nadir) / (ideal - nadir + 1e-6)\n\n    # Select solution with highest balanced frontier score\n    frontier_scores = np.max(normalized, axis=1) - np.min(normalized, axis=1)\n    selected_idx = np.argmax(frontier_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Objective-balanced neighborhood exploration\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    exploration_size = min(3, max(1, int(n_items * 0.05 * (1 - current_weight / capacity))))\n\n    for _ in range(exploration_size):\n        # Select candidate items with balanced potential\n        item_scores = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        item_scores[new_solution == 1] *= 0.5\n        candidate_indices = np.argsort(item_scores)[-exploration_size*2:]\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in candidate_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            # Balanced improvement metric\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 * delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Constrained substitution phase\n    if best_improvement == 0:\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            # Find best substitution pair\n            best_pair = None\n            best_pair_score = -np.inf\n\n            for out_idx in out_items:\n                for in_idx in in_items:\n                    new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                    if new_weight <= capacity:\n                        # Trade-off score\n                        score = (value1_lst[in_idx] - value1_lst[out_idx]) * (value2_lst[in_idx] - value2_lst[out_idx])\n                        if score > best_pair_score:\n                            best_pair_score = score\n                            best_pair = (out_idx, in_idx)\n\n            if best_pair is not None:\n                out_idx, in_idx = best_pair\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.9473491897489388,
            0.43327468633651733
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Dynamic Pareto frontier analysis\n    objectives = np.array([obj for _, obj in archive])\n    ideal = np.max(objectives, axis=0)\n    nadir = np.min(objectives, axis=0)\n    normalized = (objectives - nadir) / (ideal - nadir + 1e-6)\n\n    # Select solution with highest balanced frontier score\n    frontier_scores = np.max(normalized, axis=1) - np.min(normalized, axis=1)\n    selected_idx = np.argmax(frontier_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Objective-balanced neighborhood exploration\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    exploration_size = min(3, max(1, int(n_items * 0.05 * (1 - current_weight / capacity))))\n\n    for _ in range(exploration_size):\n        # Select candidate items with balanced potential\n        item_scores = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n        item_scores[new_solution == 1] *= 0.5\n        candidate_indices = np.argsort(item_scores)[-exploration_size*2:]\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in candidate_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            # Balanced improvement metric\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 * delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Constrained substitution phase\n    if best_improvement == 0:\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            # Find best substitution pair\n            best_pair = None\n            best_pair_score = -np.inf\n\n            for out_idx in out_items:\n                for in_idx in in_items:\n                    new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                    if new_weight <= capacity:\n                        # Trade-off score\n                        score = (value1_lst[in_idx] - value1_lst[out_idx]) * (value2_lst[in_idx] - value2_lst[out_idx])\n                        if score > best_pair_score:\n                            best_pair_score = score\n                            best_pair = (out_idx, in_idx)\n\n            if best_pair is not None:\n                out_idx, in_idx = best_pair\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n\n    return new_solution\n\n",
        "operation": "m1"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n        I have one algorithm with its code as follows. Algorithm description:\n        The algorithm combines adaptive Pareto-aware selection (prioritizing solutions with high crowding distance and Pareto rank) with a hybrid local search that alternates between probabilistic subset flips (weighted by marginal gains in both objectives) and feasibility-preserving swaps, dynamically adjusting neighborhood size based on remaining capacity to ensure high-quality, diverse, and feasible neighbors. It emphasizes marginal improvement over both objectives while maintaining feasibility through careful weight tracking and constrained random swaps. The selection process prioritizes solutions that are both non-dominated and well-distributed in the objective space, while the local search intelligently explores the neighborhood to balance exploration and exploitation.\n        Code:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive Pareto-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Value-weighted diversification\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Feasibility-preserving swaps\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\n        Please identify the main algorithm parameters and help me in creating a new algorithm that has different parameter settings to equations compared to the provided algorithm.\n        1. First, describe your new algorithm and main steps in one sentence. The description must be inside within boxed {}.\n        2. Next, implement the idea in the following Python function:\n        def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n        Check syntax, code carefully before returning the final function. Do not give additional explanations.",
        "sample_order": 119,
        "algorithm": "This algorithm combines Pareto-frontier selection with criticality-weighted local search to generate high-quality neighbors for the Bi-objective Knapsack Problem. It prioritizes solutions on the Pareto frontier and uses item criticality scores to guide flips and swaps, while dynamically adjusting neighborhood size based on remaining capacity to balance exploration and exploitation. The method ensures feasibility by checking weight constraints before applying changes.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Dynamic Pareto-frontier selection with criticality weighting\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if not pareto_front:\n        pareto_front = list(range(len(archive)))\n\n    # Calculate criticality scores (value/weight ratio)\n    criticality = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    selection_probs = np.zeros(len(archive))\n    for i in pareto_front:\n        selection_probs[i] = np.sum(criticality[archive[i][0] == 1]) / np.sum(archive[i][0])\n\n    selected_idx = np.random.choice(len(archive), p=selection_probs/np.sum(selection_probs))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive neighborhood exploration\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = min(10, max(3, int(n_items * 0.2 * (remaining_capacity / capacity))))\n\n    # Criticality-guided flips\n    for _ in range(neighborhood_size):\n        flip_candidates = np.where(new_solution == 1)[0] if np.random.rand() < 0.7 else np.where(new_solution == 0)[0]\n        if len(flip_candidates) == 0:\n            continue\n\n        candidate_scores = criticality[flip_candidates] * (1 if new_solution[flip_candidates[0]] == 1 else -1)\n        candidate_idx = np.random.choice(flip_candidates, p=candidate_scores/np.sum(candidate_scores))\n\n        temp_solution = new_solution.copy()\n        temp_solution[candidate_idx] = 1 - temp_solution[candidate_idx]\n\n        new_weight = current_weight\n        if temp_solution[candidate_idx] == 1:\n            new_weight += weight_lst[candidate_idx]\n        else:\n            new_weight -= weight_lst[candidate_idx]\n\n        if new_weight <= capacity:\n            new_solution = temp_solution\n            current_weight = new_weight\n\n    # Feasibility-aware swaps\n    if np.random.rand() < 0.3:\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_scores = criticality[out_items] * (current_weight - weight_lst[out_items]) / capacity\n            in_scores = criticality[in_items] * weight_lst[in_items] / capacity\n\n            out_idx = np.random.choice(out_items, p=out_scores/np.sum(out_scores))\n            in_idx = np.random.choice(in_items, p=in_scores/np.sum(in_scores))\n\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.5065843723983441,
            3.5859779715538025
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Dynamic Pareto-frontier selection with criticality weighting\n    objectives = np.array([obj for _, obj in archive])\n    pareto_front = []\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_front.append(i)\n\n    if not pareto_front:\n        pareto_front = list(range(len(archive)))\n\n    # Calculate criticality scores (value/weight ratio)\n    criticality = (value1_lst + value2_lst) / (weight_lst + 1e-6)\n    selection_probs = np.zeros(len(archive))\n    for i in pareto_front:\n        selection_probs[i] = np.sum(criticality[archive[i][0] == 1]) / np.sum(archive[i][0])\n\n    selected_idx = np.random.choice(len(archive), p=selection_probs/np.sum(selection_probs))\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Adaptive neighborhood exploration\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = min(10, max(3, int(n_items * 0.2 * (remaining_capacity / capacity))))\n\n    # Criticality-guided flips\n    for _ in range(neighborhood_size):\n        flip_candidates = np.where(new_solution == 1)[0] if np.random.rand() < 0.7 else np.where(new_solution == 0)[0]\n        if len(flip_candidates) == 0:\n            continue\n\n        candidate_scores = criticality[flip_candidates] * (1 if new_solution[flip_candidates[0]] == 1 else -1)\n        candidate_idx = np.random.choice(flip_candidates, p=candidate_scores/np.sum(candidate_scores))\n\n        temp_solution = new_solution.copy()\n        temp_solution[candidate_idx] = 1 - temp_solution[candidate_idx]\n\n        new_weight = current_weight\n        if temp_solution[candidate_idx] == 1:\n            new_weight += weight_lst[candidate_idx]\n        else:\n            new_weight -= weight_lst[candidate_idx]\n\n        if new_weight <= capacity:\n            new_solution = temp_solution\n            current_weight = new_weight\n\n    # Feasibility-aware swaps\n    if np.random.rand() < 0.3:\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_scores = criticality[out_items] * (current_weight - weight_lst[out_items]) / capacity\n            in_scores = criticality[in_items] * weight_lst[in_items] / capacity\n\n            out_idx = np.random.choice(out_items, p=out_scores/np.sum(out_scores))\n            in_idx = np.random.choice(in_items, p=in_scores/np.sum(in_scores))\n\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n\n    return new_solution\n\n",
        "operation": "m2"
    },
    {
        "prompt": "You are solving a Bi-objective Knapsack Problem (BI-KP), where each item has a weight and two profit values: value1 and value2. The goal is to select a subset of items such that the total weight does not exceed a given capacity, while simultaneously maximizing the total value in both objective spaces. Given an archive of non-dominated solutions, where each solution is a binary numpy array indicating item inclusion (1) or exclusion (0), and its corresponding objective is a tuple of two values (total value1, total value2), design a heuristic function named 'select_neighbor' that selects one solution from the archive and apply a novel or hybrid local search operator to generate a neighbor solution from it. Must always ensure that the generated neighbor solution remains feasible, i.e., the total weight must not exceed the knapsack capacity Please perform an intelligent random selection from among the solutions that show promising potential for further local improvement. Using a creative local search strategy that you design yourself, avoid 2-opt, go beyond standard approaches to design a method that yields higher-quality solutions across multiple objectives. The function should return the new neighbor solution.\n            I have 5 existing algorithms with their codes as follows:\n            No. 1 algorithm's description and the corresponding code are:\nThe algorithm combines adaptive Pareto-aware selection (prioritizing solutions with high crowding distance and Pareto rank) with a hybrid local search that alternates between probabilistic subset flips (weighted by marginal gains in both objectives) and feasibility-preserving swaps, dynamically adjusting neighborhood size based on remaining capacity to ensure high-quality, diverse, and feasible neighbors. It emphasizes marginal improvement over both objectives while maintaining feasibility through careful weight tracking and constrained random swaps. The selection process prioritizes solutions that are both non-dominated and well-distributed in the objective space, while the local search intelligently explores the neighborhood to balance exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive Pareto-aware selection\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Value-weighted diversification\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Feasibility-preserving swaps\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\nNo. 2 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive using Pareto-ranked crowding-distance metrics, then applies a hybrid local search combining adaptive subset flips and objective-weighted swaps, dynamically adjusting neighborhood size based on solution quality and feasibility to generate high-quality, feasible neighbor solutions. It prioritizes solutions with high crowding distance (diversity) and Pareto rank (non-dominated status), then uses weighted improvements in both objectives to guide local moves while ensuring feasibility through careful weight checks. The neighborhood size adapts to the remaining capacity, balancing exploration and exploitation.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Pareto-ranked selection with crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    selection_scores = crowding_distances * pareto_rank\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search with adaptive operators\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Adaptive subset flips with objective-weighted selection\n    for _ in range(neighborhood_size):\n        subset_size = min(5, max(1, int(n_items * 0.3)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Objective-weighted swaps if no improvement\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n\nNo. 3 algorithm's description and the corresponding code are:\nThe algorithm combines Pareto ranking with crowding-distance selection to prioritize diverse, high-quality solutions from the archive, then applies an adaptive cluster-based local search that dynamically adjusts flip cluster sizes based on weight utilization and objective weights, ensuring feasibility through precomputed item contributions and move validation. It balances exploration/exploitation by flipping items within clusters while favoring moves that improve both objectives proportionally, falling back to random valid flips if no improvement is found.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Pareto ranking and crowding-distance selection\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate crowding distances\n            crowding_distances = []\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max(obj[0] for _, obj in pareto_front) - min(obj[0] for _, obj in pareto_front) + 1e-6) + \\\n                       (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max(obj[1] for _, obj in pareto_front) - min(obj[1] for _, obj in pareto_front) + 1e-6)\n                crowding_distances.append(dist)\n\n            # Select solution with highest crowding distance\n            max_dist_idx = np.argmax(crowding_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Adaptive cluster-based item flips\n    cluster_size = max(1, int(len(new_solution) * 0.1 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    # Calculate objective weights based on current solution\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 4 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from the archive by prioritizing those with high normalized objective values, then applies a hybrid local search that dynamically adjusts weights based on the solution's dominance and evaluates flipping a random subset of items to maximize a weighted sum of both objectives, falling back to probabilistic flips if no improvement is found. It ensures feasibility by checking weight constraints and dynamically balances exploration/exploitation through weighted contributions.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic weight adjustment based on current solution's dominance\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives with dynamic weights\n    # 3. Apply a greedy selection based on the weighted combined objective improvement\n\n    subset_size = max(1, int(len(new_solution) * 0.3))  # 30% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contribution\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            # Calculate potential contributions\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n            # Select with probability proportional to contribution\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\nNo. 5 algorithm's description and the corresponding code are:\nThe algorithm selects a promising solution from an archive by prioritizing those with high normalized objective values, then applies a hybrid local search that evaluates flipping a random subset of items to maximize combined objective improvements while ensuring feasibility. If no improving move is found, it performs a random valid flip to maintain diversity. The selection and improvement criteria balance both objectives through a weighted sum, with weight normalization to avoid bias.\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Sort solutions by the sum of normalized objectives to prioritize promising ones\n        normalized_scores = []\n        max_v1 = max(obj[0] for _, obj in archive)\n        max_v2 = max(obj[1] for _, obj in archive)\n        for sol, obj in archive:\n            norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n            norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n            normalized_scores.append(norm_v1 + norm_v2)\n        # Select top 30% of solutions and choose randomly among them\n        threshold = np.percentile(normalized_scores, 70)\n        candidates = [sol for (sol, _), score in zip(archive, normalized_scores) if score >= threshold]\n        base_solution = random.choice(candidates).copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search strategy:\n    # 1. Randomly select a subset of items to potentially swap\n    # 2. Evaluate potential improvements in both objectives\n    # 3. Apply a greedy selection based on the combined objective improvement\n\n    # Step 1: Random subset selection\n    subset_size = max(1, int(len(new_solution) * 0.2))  # 20% of items\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 2: Evaluate potential improvements\n    best_improvement = 0\n    best_candidate = None\n\n    for idx in subset_indices:\n        # Try flipping the item\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        # Calculate new weight\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        # Check feasibility\n        if new_weight > capacity:\n            continue\n\n        # Calculate objective improvements\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Use a weighted sum of improvements as the selection criterion\n        improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)  # Avoid division by zero\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    # Step 3: Apply the best candidate if found\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # If no improvement found, perform a random valid flip\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n\n\n\n            Additionally, here is a long-term reflection that provides higher-level guidance for improvement:\n            **Keywords**: Adaptive selection, hybrid local search, Pareto-aware, dynamic feasibility\n\n**Advice**:\n1. Use **weighted crowding-distance** to select high-diversity solutions.\n2. Hybridize local search with **adaptive subset flips** and **objective-weighted swaps**.\n3. Dynamically adjust neighborhood size based on solution quality.\n4. Precompute item contributions and validate feasibility greedily.\n\n**Avoid**:\n- Static operators (fixed-size swaps).\n- Random dominance-based selection; prefer Pareto-ranked candidates.\n- Ignoring weight constraints; always validate feasibility.\n\n**Explanation**: This approach balances exploration and exploitation by adaptively selecting solutions and using hybrid, dynamic operators to escape local optima while ensuring feasibility. *(Word count: 50)*\n\n*(Total word count: 50)*\n\n            Please help me create a new algorithm that is inspired by all the above algorithms and the long-term reflection, aiming to achieve objective values lower than any of them.\n\n            1. Firstly, list some ideas in the provided algorithms and the long-term reflection that are clearly helpful for designing a better algorithm.\n            2. Secondly, based on the listed ideas, describe the design idea and main steps of your new algorithm in one long, detailed sentence. The description must be enclosed within boxed {}.\n            3. Thirdly, implement the idea in the following Python function:\n            def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    base_solution = archive[0][0].copy()\n    new_solution = base_solution.copy()\n    new_solution[0], new_solution[1] = new_solution[1], new_solution[0]\n\n    return new_solution\n\n\n\n            Check syntax, code carefully before returning the final function. Do not give any additional explanations.\n            ",
        "sample_order": 120,
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive Pareto-aware selection with weighted crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n\n    # Calculate weighted crowding distance\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Pareto ranking\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    # Combine crowding distance and pareto rank with adaptive weights\n    total_v1 = np.sum(value1_lst)\n    total_v2 = np.sum(value2_lst)\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    selection_scores = crowding_distances * pareto_rank * (weight_v1 * objectives[:, 0] + weight_v2 * objectives[:, 1])\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search with adaptive operators\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Adaptive cluster-based flip with objective-weighted evaluation\n    cluster_size = max(1, int(n_items * 0.2 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(n_items, size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n        current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Objective-weighted swaps if no improvement\n    if best_improvement == -float('inf'):\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                # Select items with probability based on their potential contribution\n                out_contributions = []\n                for out_idx in out_items:\n                    potential_v1 = -value1_lst[out_idx]\n                    potential_v2 = -value2_lst[out_idx]\n                    out_contributions.append(weight_v1 * potential_v1 + weight_v2 * potential_v2)\n\n                in_contributions = []\n                for in_idx in in_items:\n                    potential_v1 = value1_lst[in_idx]\n                    potential_v2 = value2_lst[in_idx]\n                    in_contributions.append(weight_v1 * potential_v1 + weight_v2 * potential_v2)\n\n                if sum(out_contributions) > 0 and sum(in_contributions) > 0:\n                    out_probs = [c / sum(out_contributions) for c in out_contributions]\n                    in_probs = [c / sum(in_contributions) for c in in_contributions]\n\n                    out_idx = np.random.choice(out_items, p=out_probs)\n                    in_idx = np.random.choice(in_items, p=in_probs)\n\n                    new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                    if new_weight <= capacity:\n                        delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                        delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                        if (weight_v1 * delta_v1 + weight_v2 * delta_v2) > 0:\n                            new_solution[out_idx] = 0\n                            new_solution[in_idx] = 1\n                            current_weight = new_weight\n                            break\n\n    # Fallback to random flip if no improvement found\n    if best_improvement == -float('inf'):\n        valid_indices = [i for i in range(n_items) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8323358605090554,
            0.8204853534698486
        ],
        "program": "\nimport numpy as np\nfrom typing import List, Tuple\nimport random \n\ndef select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    \"\"\"\n    Select a promising solution from the archive and generate a neighbor solution from it.\n\n    Args:\n    archive: List of (solution, objective) pairs. Each solution is a binary numpy array (0/1) of item selections.\n             Each objective is a tuple of two float values (total value1, total value2).\n    weight_lst: Numpy array of shape (N, ), item weights.\n    value1_lst: Numpy array of shape (N, ), item values for objective 1.\n    value2_lst: Numpy array of shape (N, ), item values for objective 2.\n    capacity: Maximum allowed total weight.\n\n    Returns:\n    A new neighbor solution (numpy array).\n    \"\"\"\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Adaptive Pareto-aware selection with weighted crowding distance\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n\n    # Calculate weighted crowding distance\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Pareto ranking\n    pareto_rank = np.zeros(len(archive))\n    for i, (sol, obj) in enumerate(archive):\n        dominated = False\n        for j, (_, other_obj) in enumerate(archive):\n            if i != j and other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n        if not dominated:\n            pareto_rank[i] = 1\n\n    # Combine crowding distance and pareto rank with adaptive weights\n    total_v1 = np.sum(value1_lst)\n    total_v2 = np.sum(value2_lst)\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    selection_scores = crowding_distances * pareto_rank * (weight_v1 * objectives[:, 0] + weight_v2 * objectives[:, 1])\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Hybrid local search with adaptive operators\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Adaptive cluster-based flip with objective-weighted evaluation\n    cluster_size = max(1, int(n_items * 0.2 * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(n_items, size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n        current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Objective-weighted swaps if no improvement\n    if best_improvement == -float('inf'):\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                # Select items with probability based on their potential contribution\n                out_contributions = []\n                for out_idx in out_items:\n                    potential_v1 = -value1_lst[out_idx]\n                    potential_v2 = -value2_lst[out_idx]\n                    out_contributions.append(weight_v1 * potential_v1 + weight_v2 * potential_v2)\n\n                in_contributions = []\n                for in_idx in in_items:\n                    potential_v1 = value1_lst[in_idx]\n                    potential_v2 = value2_lst[in_idx]\n                    in_contributions.append(weight_v1 * potential_v1 + weight_v2 * potential_v2)\n\n                if sum(out_contributions) > 0 and sum(in_contributions) > 0:\n                    out_probs = [c / sum(out_contributions) for c in out_contributions]\n                    in_probs = [c / sum(in_contributions) for c in in_contributions]\n\n                    out_idx = np.random.choice(out_items, p=out_probs)\n                    in_idx = np.random.choice(in_items, p=in_probs)\n\n                    new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                    if new_weight <= capacity:\n                        delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                        delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                        if (weight_v1 * delta_v1 + weight_v2 * delta_v2) > 0:\n                            new_solution[out_idx] = 0\n                            new_solution[in_idx] = 1\n                            current_weight = new_weight\n                            break\n\n    # Fallback to random flip if no improvement found\n    if best_improvement == -float('inf'):\n        valid_indices = [i for i in range(n_items) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "operation": "s1"
    }
]