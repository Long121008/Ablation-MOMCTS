[
    {
        "algorithm": "The algorithm selects a random solution from the archive and applies a hybrid local search combining probabilistic flips and item swaps, prioritizing item swaps to ensure feasibility while occasionally flipping items based on their value-to-weight ratio. It iteratively refines the solution by attempting up to 10 feasible moves, breaking early if a valid swap is found, to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Select a promising solution from the archive\n    selected_idx = random.randint(0, len(archive) - 1)\n    selected_solution, _ = archive[selected_idx]\n    base_solution = selected_solution.copy()\n    new_solution = base_solution.copy()\n\n    # Calculate current total weight\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Hybrid local search: probabilistic flips and item swaps\n    for _ in range(10):  # Number of attempts to find a feasible neighbor\n        # Try probabilistic flip: randomly flip items with probability based on their value-to-weight ratio\n        flip_candidates = np.where(new_solution == 1)[0]\n        if len(flip_candidates) > 0:\n            flip_idx = random.choice(flip_candidates)\n            if random.random() < 0.5:  # 50% chance to flip\n                new_weight = current_weight - weight_lst[flip_idx]\n                if new_weight <= capacity:\n                    new_solution[flip_idx] = 0\n                    current_weight = new_weight\n\n        # Try item swap: swap an item in with one out\n        out_items = np.where(new_solution == 1)[0]\n        in_items = np.where(new_solution == 0)[0]\n\n        if len(out_items) > 0 and len(in_items) > 0:\n            out_idx = random.choice(out_items)\n            in_idx = random.choice(in_items)\n\n            # Check if swap is feasible\n            new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n            if new_weight <= capacity:\n                new_solution[out_idx] = 0\n                new_solution[in_idx] = 1\n                current_weight = new_weight\n                break\n\n    return new_solution\n\n",
        "score": [
            -0.5069312012465736,
            0.26115044951438904
        ]
    },
    {
        "algorithm": "This heuristic selects a base solution from the archive using a hybrid of Pareto dominance (70%) and value-based selection (30%), then applies a dynamic neighborhood search with adaptive objective weighting and probabilistic item flipping to generate a high-quality neighbor solution while ensuring feasibility. The algorithm prioritizes solutions with higher hypervolume contributions on the Pareto front and dynamically adjusts neighborhood size and objective weights based on solution quality and remaining capacity. It uses momentum in improvement calculations and adaptive temperature for probabilistic flips to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Hybrid selection: 70% Pareto front, 30% value-based selection\n        if np.random.random() < 0.7:\n            # Pareto front selection\n            pareto_front = []\n            for sol, obj in archive:\n                dominated = False\n                to_remove = []\n                for i, (_, other_obj) in enumerate(pareto_front):\n                    if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                        dominated = True\n                        break\n                    if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                        to_remove.append(i)\n                if not dominated:\n                    for i in sorted(to_remove, reverse=True):\n                        del pareto_front[i]\n                    pareto_front.append((sol, obj))\n\n            if len(pareto_front) > 1:\n                # Calculate normalized hypervolume contributions\n                max_v1 = max(obj[0] for _, obj in pareto_front)\n                max_v2 = max(obj[1] for _, obj in pareto_front)\n                contributions = []\n                for i, (sol, obj) in enumerate(pareto_front):\n                    left = pareto_front[i-1][1] if i > 0 else (0, 0)\n                    right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n                    contribution = (right[0] - left[0]) * (right[1] - left[1])\n                    contributions.append(contribution)\n\n                # Select solution with highest hypervolume contribution\n                max_contrib_idx = np.argmax(contributions)\n                base_solution = pareto_front[max_contrib_idx][0].copy()\n            else:\n                base_solution = pareto_front[0][0].copy()\n        else:\n            # Value-based selection (select solution with highest sum of normalized objectives)\n            normalized_values = []\n            max_v1 = max(obj[0] for _, obj in archive)\n            max_v2 = max(obj[1] for _, obj in archive)\n            for sol, obj in archive:\n                norm_v1 = obj[0] / max_v1 if max_v1 > 0 else 0\n                norm_v2 = obj[1] / max_v2 if max_v2 > 0 else 0\n                normalized_values.append(norm_v1 + norm_v2)\n\n            max_value_idx = np.argmax(normalized_values)\n            base_solution = archive[max_value_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size based on solution quality and remaining capacity\n    total_items = len(new_solution)\n    solution_quality = np.sum(value1_lst[new_solution == 1] + value2_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n    neighborhood_size = max(1, int(total_items * (0.1 + 0.4 * (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)) *\n                    (remaining_capacity / capacity))))\n\n    if neighborhood_size > 0:\n        neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate dynamic objective weights with momentum\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = 0.7 * (total_v2 / (total_v1 + total_v2 + 1e-6)) + 0.3 * (total_v1 / (total_v1 + total_v2 + 1e-6))\n        weight_v2 = 1 - weight_v1\n\n        # Adaptive local search with momentum\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.8  # Higher momentum for more aggressive search\n\n        for idx in neighborhood_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                # Calculate adaptive temperature based on solution quality\n                temperature = 1 - (solution_quality / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6))\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8688276949567951,
            0.37794408202171326
        ]
    },
    {
        "algorithm": "The algorithm first identifies the Pareto front from the archive and selects a base solution using a weighted crowding-distance metric to prioritize high-diversity solutions. It then applies a hybrid local search that dynamically adjusts the neighborhood size based on the solution's weight utilization and objective weights, evaluating flips within the neighborhood while ensuring feasibility. If no improving move is found, it performs a probabilistic flip based on potential weighted contributions, dynamically adjusting objective weights to balance exploration and exploitation.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Identify Pareto front\n        pareto_front = []\n        for sol, obj in archive:\n            dominated = False\n            to_remove = []\n            for i, (_, other_obj) in enumerate(pareto_front):\n                if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                    dominated = True\n                    break\n                if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                    to_remove.append(i)\n            if not dominated:\n                for i in sorted(to_remove, reverse=True):\n                    del pareto_front[i]\n                pareto_front.append((sol, obj))\n\n        if len(pareto_front) > 1:\n            # Calculate weighted crowding distances\n            weighted_distances = []\n            max_v1 = max(obj[0] for _, obj in pareto_front)\n            max_v2 = max(obj[1] for _, obj in pareto_front)\n            min_v1 = min(obj[0] for _, obj in pareto_front)\n            min_v2 = min(obj[1] for _, obj in pareto_front)\n\n            for i, (sol, obj) in enumerate(pareto_front):\n                left = pareto_front[i-1][1] if i > 0 else (float('inf'), float('inf'))\n                right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (float('inf'), float('inf'))\n                dist_v1 = (abs(obj[0] - left[0]) + abs(obj[0] - right[0])) / (max_v1 - min_v1 + 1e-6)\n                dist_v2 = (abs(obj[1] - left[1]) + abs(obj[1] - right[1])) / (max_v2 - min_v2 + 1e-6)\n                weighted_dist = 0.5 * dist_v1 + 0.5 * dist_v2\n                weighted_distances.append(weighted_dist)\n\n            # Select solution with highest weighted crowding distance\n            max_dist_idx = np.argmax(weighted_distances)\n            base_solution = pareto_front[max_dist_idx][0].copy()\n        else:\n            base_solution = pareto_front[0][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Dynamic neighborhood size adjustment (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - current_weight / capacity))))\n    neighborhood_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Hybrid local search with weighted objective improvements\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in neighborhood_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential weighted contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8923749103075588,
            0.4989009499549866
        ]
    },
    {
        "algorithm": "The algorithm combines Pareto front clustering with dynamic objective-weighted cluster flips, prioritizing solutions based on crowding distance and hypervolume contributions while adaptively adjusting flip probabilities and cluster sizes to balance exploration and exploitation, using momentum to emphasize high-potential regions of the search space. Key design features include weighted improvement calculations, adaptive cluster sizing, and probabilistic flips with temperature-based selection to ensure feasible neighbor solutions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Cluster solutions based on Pareto dominance and crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with highest combined crowding distance and hypervolume contribution\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic cluster flip with momentum\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Adaptive cluster size based on weight utilization and solution quality\n    solution_quality = (total_v1 + total_v2) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    cluster_size = max(1, int(len(new_solution) * (0.3 + 0.2 * solution_quality) * (1 - current_weight / capacity)))\n    cluster_indices = np.random.choice(len(new_solution), size=cluster_size, replace=False)\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.9  # Higher momentum for more aggressive search\n\n    # Evaluate cluster flips with momentum\n    for idx in cluster_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                         (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                         (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -1.0478521338732665,
            0.5647115409374237
        ]
    },
    {
        "algorithm": "The algorithm first filters non-dominated solutions from the archive and selects a base solution using a weighted crowding-distance metric that balances Pareto dominance and diversity. It then applies a hybrid local search with dynamic neighborhood sizes (20-50% of items) based on weight utilization, using objective-weighted improvement criteria to prioritize flips that enhance both objectives proportionally. If no improvement is found, it performs a probabilistic flip based on potential contributions while ensuring feasibility. The method dynamically adjusts objective weights to balance exploration and exploitation, generating high-quality neighbors that dominate or complement existing solutions.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Filter non-dominated solutions\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    # Step 2: Select solution with weighted crowding distance\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Step 3: Dynamic neighborhood size based on weight utilization\n    weight_utilization = current_weight / capacity\n    subset_size = max(1, int(len(new_solution) * (0.2 + 0.3 * (1 - weight_utilization))))\n    subset_indices = np.random.choice(len(new_solution), size=subset_size, replace=False)\n\n    # Step 4: Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    # Step 5: Hybrid local search with cluster flips\n    best_improvement = -float('inf')\n    best_candidate = None\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement considering both objectives\n        improvement = weight_v1 * delta_v1 + weight_v2 * delta_v2\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip based on potential contributions\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = weight_v1 * potential_v1 + weight_v2 * potential_v2\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.9225959738247063,
            0.5165721774101257
        ]
    },
    {
        "algorithm": "The algorithm selects a base solution from the archive using a weighted crowding-distance metric to prioritize high-diversity solutions, then applies a hybrid local search that adaptively flips 20-50% of items based on remaining capacity, with objective-weighted swaps and momentum-based improvement calculations to ensure feasibility and balance exploration/exploitation. The key variables are `neighborhood_size`, `weight_v1/v2`, and `momentum`, which dynamically adjust the search intensity and balance between objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if len(archive) == 1:\n        base_solution = archive[0][0].copy()\n    else:\n        # Select solution using weighted crowding-distance metric\n        crowding_distances = []\n        sorted_archive = sorted(archive, key=lambda x: (x[1][0], x[1][1]))\n\n        for i in range(len(sorted_archive)):\n            if i == 0 or i == len(sorted_archive) - 1:\n                crowding_distances.append(float('inf'))\n            else:\n                dist_v1 = sorted_archive[i+1][1][0] - sorted_archive[i-1][1][0]\n                dist_v2 = sorted_archive[i+1][1][1] - sorted_archive[i-1][1][1]\n                crowding_distances.append(dist_v1 + dist_v2)\n\n        max_dist_idx = np.argmax(crowding_distances)\n        base_solution = sorted_archive[max_dist_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Dynamic neighborhood size (20-50% of items)\n    total_items = len(new_solution)\n    neighborhood_size = max(1, min(total_items, int(total_items * (0.2 + 0.3 * (remaining_capacity / capacity)))))\n\n    if neighborhood_size > 0:\n        # Adaptive subset flips with objective-weighted swaps\n        subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n        # Calculate objective weights\n        total_v1 = np.sum(value1_lst[new_solution == 1])\n        total_v2 = np.sum(value2_lst[new_solution == 1])\n        weight_v1 = total_v1 / (total_v1 + total_v2 + 1e-6)\n        weight_v2 = total_v2 / (total_v1 + total_v2 + 1e-6)\n\n        best_improvement = -float('inf')\n        best_candidate = None\n        momentum = 0.7  # Higher momentum for more aggressive search\n\n        for idx in subset_indices:\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            if new_weight > capacity:\n                continue\n\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Weighted improvement with momentum\n            improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n        else:\n            # Probabilistic flip with adaptive temperature\n            valid_indices = [i for i in range(len(new_solution)) if\n                           (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                           (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n            if valid_indices:\n                temperature = 1 - (remaining_capacity / capacity)\n                contributions = []\n                for i in valid_indices:\n                    potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                    potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                    contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                    contributions.append(contribution)\n\n                if sum(contributions) > 0:\n                    probabilities = [c / sum(contributions) for c in contributions]\n                    idx = np.random.choice(valid_indices, p=probabilities)\n                else:\n                    idx = random.choice(valid_indices)\n                new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8753876025617844,
            0.396170973777771
        ]
    },
    {
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Identify Pareto front and select solution with highest weighted crowding distance\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        # Weight crowding distances by solution quality\n        total_v1 = np.array([np.sum(value1_lst[sol == 1]) for sol, _ in pareto_front])\n        total_v2 = np.array([np.sum(value2_lst[sol == 1]) for sol, _ in pareto_front])\n        quality_scores = (total_v1 + total_v2) / (np.max(total_v1 + total_v2) + 1e-6)\n        weighted_distances = crowding_distances * quality_scores\n        selected_idx = np.argmax(weighted_distances)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 2: Dynamic neighborhood size based on weight utilization and solution quality\n    total_items = len(new_solution)\n    weight_utilization = current_weight / capacity\n    solution_quality = (np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - weight_utilization) * solution_quality)))\n\n    # Step 3: Hybrid local search with weighted swaps and subset flips\n    subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.8  # Higher momentum for more aggressive search\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        # Weighted improvement with momentum\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        # Probabilistic flip with adaptive temperature\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8966897838182759,
            0.5121578574180603
        ]
    },
    {
        "algorithm": "The algorithm selects a promising solution from the archive using a weighted combination of crowding distance and hypervolume contribution, then applies a hybrid local search with dynamic neighborhood size, weighted objective swaps, and probabilistic exploration to generate a feasible neighbor solution that balances both objectives while prioritizing higher-quality improvements. The method adapts to solution quality and weight utilization, ensuring feasible moves while favoring high-impact changes based on dynamic objective weights and momentum.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select solution with highest weighted crowding-distance and hypervolume contribution\n    pareto_front = []\n    for sol, obj in archive:\n        dominated = False\n        to_remove = []\n        for i, (_, other_obj) in enumerate(pareto_front):\n            if other_obj[0] >= obj[0] and other_obj[1] >= obj[1] and (other_obj[0] > obj[0] or other_obj[1] > obj[1]):\n                dominated = True\n                break\n            if obj[0] >= other_obj[0] and obj[1] >= other_obj[1] and (obj[0] > other_obj[0] or obj[1] > other_obj[1]):\n                to_remove.append(i)\n        if not dominated:\n            for i in sorted(to_remove, reverse=True):\n                del pareto_front[i]\n            pareto_front.append((sol, obj))\n\n    if not pareto_front:\n        pareto_front = archive.copy()\n\n    if len(pareto_front) == 1:\n        base_solution = pareto_front[0][0].copy()\n    else:\n        objectives = np.array([obj for _, obj in pareto_front])\n        crowding_distances = np.zeros(len(pareto_front))\n        for obj_idx in range(2):\n            sorted_indices = np.argsort(objectives[:, obj_idx])\n            crowding_distances[sorted_indices[0]] = np.inf\n            crowding_distances[sorted_indices[-1]] = np.inf\n            for i in range(1, len(pareto_front) - 1):\n                if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                    crowding_distances[sorted_indices[i]] = np.inf\n                else:\n                    crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n        max_v1 = max(obj[0] for _, obj in pareto_front)\n        max_v2 = max(obj[1] for _, obj in pareto_front)\n        contributions = []\n        for i, (sol, obj) in enumerate(pareto_front):\n            left = pareto_front[i-1][1] if i > 0 else (0, 0)\n            right = pareto_front[i+1][1] if i < len(pareto_front)-1 else (max_v1, max_v2)\n            contribution = (right[0] - left[0]) * (right[1] - left[1])\n            contributions.append(contribution)\n\n        combined_scores = crowding_distances + np.array(contributions)\n        selected_idx = np.argmax(combined_scores)\n        base_solution = pareto_front[selected_idx][0].copy()\n\n    new_solution = base_solution.copy()\n    current_weight = np.sum(weight_lst[new_solution == 1])\n    remaining_capacity = capacity - current_weight\n\n    # Step 2: Dynamic neighborhood size based on weight utilization and solution quality\n    total_items = len(new_solution)\n    weight_utilization = current_weight / capacity\n    solution_quality = (np.sum(value1_lst[new_solution == 1]) + np.sum(value2_lst[new_solution == 1])) / (np.sum(value1_lst) + np.sum(value2_lst) + 1e-6)\n    neighborhood_size = max(1, int(total_items * (0.2 + 0.3 * (1 - weight_utilization) * solution_quality)))\n\n    # Step 3: Hybrid local search with weighted swaps and subset flips\n    subset_indices = np.random.choice(total_items, size=neighborhood_size, replace=False)\n\n    # Calculate dynamic objective weights\n    total_v1 = np.sum(value1_lst[new_solution == 1])\n    total_v2 = np.sum(value2_lst[new_solution == 1])\n    weight_v1 = total_v2 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n    weight_v2 = total_v1 / (total_v1 + total_v2 + 1e-6) if (total_v1 + total_v2) > 0 else 0.5\n\n    best_improvement = -float('inf')\n    best_candidate = None\n    momentum = 0.8\n\n    for idx in subset_indices:\n        temp_solution = new_solution.copy()\n        temp_solution[idx] = 1 - temp_solution[idx]\n\n        new_weight = current_weight\n        if temp_solution[idx] == 1:\n            new_weight += weight_lst[idx]\n        else:\n            new_weight -= weight_lst[idx]\n\n        if new_weight > capacity:\n            continue\n\n        delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n        delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n        improvement = momentum * (weight_v1 * delta_v1 + weight_v2 * delta_v2) + (1 - momentum) * (delta_v1 + delta_v2)\n\n        if improvement > best_improvement:\n            best_improvement = improvement\n            best_candidate = temp_solution\n\n    if best_candidate is not None:\n        new_solution = best_candidate\n    else:\n        valid_indices = [i for i in range(len(new_solution)) if\n                        (new_solution[i] == 1 and current_weight - weight_lst[i] <= capacity) or\n                        (new_solution[i] == 0 and current_weight + weight_lst[i] <= capacity)]\n        if valid_indices:\n            temperature = 1 - solution_quality\n            contributions = []\n            for i in valid_indices:\n                potential_v1 = value1_lst[i] if new_solution[i] == 0 else -value1_lst[i]\n                potential_v2 = value2_lst[i] if new_solution[i] == 0 else -value2_lst[i]\n                contribution = (weight_v1 * potential_v1 + weight_v2 * potential_v2) / (temperature + 1e-6)\n                contributions.append(contribution)\n\n            if sum(contributions) > 0:\n                probabilities = [c / sum(contributions) for c in contributions]\n                idx = np.random.choice(valid_indices, p=probabilities)\n            else:\n                idx = random.choice(valid_indices)\n            new_solution[idx] = 1 - new_solution[idx]\n\n    return new_solution\n\n",
        "score": [
            -0.8847395620932276,
            0.4530027508735657
        ]
    },
    {
        "algorithm": null,
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        return np.zeros_like(weight_lst, dtype=int)\n\n    # Step 1: Adaptive selection - prioritize solutions with high crowding distance and normalized objective diversity\n    objectives = np.array([obj for _, obj in archive])\n    crowding_distances = np.zeros(len(archive))\n    for obj_idx in range(2):\n        sorted_indices = np.argsort(objectives[:, obj_idx])\n        crowding_distances[sorted_indices[0]] = np.inf\n        crowding_distances[sorted_indices[-1]] = np.inf\n        for i in range(1, len(archive) - 1):\n            if objectives[sorted_indices[-1], obj_idx] == objectives[sorted_indices[0], obj_idx]:\n                crowding_distances[sorted_indices[i]] = np.inf\n            else:\n                crowding_distances[sorted_indices[i]] += (objectives[sorted_indices[i+1], obj_idx] - objectives[sorted_indices[i-1], obj_idx]) / (objectives[sorted_indices[-1], obj_idx] - objectives[sorted_indices[0], obj_idx])\n\n    # Combine crowding distance with normalized objective diversity\n    max_v1, max_v2 = np.max(objectives, axis=0)\n    normalized_diversity = np.zeros(len(archive))\n    for i, (v1, v2) in enumerate(objectives):\n        normalized_diversity[i] = (v1 / max_v1 + v2 / max_v2) if max_v1 > 0 and max_v2 > 0 else 0\n\n    selection_scores = crowding_distances * normalized_diversity\n    selected_idx = np.argmax(selection_scores)\n    base_solution = archive[selected_idx][0].copy()\n    current_weight = np.sum(weight_lst[base_solution == 1])\n\n    # Step 2: Hybrid local search - adaptive subset flips with objective-weighted improvements\n    new_solution = base_solution.copy()\n    n_items = len(weight_lst)\n\n    # Dynamic neighborhood size based on solution quality\n    neighborhood_size = min(5, max(1, int(n_items * 0.1 * (1 - current_weight / capacity))))\n\n    # Strategy 1: Adaptive subset flip with objective-weighted selection\n    for _ in range(neighborhood_size):\n        # Select a random subset of items to consider\n        subset_size = min(3, max(1, int(n_items * 0.2)))\n        subset_indices = np.random.choice(n_items, size=subset_size, replace=False)\n\n        best_improvement = 0\n        best_candidate = None\n\n        for idx in subset_indices:\n            # Try flipping the item\n            temp_solution = new_solution.copy()\n            temp_solution[idx] = 1 - temp_solution[idx]\n\n            # Calculate new weight\n            new_weight = current_weight\n            if temp_solution[idx] == 1:\n                new_weight += weight_lst[idx]\n            else:\n                new_weight -= weight_lst[idx]\n\n            # Check feasibility\n            if new_weight > capacity:\n                continue\n\n            # Calculate objective improvements\n            delta_v1 = value1_lst[idx] if temp_solution[idx] == 1 else -value1_lst[idx]\n            delta_v2 = value2_lst[idx] if temp_solution[idx] == 1 else -value2_lst[idx]\n\n            # Objective-weighted improvement\n            improvement = (delta_v1 + delta_v2) / (weight_lst[idx] + 1e-6)\n\n            if improvement > best_improvement:\n                best_improvement = improvement\n                best_candidate = temp_solution\n\n        if best_candidate is not None:\n            new_solution = best_candidate\n            current_weight = np.sum(weight_lst[new_solution == 1])\n\n    # Strategy 2: Objective-weighted item swaps if no improvement found\n    if best_improvement == 0:\n        for _ in range(neighborhood_size):\n            out_items = np.where(new_solution == 1)[0]\n            in_items = np.where(new_solution == 0)[0]\n\n            if len(out_items) > 0 and len(in_items) > 0:\n                out_idx = np.random.choice(out_items)\n                in_idx = np.random.choice(in_items)\n\n                new_weight = current_weight - weight_lst[out_idx] + weight_lst[in_idx]\n                if new_weight <= capacity:\n                    # Objective-weighted swap decision\n                    delta_v1 = value1_lst[in_idx] - value1_lst[out_idx]\n                    delta_v2 = value2_lst[in_idx] - value2_lst[out_idx]\n                    if (delta_v1 + delta_v2) > 0:\n                        new_solution[out_idx] = 0\n                        new_solution[in_idx] = 1\n                        current_weight = new_weight\n                        break\n\n    return new_solution\n\n",
        "score": [
            -0.9603317168957264,
            1.4646897912025452
        ]
    },
    {
        "algorithm": "The algorithm selects the most promising solution from the archive (based on average value-to-weight density) and applies a hybrid local search strategy that prioritizes swapping low-value items for high-value ones while ensuring feasibility through adaptive perturbations. If no swaps are feasible, it randomly removes items to free up capacity. The approach balances exploration (via adaptive swaps) and exploitation (via value-to-weight ratios) to improve both objectives.",
        "function": "def select_neighbor(archive: List[Tuple[np.ndarray, Tuple[float, float]]], weight_lst: np.ndarray, value1_lst: np.ndarray, value2_lst: np.ndarray, capacity: float) -> np.ndarray:\n    if not archive:\n        raise ValueError(\"Archive is empty\")\n\n    # Step 1: Select a promising solution from the archive\n    # Calculate the density (value/weight) for each objective\n    densities = []\n    for sol, (v1, v2) in archive:\n        included = sol == 1\n        total_weight = np.sum(weight_lst[included])\n        if total_weight == 0:\n            density1 = density2 = 0\n        else:\n            density1 = np.sum(value1_lst[included]) / total_weight\n            density2 = np.sum(value2_lst[included]) / total_weight\n        densities.append((density1 + density2) / 2)  # Average density\n\n    # Select the solution with the highest density (most promising for improvement)\n    best_idx = np.argmax(densities)\n    base_solution = archive[best_idx][0].copy()\n    new_solution = base_solution.copy()\n\n    # Step 2: Apply novel local search operator\n    # Hybrid strategy: adaptive item swaps and perturbations\n    included_items = np.where(new_solution == 1)[0]\n    excluded_items = np.where(new_solution == 0)[0]\n\n    if len(included_items) > 0 and len(excluded_items) > 0:\n        # Adaptive swap: replace a low-value item with a high-value item\n        # Calculate value-to-weight ratio for included items\n        included_ratios = (value1_lst[included_items] + value2_lst[included_items]) / weight_lst[included_items]\n        worst_included = included_items[np.argmin(included_ratios)]\n\n        # Calculate value-to-weight ratio for excluded items\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        # Check if swap is feasible\n        current_weight = np.sum(weight_lst[new_solution == 1])\n        swap_weight_diff = weight_lst[best_excluded] - weight_lst[worst_included]\n\n        if current_weight + swap_weight_diff <= capacity:\n            new_solution[worst_included] = 0\n            new_solution[best_excluded] = 1\n        else:\n            # If swap is not feasible, perform a perturbation\n            # Randomly flip a small number of items to find a feasible solution\n            max_perturbations = min(3, len(included_items))\n            for _ in range(max_perturbations):\n                candidate = random.choice(included_items)\n                if current_weight - weight_lst[candidate] <= capacity:\n                    new_solution[candidate] = 0\n                    current_weight -= weight_lst[candidate]\n                    break\n\n    # If no items are included, add the best item if possible\n    elif len(included_items) == 0 and len(excluded_items) > 0:\n        # Add the item with the highest value-to-weight ratio\n        excluded_ratios = (value1_lst[excluded_items] + value2_lst[excluded_items]) / weight_lst[excluded_items]\n        best_excluded = excluded_items[np.argmax(excluded_ratios)]\n\n        if weight_lst[best_excluded] <= capacity:\n            new_solution[best_excluded] = 1\n\n    return new_solution\n\n",
        "score": [
            -0.41552698679575917,
            0.3817824423313141
        ]
    }
]